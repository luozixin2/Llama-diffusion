ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 8 CUDA devices:
  Device 0: NVIDIA GeForce RTX 4090, compute capability 8.9, VMM: yes
  Device 1: NVIDIA GeForce RTX 4090, compute capability 8.9, VMM: yes
  Device 2: NVIDIA GeForce RTX 4090, compute capability 8.9, VMM: yes
  Device 3: NVIDIA GeForce RTX 4090, compute capability 8.9, VMM: yes
  Device 4: NVIDIA GeForce RTX 4090, compute capability 8.9, VMM: yes
  Device 5: NVIDIA GeForce RTX 4090, compute capability 8.9, VMM: yes
  Device 6: NVIDIA GeForce RTX 4090, compute capability 8.9, VMM: yes
  Device 7: NVIDIA GeForce RTX 4090, compute capability 8.9, VMM: yes
llama_model_load_from_file_impl: using device CUDA0 (NVIDIA GeForce RTX 4090) (0000:01:00.0) - 47437 MiB free
llama_model_load_from_file_impl: using device CUDA1 (NVIDIA GeForce RTX 4090) (0000:24:00.0) - 48126 MiB free
llama_model_load_from_file_impl: using device CUDA2 (NVIDIA GeForce RTX 4090) (0000:41:00.0) - 48126 MiB free
llama_model_load_from_file_impl: using device CUDA3 (NVIDIA GeForce RTX 4090) (0000:61:00.0) - 48126 MiB free
llama_model_load_from_file_impl: using device CUDA4 (NVIDIA GeForce RTX 4090) (0000:81:00.0) - 48126 MiB free
llama_model_load_from_file_impl: using device CUDA5 (NVIDIA GeForce RTX 4090) (0000:a1:00.0) - 48126 MiB free
llama_model_load_from_file_impl: using device CUDA6 (NVIDIA GeForce RTX 4090) (0000:c1:00.0) - 48126 MiB free
llama_model_load_from_file_impl: using device CUDA7 (NVIDIA GeForce RTX 4090) (0000:e1:00.0) - 48126 MiB free
llama_model_loader: loaded meta data with 29 key-value pairs and 311 tensors from /home/lzx/SDAR/training/model/SDAR-1.7B-Chat/SDAR-1.7B-Chat-F16.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen3
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = SDAR 1.7B Chat
llama_model_loader: - kv   3:                           general.finetune str              = Chat
llama_model_loader: - kv   4:                           general.basename str              = SDAR
llama_model_loader: - kv   5:                         general.size_label str              = 1.7B
llama_model_loader: - kv   6:                            general.license str              = apache-2.0
llama_model_loader: - kv   7:                          qwen3.block_count u32              = 28
llama_model_loader: - kv   8:                       qwen3.context_length u32              = 32768
llama_model_loader: - kv   9:                     qwen3.embedding_length u32              = 2048
llama_model_loader: - kv  10:                  qwen3.feed_forward_length u32              = 6144
llama_model_loader: - kv  11:                 qwen3.attention.head_count u32              = 16
llama_model_loader: - kv  12:              qwen3.attention.head_count_kv u32              = 8
llama_model_loader: - kv  13:                       qwen3.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  14:     qwen3.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  15:                 qwen3.attention.key_length u32              = 128
llama_model_loader: - kv  16:               qwen3.attention.value_length u32              = 128
llama_model_loader: - kv  17:                          general.file_type u32              = 1
llama_model_loader: - kv  18:               general.quantization_version u32              = 2
llama_model_loader: - kv  19:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  20:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  21:                      tokenizer.ggml.tokens arr[str,151936]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  22:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  23:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  24:                tokenizer.ggml.eos_token_id u32              = 151643
llama_model_loader: - kv  25:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  26:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  27:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  28:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - type  f32:  113 tensors
llama_model_loader: - type  f16:  198 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = F16
print_info: file size   = 3.78 GiB (16.00 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token: 151669 '<|MASK|>' is not marked as EOG
load: control token: 151660 '<|fim_middle|>' is not marked as EOG
load: control token: 151659 '<|fim_prefix|>' is not marked as EOG
load: control token: 151653 '<|vision_end|>' is not marked as EOG
load: control token: 151648 '<|box_start|>' is not marked as EOG
load: control token: 151646 '<|object_ref_start|>' is not marked as EOG
load: control token: 151649 '<|box_end|>' is not marked as EOG
load: control token: 151655 '<|image_pad|>' is not marked as EOG
load: control token: 151651 '<|quad_end|>' is not marked as EOG
load: control token: 151647 '<|object_ref_end|>' is not marked as EOG
load: control token: 151652 '<|vision_start|>' is not marked as EOG
load: control token: 151654 '<|vision_pad|>' is not marked as EOG
load: control token: 151656 '<|video_pad|>' is not marked as EOG
load: control token: 151644 '<|im_start|>' is not marked as EOG
load: control token: 151661 '<|fim_suffix|>' is not marked as EOG
load: control token: 151650 '<|quad_start|>' is not marked as EOG
load: printing all EOG tokens:
load:   - 151643 ('<|endoftext|>')
load:   - 151645 ('<|im_end|>')
load:   - 151662 ('<|fim_pad|>')
load:   - 151663 ('<|repo_name|>')
load:   - 151664 ('<|file_sep|>')
load: special tokens cache size = 27
load: token to piece cache size = 0.9311 MB
print_info: arch             = qwen3
print_info: vocab_only       = 0
print_info: n_ctx_train      = 32768
print_info: n_embd           = 2048
print_info: n_embd_inp       = 2048
print_info: n_layer          = 28
print_info: n_head           = 16
print_info: n_head_kv        = 8
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: is_swa_any       = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 2
print_info: n_embd_k_gqa     = 1024
print_info: n_embd_v_gqa     = 1024
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-06
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 6144
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: n_expert_groups  = 0
print_info: n_group_used     = 0
print_info: causal attn      = 1
print_info: pooling type     = -1
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 1000000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 32768
print_info: rope_finetuned   = unknown
print_info: model type       = 1.7B
print_info: model params     = 2.03 B
print_info: general.name     = SDAR 1.7B Chat
print_info: vocab type       = BPE
print_info: n_vocab          = 151936
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151643 '<|endoftext|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151643 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: layer   0 assigned to device CUDA0, is_swa = 0
load_tensors: layer   1 assigned to device CUDA0, is_swa = 0
load_tensors: layer   2 assigned to device CUDA0, is_swa = 0
load_tensors: layer   3 assigned to device CUDA0, is_swa = 0
load_tensors: layer   4 assigned to device CUDA1, is_swa = 0
load_tensors: layer   5 assigned to device CUDA1, is_swa = 0
load_tensors: layer   6 assigned to device CUDA1, is_swa = 0
load_tensors: layer   7 assigned to device CUDA1, is_swa = 0
load_tensors: layer   8 assigned to device CUDA2, is_swa = 0
load_tensors: layer   9 assigned to device CUDA2, is_swa = 0
load_tensors: layer  10 assigned to device CUDA2, is_swa = 0
load_tensors: layer  11 assigned to device CUDA3, is_swa = 0
load_tensors: layer  12 assigned to device CUDA3, is_swa = 0
load_tensors: layer  13 assigned to device CUDA3, is_swa = 0
load_tensors: layer  14 assigned to device CUDA3, is_swa = 0
load_tensors: layer  15 assigned to device CUDA4, is_swa = 0
load_tensors: layer  16 assigned to device CUDA4, is_swa = 0
load_tensors: layer  17 assigned to device CUDA4, is_swa = 0
load_tensors: layer  18 assigned to device CUDA4, is_swa = 0
load_tensors: layer  19 assigned to device CUDA5, is_swa = 0
load_tensors: layer  20 assigned to device CUDA5, is_swa = 0
load_tensors: layer  21 assigned to device CUDA5, is_swa = 0
load_tensors: layer  22 assigned to device CUDA6, is_swa = 0
load_tensors: layer  23 assigned to device CUDA6, is_swa = 0
load_tensors: layer  24 assigned to device CUDA6, is_swa = 0
load_tensors: layer  25 assigned to device CUDA6, is_swa = 0
load_tensors: layer  26 assigned to device CUDA7, is_swa = 0
load_tensors: layer  27 assigned to device CUDA7, is_swa = 0
load_tensors: layer  28 assigned to device CUDA7, is_swa = 0
create_tensor: loading tensor token_embd.weight
create_tensor: loading tensor output_norm.weight
create_tensor: loading tensor output.weight
create_tensor: loading tensor blk.0.attn_norm.weight
create_tensor: loading tensor blk.0.attn_q.weight
create_tensor: loading tensor blk.0.attn_k.weight
create_tensor: loading tensor blk.0.attn_v.weight
create_tensor: loading tensor blk.0.attn_output.weight
create_tensor: loading tensor blk.0.attn_k_norm.weight
create_tensor: loading tensor blk.0.attn_q_norm.weight
create_tensor: loading tensor blk.0.ffn_norm.weight
create_tensor: loading tensor blk.0.ffn_gate.weight
create_tensor: loading tensor blk.0.ffn_down.weight
create_tensor: loading tensor blk.0.ffn_up.weight
create_tensor: loading tensor blk.1.attn_norm.weight
create_tensor: loading tensor blk.1.attn_q.weight
create_tensor: loading tensor blk.1.attn_k.weight
create_tensor: loading tensor blk.1.attn_v.weight
create_tensor: loading tensor blk.1.attn_output.weight
create_tensor: loading tensor blk.1.attn_k_norm.weight
create_tensor: loading tensor blk.1.attn_q_norm.weight
create_tensor: loading tensor blk.1.ffn_norm.weight
create_tensor: loading tensor blk.1.ffn_gate.weight
create_tensor: loading tensor blk.1.ffn_down.weight
create_tensor: loading tensor blk.1.ffn_up.weight
create_tensor: loading tensor blk.2.attn_norm.weight
create_tensor: loading tensor blk.2.attn_q.weight
create_tensor: loading tensor blk.2.attn_k.weight
create_tensor: loading tensor blk.2.attn_v.weight
create_tensor: loading tensor blk.2.attn_output.weight
create_tensor: loading tensor blk.2.attn_k_norm.weight
create_tensor: loading tensor blk.2.attn_q_norm.weight
create_tensor: loading tensor blk.2.ffn_norm.weight
create_tensor: loading tensor blk.2.ffn_gate.weight
create_tensor: loading tensor blk.2.ffn_down.weight
create_tensor: loading tensor blk.2.ffn_up.weight
create_tensor: loading tensor blk.3.attn_norm.weight
create_tensor: loading tensor blk.3.attn_q.weight
create_tensor: loading tensor blk.3.attn_k.weight
create_tensor: loading tensor blk.3.attn_v.weight
create_tensor: loading tensor blk.3.attn_output.weight
create_tensor: loading tensor blk.3.attn_k_norm.weight
create_tensor: loading tensor blk.3.attn_q_norm.weight
create_tensor: loading tensor blk.3.ffn_norm.weight
create_tensor: loading tensor blk.3.ffn_gate.weight
create_tensor: loading tensor blk.3.ffn_down.weight
create_tensor: loading tensor blk.3.ffn_up.weight
create_tensor: loading tensor blk.4.attn_norm.weight
create_tensor: loading tensor blk.4.attn_q.weight
create_tensor: loading tensor blk.4.attn_k.weight
create_tensor: loading tensor blk.4.attn_v.weight
create_tensor: loading tensor blk.4.attn_output.weight
create_tensor: loading tensor blk.4.attn_k_norm.weight
create_tensor: loading tensor blk.4.attn_q_norm.weight
create_tensor: loading tensor blk.4.ffn_norm.weight
create_tensor: loading tensor blk.4.ffn_gate.weight
create_tensor: loading tensor blk.4.ffn_down.weight
create_tensor: loading tensor blk.4.ffn_up.weight
create_tensor: loading tensor blk.5.attn_norm.weight
create_tensor: loading tensor blk.5.attn_q.weight
create_tensor: loading tensor blk.5.attn_k.weight
create_tensor: loading tensor blk.5.attn_v.weight
create_tensor: loading tensor blk.5.attn_output.weight
create_tensor: loading tensor blk.5.attn_k_norm.weight
create_tensor: loading tensor blk.5.attn_q_norm.weight
create_tensor: loading tensor blk.5.ffn_norm.weight
create_tensor: loading tensor blk.5.ffn_gate.weight
create_tensor: loading tensor blk.5.ffn_down.weight
create_tensor: loading tensor blk.5.ffn_up.weight
create_tensor: loading tensor blk.6.attn_norm.weight
create_tensor: loading tensor blk.6.attn_q.weight
create_tensor: loading tensor blk.6.attn_k.weight
create_tensor: loading tensor blk.6.attn_v.weight
create_tensor: loading tensor blk.6.attn_output.weight
create_tensor: loading tensor blk.6.attn_k_norm.weight
create_tensor: loading tensor blk.6.attn_q_norm.weight
create_tensor: loading tensor blk.6.ffn_norm.weight
create_tensor: loading tensor blk.6.ffn_gate.weight
create_tensor: loading tensor blk.6.ffn_down.weight
create_tensor: loading tensor blk.6.ffn_up.weight
create_tensor: loading tensor blk.7.attn_norm.weight
create_tensor: loading tensor blk.7.attn_q.weight
create_tensor: loading tensor blk.7.attn_k.weight
create_tensor: loading tensor blk.7.attn_v.weight
create_tensor: loading tensor blk.7.attn_output.weight
create_tensor: loading tensor blk.7.attn_k_norm.weight
create_tensor: loading tensor blk.7.attn_q_norm.weight
create_tensor: loading tensor blk.7.ffn_norm.weight
create_tensor: loading tensor blk.7.ffn_gate.weight
create_tensor: loading tensor blk.7.ffn_down.weight
create_tensor: loading tensor blk.7.ffn_up.weight
create_tensor: loading tensor blk.8.attn_norm.weight
create_tensor: loading tensor blk.8.attn_q.weight
create_tensor: loading tensor blk.8.attn_k.weight
create_tensor: loading tensor blk.8.attn_v.weight
create_tensor: loading tensor blk.8.attn_output.weight
create_tensor: loading tensor blk.8.attn_k_norm.weight
create_tensor: loading tensor blk.8.attn_q_norm.weight
create_tensor: loading tensor blk.8.ffn_norm.weight
create_tensor: loading tensor blk.8.ffn_gate.weight
create_tensor: loading tensor blk.8.ffn_down.weight
create_tensor: loading tensor blk.8.ffn_up.weight
create_tensor: loading tensor blk.9.attn_norm.weight
create_tensor: loading tensor blk.9.attn_q.weight
create_tensor: loading tensor blk.9.attn_k.weight
create_tensor: loading tensor blk.9.attn_v.weight
create_tensor: loading tensor blk.9.attn_output.weight
create_tensor: loading tensor blk.9.attn_k_norm.weight
create_tensor: loading tensor blk.9.attn_q_norm.weight
create_tensor: loading tensor blk.9.ffn_norm.weight
create_tensor: loading tensor blk.9.ffn_gate.weight
create_tensor: loading tensor blk.9.ffn_down.weight
create_tensor: loading tensor blk.9.ffn_up.weight
create_tensor: loading tensor blk.10.attn_norm.weight
create_tensor: loading tensor blk.10.attn_q.weight
create_tensor: loading tensor blk.10.attn_k.weight
create_tensor: loading tensor blk.10.attn_v.weight
create_tensor: loading tensor blk.10.attn_output.weight
create_tensor: loading tensor blk.10.attn_k_norm.weight
create_tensor: loading tensor blk.10.attn_q_norm.weight
create_tensor: loading tensor blk.10.ffn_norm.weight
create_tensor: loading tensor blk.10.ffn_gate.weight
create_tensor: loading tensor blk.10.ffn_down.weight
create_tensor: loading tensor blk.10.ffn_up.weight
create_tensor: loading tensor blk.11.attn_norm.weight
create_tensor: loading tensor blk.11.attn_q.weight
create_tensor: loading tensor blk.11.attn_k.weight
create_tensor: loading tensor blk.11.attn_v.weight
create_tensor: loading tensor blk.11.attn_output.weight
create_tensor: loading tensor blk.11.attn_k_norm.weight
create_tensor: loading tensor blk.11.attn_q_norm.weight
create_tensor: loading tensor blk.11.ffn_norm.weight
create_tensor: loading tensor blk.11.ffn_gate.weight
create_tensor: loading tensor blk.11.ffn_down.weight
create_tensor: loading tensor blk.11.ffn_up.weight
create_tensor: loading tensor blk.12.attn_norm.weight
create_tensor: loading tensor blk.12.attn_q.weight
create_tensor: loading tensor blk.12.attn_k.weight
create_tensor: loading tensor blk.12.attn_v.weight
create_tensor: loading tensor blk.12.attn_output.weight
create_tensor: loading tensor blk.12.attn_k_norm.weight
create_tensor: loading tensor blk.12.attn_q_norm.weight
create_tensor: loading tensor blk.12.ffn_norm.weight
create_tensor: loading tensor blk.12.ffn_gate.weight
create_tensor: loading tensor blk.12.ffn_down.weight
create_tensor: loading tensor blk.12.ffn_up.weight
create_tensor: loading tensor blk.13.attn_norm.weight
create_tensor: loading tensor blk.13.attn_q.weight
create_tensor: loading tensor blk.13.attn_k.weight
create_tensor: loading tensor blk.13.attn_v.weight
create_tensor: loading tensor blk.13.attn_output.weight
create_tensor: loading tensor blk.13.attn_k_norm.weight
create_tensor: loading tensor blk.13.attn_q_norm.weight
create_tensor: loading tensor blk.13.ffn_norm.weight
create_tensor: loading tensor blk.13.ffn_gate.weight
create_tensor: loading tensor blk.13.ffn_down.weight
create_tensor: loading tensor blk.13.ffn_up.weight
create_tensor: loading tensor blk.14.attn_norm.weight
create_tensor: loading tensor blk.14.attn_q.weight
create_tensor: loading tensor blk.14.attn_k.weight
create_tensor: loading tensor blk.14.attn_v.weight
create_tensor: loading tensor blk.14.attn_output.weight
create_tensor: loading tensor blk.14.attn_k_norm.weight
create_tensor: loading tensor blk.14.attn_q_norm.weight
create_tensor: loading tensor blk.14.ffn_norm.weight
create_tensor: loading tensor blk.14.ffn_gate.weight
create_tensor: loading tensor blk.14.ffn_down.weight
create_tensor: loading tensor blk.14.ffn_up.weight
create_tensor: loading tensor blk.15.attn_norm.weight
create_tensor: loading tensor blk.15.attn_q.weight
create_tensor: loading tensor blk.15.attn_k.weight
create_tensor: loading tensor blk.15.attn_v.weight
create_tensor: loading tensor blk.15.attn_output.weight
create_tensor: loading tensor blk.15.attn_k_norm.weight
create_tensor: loading tensor blk.15.attn_q_norm.weight
create_tensor: loading tensor blk.15.ffn_norm.weight
create_tensor: loading tensor blk.15.ffn_gate.weight
create_tensor: loading tensor blk.15.ffn_down.weight
create_tensor: loading tensor blk.15.ffn_up.weight
create_tensor: loading tensor blk.16.attn_norm.weight
create_tensor: loading tensor blk.16.attn_q.weight
create_tensor: loading tensor blk.16.attn_k.weight
create_tensor: loading tensor blk.16.attn_v.weight
create_tensor: loading tensor blk.16.attn_output.weight
create_tensor: loading tensor blk.16.attn_k_norm.weight
create_tensor: loading tensor blk.16.attn_q_norm.weight
create_tensor: loading tensor blk.16.ffn_norm.weight
create_tensor: loading tensor blk.16.ffn_gate.weight
create_tensor: loading tensor blk.16.ffn_down.weight
create_tensor: loading tensor blk.16.ffn_up.weight
create_tensor: loading tensor blk.17.attn_norm.weight
create_tensor: loading tensor blk.17.attn_q.weight
create_tensor: loading tensor blk.17.attn_k.weight
create_tensor: loading tensor blk.17.attn_v.weight
create_tensor: loading tensor blk.17.attn_output.weight
create_tensor: loading tensor blk.17.attn_k_norm.weight
create_tensor: loading tensor blk.17.attn_q_norm.weight
create_tensor: loading tensor blk.17.ffn_norm.weight
create_tensor: loading tensor blk.17.ffn_gate.weight
create_tensor: loading tensor blk.17.ffn_down.weight
create_tensor: loading tensor blk.17.ffn_up.weight
create_tensor: loading tensor blk.18.attn_norm.weight
create_tensor: loading tensor blk.18.attn_q.weight
create_tensor: loading tensor blk.18.attn_k.weight
create_tensor: loading tensor blk.18.attn_v.weight
create_tensor: loading tensor blk.18.attn_output.weight
create_tensor: loading tensor blk.18.attn_k_norm.weight
create_tensor: loading tensor blk.18.attn_q_norm.weight
create_tensor: loading tensor blk.18.ffn_norm.weight
create_tensor: loading tensor blk.18.ffn_gate.weight
create_tensor: loading tensor blk.18.ffn_down.weight
create_tensor: loading tensor blk.18.ffn_up.weight
create_tensor: loading tensor blk.19.attn_norm.weight
create_tensor: loading tensor blk.19.attn_q.weight
create_tensor: loading tensor blk.19.attn_k.weight
create_tensor: loading tensor blk.19.attn_v.weight
create_tensor: loading tensor blk.19.attn_output.weight
create_tensor: loading tensor blk.19.attn_k_norm.weight
create_tensor: loading tensor blk.19.attn_q_norm.weight
create_tensor: loading tensor blk.19.ffn_norm.weight
create_tensor: loading tensor blk.19.ffn_gate.weight
create_tensor: loading tensor blk.19.ffn_down.weight
create_tensor: loading tensor blk.19.ffn_up.weight
create_tensor: loading tensor blk.20.attn_norm.weight
create_tensor: loading tensor blk.20.attn_q.weight
create_tensor: loading tensor blk.20.attn_k.weight
create_tensor: loading tensor blk.20.attn_v.weight
create_tensor: loading tensor blk.20.attn_output.weight
create_tensor: loading tensor blk.20.attn_k_norm.weight
create_tensor: loading tensor blk.20.attn_q_norm.weight
create_tensor: loading tensor blk.20.ffn_norm.weight
create_tensor: loading tensor blk.20.ffn_gate.weight
create_tensor: loading tensor blk.20.ffn_down.weight
create_tensor: loading tensor blk.20.ffn_up.weight
create_tensor: loading tensor blk.21.attn_norm.weight
create_tensor: loading tensor blk.21.attn_q.weight
create_tensor: loading tensor blk.21.attn_k.weight
create_tensor: loading tensor blk.21.attn_v.weight
create_tensor: loading tensor blk.21.attn_output.weight
create_tensor: loading tensor blk.21.attn_k_norm.weight
create_tensor: loading tensor blk.21.attn_q_norm.weight
create_tensor: loading tensor blk.21.ffn_norm.weight
create_tensor: loading tensor blk.21.ffn_gate.weight
create_tensor: loading tensor blk.21.ffn_down.weight
create_tensor: loading tensor blk.21.ffn_up.weight
create_tensor: loading tensor blk.22.attn_norm.weight
create_tensor: loading tensor blk.22.attn_q.weight
create_tensor: loading tensor blk.22.attn_k.weight
create_tensor: loading tensor blk.22.attn_v.weight
create_tensor: loading tensor blk.22.attn_output.weight
create_tensor: loading tensor blk.22.attn_k_norm.weight
create_tensor: loading tensor blk.22.attn_q_norm.weight
create_tensor: loading tensor blk.22.ffn_norm.weight
create_tensor: loading tensor blk.22.ffn_gate.weight
create_tensor: loading tensor blk.22.ffn_down.weight
create_tensor: loading tensor blk.22.ffn_up.weight
create_tensor: loading tensor blk.23.attn_norm.weight
create_tensor: loading tensor blk.23.attn_q.weight
create_tensor: loading tensor blk.23.attn_k.weight
create_tensor: loading tensor blk.23.attn_v.weight
create_tensor: loading tensor blk.23.attn_output.weight
create_tensor: loading tensor blk.23.attn_k_norm.weight
create_tensor: loading tensor blk.23.attn_q_norm.weight
create_tensor: loading tensor blk.23.ffn_norm.weight
create_tensor: loading tensor blk.23.ffn_gate.weight
create_tensor: loading tensor blk.23.ffn_down.weight
create_tensor: loading tensor blk.23.ffn_up.weight
create_tensor: loading tensor blk.24.attn_norm.weight
create_tensor: loading tensor blk.24.attn_q.weight
create_tensor: loading tensor blk.24.attn_k.weight
create_tensor: loading tensor blk.24.attn_v.weight
create_tensor: loading tensor blk.24.attn_output.weight
create_tensor: loading tensor blk.24.attn_k_norm.weight
create_tensor: loading tensor blk.24.attn_q_norm.weight
create_tensor: loading tensor blk.24.ffn_norm.weight
create_tensor: loading tensor blk.24.ffn_gate.weight
create_tensor: loading tensor blk.24.ffn_down.weight
create_tensor: loading tensor blk.24.ffn_up.weight
create_tensor: loading tensor blk.25.attn_norm.weight
create_tensor: loading tensor blk.25.attn_q.weight
create_tensor: loading tensor blk.25.attn_k.weight
create_tensor: loading tensor blk.25.attn_v.weight
create_tensor: loading tensor blk.25.attn_output.weight
create_tensor: loading tensor blk.25.attn_k_norm.weight
create_tensor: loading tensor blk.25.attn_q_norm.weight
create_tensor: loading tensor blk.25.ffn_norm.weight
create_tensor: loading tensor blk.25.ffn_gate.weight
create_tensor: loading tensor blk.25.ffn_down.weight
create_tensor: loading tensor blk.25.ffn_up.weight
create_tensor: loading tensor blk.26.attn_norm.weight
create_tensor: loading tensor blk.26.attn_q.weight
create_tensor: loading tensor blk.26.attn_k.weight
create_tensor: loading tensor blk.26.attn_v.weight
create_tensor: loading tensor blk.26.attn_output.weight
create_tensor: loading tensor blk.26.attn_k_norm.weight
create_tensor: loading tensor blk.26.attn_q_norm.weight
create_tensor: loading tensor blk.26.ffn_norm.weight
create_tensor: loading tensor blk.26.ffn_gate.weight
create_tensor: loading tensor blk.26.ffn_down.weight
create_tensor: loading tensor blk.26.ffn_up.weight
create_tensor: loading tensor blk.27.attn_norm.weight
create_tensor: loading tensor blk.27.attn_q.weight
create_tensor: loading tensor blk.27.attn_k.weight
create_tensor: loading tensor blk.27.attn_v.weight
create_tensor: loading tensor blk.27.attn_output.weight
create_tensor: loading tensor blk.27.attn_k_norm.weight
create_tensor: loading tensor blk.27.attn_q_norm.weight
create_tensor: loading tensor blk.27.ffn_norm.weight
create_tensor: loading tensor blk.27.ffn_gate.weight
create_tensor: loading tensor blk.27.ffn_down.weight
create_tensor: loading tensor blk.27.ffn_up.weight
load_tensors: tensor 'token_embd.weight' (f16) (and 0 others) cannot be used with preferred buffer type CUDA_Host, using CPU instead
load_tensors: offloading 28 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 29/29 layers to GPU
load_tensors:   CPU_Mapped model buffer size =   593.50 MiB
load_tensors:        CUDA0 model buffer size =   384.07 MiB
load_tensors:        CUDA1 model buffer size =   384.07 MiB
load_tensors:        CUDA2 model buffer size =   288.05 MiB
load_tensors:        CUDA3 model buffer size =   384.07 MiB
load_tensors:        CUDA4 model buffer size =   384.07 MiB
load_tensors:        CUDA5 model buffer size =   288.05 MiB
load_tensors:        CUDA6 model buffer size =   384.07 MiB
load_tensors:        CUDA7 model buffer size =   785.54 MiB
.......................................................................

================================================================================
GPU WARMUP - Running 3 iterations
================================================================================
Warmup iteration 1/3... llama_context: constructing llama_context
llama_context: n_seq_max     = 2
llama_context: n_ctx         = 8192
llama_context: n_ctx_seq     = 4096
llama_context: n_batch       = 2048
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = enabled
llama_context: kv_unified    = false
llama_context: freq_base     = 1000000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_seq (4096) < n_ctx_train (32768) -- the full capacity of the model will not be utilized
set_abort_callback: call
llama_context:  CUDA_Host  output buffer size =     1.16 MiB
llama_kv_cache: layer   0: dev = CUDA0
llama_kv_cache: layer   1: dev = CUDA0
llama_kv_cache: layer   2: dev = CUDA0
llama_kv_cache: layer   3: dev = CUDA0
llama_kv_cache: layer   4: dev = CUDA1
llama_kv_cache: layer   5: dev = CUDA1
llama_kv_cache: layer   6: dev = CUDA1
llama_kv_cache: layer   7: dev = CUDA1
llama_kv_cache: layer   8: dev = CUDA2
llama_kv_cache: layer   9: dev = CUDA2
llama_kv_cache: layer  10: dev = CUDA2
llama_kv_cache: layer  11: dev = CUDA3
llama_kv_cache: layer  12: dev = CUDA3
llama_kv_cache: layer  13: dev = CUDA3
llama_kv_cache: layer  14: dev = CUDA3
llama_kv_cache: layer  15: dev = CUDA4
llama_kv_cache: layer  16: dev = CUDA4
llama_kv_cache: layer  17: dev = CUDA4
llama_kv_cache: layer  18: dev = CUDA4
llama_kv_cache: layer  19: dev = CUDA5
llama_kv_cache: layer  20: dev = CUDA5
llama_kv_cache: layer  21: dev = CUDA5
llama_kv_cache: layer  22: dev = CUDA6
llama_kv_cache: layer  23: dev = CUDA6
llama_kv_cache: layer  24: dev = CUDA6
llama_kv_cache: layer  25: dev = CUDA6
llama_kv_cache: layer  26: dev = CUDA7
llama_kv_cache: layer  27: dev = CUDA7
llama_kv_cache:      CUDA0 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA1 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA2 KV buffer size =    96.00 MiB
llama_kv_cache:      CUDA3 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA4 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA5 KV buffer size =    96.00 MiB
llama_kv_cache:      CUDA6 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA7 KV buffer size =    64.00 MiB
llama_kv_cache: size =  896.00 MiB (  4096 cells,  28 layers,  2/2 seqs), K (f16):  448.00 MiB, V (f16):  448.00 MiB
llama_context: enumerating backends
llama_context: backend_ptrs.size() = 9
llama_context: max_nodes = 2488
llama_context: pipeline parallelism enabled (n_copies=4)
llama_context: reserving full memory module
llama_context: worst-case: n_tokens = 512, n_seqs = 2, n_outputs = 2
graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  2, n_outputs =  512
graph_reserve: reserving a graph for ubatch with n_tokens =    2, n_seqs =  2, n_outputs =    2
graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  2, n_outputs =  512
llama_context:      CUDA0 compute buffer size =   100.04 MiB
llama_context:      CUDA1 compute buffer size =    96.04 MiB
llama_context:      CUDA2 compute buffer size =    96.04 MiB
llama_context:      CUDA3 compute buffer size =    96.04 MiB
llama_context:      CUDA4 compute buffer size =    96.04 MiB
llama_context:      CUDA5 compute buffer size =    96.04 MiB
llama_context:      CUDA6 compute buffer size =    96.04 MiB
llama_context:      CUDA7 compute buffer size =   348.80 MiB
llama_context:  CUDA_Host compute buffer size =    36.05 MiB
llama_context: graph nodes  = 1043
llama_context: graph splits = 9
completed in 8599.08 ms
Warmup iteration 2/3... llama_context: constructing llama_context
llama_context: n_seq_max     = 2
llama_context: n_ctx         = 8192
llama_context: n_ctx_seq     = 4096
llama_context: n_batch       = 2048
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = enabled
llama_context: kv_unified    = false
llama_context: freq_base     = 1000000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_seq (4096) < n_ctx_train (32768) -- the full capacity of the model will not be utilized
set_abort_callback: call
llama_context:  CUDA_Host  output buffer size =     1.16 MiB
llama_kv_cache: layer   0: dev = CUDA0
llama_kv_cache: layer   1: dev = CUDA0
llama_kv_cache: layer   2: dev = CUDA0
llama_kv_cache: layer   3: dev = CUDA0
llama_kv_cache: layer   4: dev = CUDA1
llama_kv_cache: layer   5: dev = CUDA1
llama_kv_cache: layer   6: dev = CUDA1
llama_kv_cache: layer   7: dev = CUDA1
llama_kv_cache: layer   8: dev = CUDA2
llama_kv_cache: layer   9: dev = CUDA2
llama_kv_cache: layer  10: dev = CUDA2
llama_kv_cache: layer  11: dev = CUDA3
llama_kv_cache: layer  12: dev = CUDA3
llama_kv_cache: layer  13: dev = CUDA3
llama_kv_cache: layer  14: dev = CUDA3
llama_kv_cache: layer  15: dev = CUDA4
llama_kv_cache: layer  16: dev = CUDA4
llama_kv_cache: layer  17: dev = CUDA4
llama_kv_cache: layer  18: dev = CUDA4
llama_kv_cache: layer  19: dev = CUDA5
llama_kv_cache: layer  20: dev = CUDA5
llama_kv_cache: layer  21: dev = CUDA5
llama_kv_cache: layer  22: dev = CUDA6
llama_kv_cache: layer  23: dev = CUDA6
llama_kv_cache: layer  24: dev = CUDA6
llama_kv_cache: layer  25: dev = CUDA6
llama_kv_cache: layer  26: dev = CUDA7
llama_kv_cache: layer  27: dev = CUDA7
llama_kv_cache:      CUDA0 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA1 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA2 KV buffer size =    96.00 MiB
llama_kv_cache:      CUDA3 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA4 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA5 KV buffer size =    96.00 MiB
llama_kv_cache:      CUDA6 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA7 KV buffer size =    64.00 MiB
llama_kv_cache: size =  896.00 MiB (  4096 cells,  28 layers,  2/2 seqs), K (f16):  448.00 MiB, V (f16):  448.00 MiB
llama_context: enumerating backends
llama_context: backend_ptrs.size() = 9
llama_context: max_nodes = 2488
llama_context: pipeline parallelism enabled (n_copies=4)
llama_context: reserving full memory module
llama_context: worst-case: n_tokens = 512, n_seqs = 2, n_outputs = 2
graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  2, n_outputs =  512
graph_reserve: reserving a graph for ubatch with n_tokens =    2, n_seqs =  2, n_outputs =    2
graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  2, n_outputs =  512
llama_context:      CUDA0 compute buffer size =   100.04 MiB
llama_context:      CUDA1 compute buffer size =    96.04 MiB
llama_context:      CUDA2 compute buffer size =    96.04 MiB
llama_context:      CUDA3 compute buffer size =    96.04 MiB
llama_context:      CUDA4 compute buffer size =    96.04 MiB
llama_context:      CUDA5 compute buffer size =    96.04 MiB
llama_context:      CUDA6 compute buffer size =    96.04 MiB
llama_context:      CUDA7 compute buffer size =   348.80 MiB
llama_context:  CUDA_Host compute buffer size =    36.05 MiB
llama_context: graph nodes  = 1043
llama_context: graph splits = 9
completed in 798.73 ms
Warmup iteration 3/3... llama_context: constructing llama_context
llama_context: n_seq_max     = 2
llama_context: n_ctx         = 8192
llama_context: n_ctx_seq     = 4096
llama_context: n_batch       = 2048
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = enabled
llama_context: kv_unified    = false
llama_context: freq_base     = 1000000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_seq (4096) < n_ctx_train (32768) -- the full capacity of the model will not be utilized
set_abort_callback: call
llama_context:  CUDA_Host  output buffer size =     1.16 MiB
llama_kv_cache: layer   0: dev = CUDA0
llama_kv_cache: layer   1: dev = CUDA0
llama_kv_cache: layer   2: dev = CUDA0
llama_kv_cache: layer   3: dev = CUDA0
llama_kv_cache: layer   4: dev = CUDA1
llama_kv_cache: layer   5: dev = CUDA1
llama_kv_cache: layer   6: dev = CUDA1
llama_kv_cache: layer   7: dev = CUDA1
llama_kv_cache: layer   8: dev = CUDA2
llama_kv_cache: layer   9: dev = CUDA2
llama_kv_cache: layer  10: dev = CUDA2
llama_kv_cache: layer  11: dev = CUDA3
llama_kv_cache: layer  12: dev = CUDA3
llama_kv_cache: layer  13: dev = CUDA3
llama_kv_cache: layer  14: dev = CUDA3
llama_kv_cache: layer  15: dev = CUDA4
llama_kv_cache: layer  16: dev = CUDA4
llama_kv_cache: layer  17: dev = CUDA4
llama_kv_cache: layer  18: dev = CUDA4
llama_kv_cache: layer  19: dev = CUDA5
llama_kv_cache: layer  20: dev = CUDA5
llama_kv_cache: layer  21: dev = CUDA5
llama_kv_cache: layer  22: dev = CUDA6
llama_kv_cache: layer  23: dev = CUDA6
llama_kv_cache: layer  24: dev = CUDA6
llama_kv_cache: layer  25: dev = CUDA6
llama_kv_cache: layer  26: dev = CUDA7
llama_kv_cache: layer  27: dev = CUDA7
llama_kv_cache:      CUDA0 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA1 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA2 KV buffer size =    96.00 MiB
llama_kv_cache:      CUDA3 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA4 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA5 KV buffer size =    96.00 MiB
llama_kv_cache:      CUDA6 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA7 KV buffer size =    64.00 MiB
llama_kv_cache: size =  896.00 MiB (  4096 cells,  28 layers,  2/2 seqs), K (f16):  448.00 MiB, V (f16):  448.00 MiB
llama_context: enumerating backends
llama_context: backend_ptrs.size() = 9
llama_context: max_nodes = 2488
llama_context: pipeline parallelism enabled (n_copies=4)
llama_context: reserving full memory module
llama_context: worst-case: n_tokens = 512, n_seqs = 2, n_outputs = 2
graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  2, n_outputs =  512
graph_reserve: reserving a graph for ubatch with n_tokens =    2, n_seqs =  2, n_outputs =    2
graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  2, n_outputs =  512
llama_context:      CUDA0 compute buffer size =   100.04 MiB
llama_context:      CUDA1 compute buffer size =    96.04 MiB
llama_context:      CUDA2 compute buffer size =    96.04 MiB
llama_context:      CUDA3 compute buffer size =    96.04 MiB
llama_context:      CUDA4 compute buffer size =    96.04 MiB
llama_context:      CUDA5 compute buffer size =    96.04 MiB
llama_context:      CUDA6 compute buffer size =    96.04 MiB
llama_context:      CUDA7 compute buffer size =   348.80 MiB
llama_context:  CUDA_Host compute buffer size =    36.05 MiB
llama_context: graph nodes  = 1043
llama_context: graph splits = 9
llama_context: constructing llama_context
llama_context: n_seq_max     = 2
llama_context: n_ctx         = 8192
llama_context: n_ctx_seq     = 4096
llama_context: n_batch       = 2048
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = enabled
llama_context: kv_unified    = false
llama_context: freq_base     = 1000000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_seq (4096) < n_ctx_train (32768) -- the full capacity of the model will not be utilized
set_abort_callback: call
llama_context:  CUDA_Host  output buffer size =     1.16 MiB
llama_kv_cache: layer   0: dev = CUDA0
llama_kv_cache: layer   1: dev = CUDA0
llama_kv_cache: layer   2: dev = CUDA0
llama_kv_cache: layer   3: dev = CUDA0
llama_kv_cache: layer   4: dev = CUDA1
llama_kv_cache: layer   5: dev = CUDA1
llama_kv_cache: layer   6: dev = CUDA1
llama_kv_cache: layer   7: dev = CUDA1
llama_kv_cache: layer   8: dev = CUDA2
llama_kv_cache: layer   9: dev = CUDA2
llama_kv_cache: layer  10: dev = CUDA2
llama_kv_cache: layer  11: dev = CUDA3
llama_kv_cache: layer  12: dev = CUDA3
llama_kv_cache: layer  13: dev = CUDA3
llama_kv_cache: layer  14: dev = CUDA3
llama_kv_cache: layer  15: dev = CUDA4
llama_kv_cache: layer  16: dev = CUDA4
llama_kv_cache: layer  17: dev = CUDA4
llama_kv_cache: layer  18: dev = CUDA4
llama_kv_cache: layer  19: dev = CUDA5
llama_kv_cache: layer  20: dev = CUDA5
llama_kv_cache: layer  21: dev = CUDA5
llama_kv_cache: layer  22: dev = CUDA6
llama_kv_cache: layer  23: dev = CUDA6
llama_kv_cache: layer  24: dev = CUDA6
llama_kv_cache: layer  25: dev = CUDA6
llama_kv_cache: layer  26: dev = CUDA7
llama_kv_cache: layer  27: dev = CUDA7
llama_kv_cache:      CUDA0 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA1 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA2 KV buffer size =    96.00 MiB
llama_kv_cache:      CUDA3 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA4 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA5 KV buffer size =    96.00 MiB
llama_kv_cache:      CUDA6 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA7 KV buffer size =    64.00 MiB
llama_kv_cache: size =  896.00 MiB (  4096 cells,  28 layers,  2/2 seqs), K (f16):  448.00 MiB, V (f16):  448.00 MiB
llama_context: enumerating backends
llama_context: backend_ptrs.size() = 9
llama_context: max_nodes = 2488
llama_context: pipeline parallelism enabled (n_copies=4)
llama_context: reserving full memory module
llama_context: worst-case: n_tokens = 512, n_seqs = 2, n_outputs = 2
graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  2, n_outputs =  512
graph_reserve: reserving a graph for ubatch with n_tokens =    2, n_seqs =  2, n_outputs =    2
graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  2, n_outputs =  512
llama_context:      CUDA0 compute buffer size =   100.04 MiB
llama_context:      CUDA1 compute buffer size =    96.04 MiB
llama_context:      CUDA2 compute buffer size =    96.04 MiB
llama_context:      CUDA3 compute buffer size =    96.04 MiB
llama_context:      CUDA4 compute buffer size =    96.04 MiB
llama_context:      CUDA5 compute buffer size =    96.04 MiB
llama_context:      CUDA6 compute buffer size =    96.04 MiB
llama_context:      CUDA7 compute buffer size =   348.80 MiB
llama_context:  CUDA_Host compute buffer size =    36.05 MiB
llama_context: graph nodes  = 1043
llama_context: graph splits = 9
llama_context: constructing llama_context
llama_context: n_seq_max     = 2
llama_context: n_ctx         = 8192
llama_context: n_ctx_seq     = 4096
llama_context: n_batch       = 2048
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = enabled
llama_context: kv_unified    = false
llama_context: freq_base     = 1000000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_seq (4096) < n_ctx_train (32768) -- the full capacity of the model will not be utilized
set_abort_callback: call
llama_context:  CUDA_Host  output buffer size =     1.16 MiB
llama_kv_cache: layer   0: dev = CUDA0
llama_kv_cache: layer   1: dev = CUDA0
llama_kv_cache: layer   2: dev = CUDA0
llama_kv_cache: layer   3: dev = CUDA0
llama_kv_cache: layer   4: dev = CUDA1
llama_kv_cache: layer   5: dev = CUDA1
llama_kv_cache: layer   6: dev = CUDA1
llama_kv_cache: layer   7: dev = CUDA1
llama_kv_cache: layer   8: dev = CUDA2
llama_kv_cache: layer   9: dev = CUDA2
llama_kv_cache: layer  10: dev = CUDA2
llama_kv_cache: layer  11: dev = CUDA3
llama_kv_cache: layer  12: dev = CUDA3
llama_kv_cache: layer  13: dev = CUDA3
llama_kv_cache: layer  14: dev = CUDA3
llama_kv_cache: layer  15: dev = CUDA4
llama_kv_cache: layer  16: dev = CUDA4
llama_kv_cache: layer  17: dev = CUDA4
llama_kv_cache: layer  18: dev = CUDA4
llama_kv_cache: layer  19: dev = CUDA5
llama_kv_cache: layer  20: dev = CUDA5
llama_kv_cache: layer  21: dev = CUDA5
llama_kv_cache: layer  22: dev = CUDA6
llama_kv_cache: layer  23: dev = CUDA6
llama_kv_cache: layer  24: dev = CUDA6
llama_kv_cache: layer  25: dev = CUDA6
llama_kv_cache: layer  26: dev = CUDA7
llama_kv_cache: layer  27: dev = CUDA7
llama_kv_cache:      CUDA0 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA1 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA2 KV buffer size =    96.00 MiB
llama_kv_cache:      CUDA3 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA4 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA5 KV buffer size =    96.00 MiB
llama_kv_cache:      CUDA6 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA7 KV buffer size =    64.00 MiB
llama_kv_cache: size =  896.00 MiB (  4096 cells,  28 layers,  2/2 seqs), K (f16):  448.00 MiB, V (f16):  448.00 MiB
llama_context: enumerating backends
llama_context: backend_ptrs.size() = 9
llama_context: max_nodes = 2488
llama_context: pipeline parallelism enabled (n_copies=4)
llama_context: reserving full memory module
llama_context: worst-case: n_tokens = 512, n_seqs = 2, n_outputs = 2
graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  2, n_outputs =  512
graph_reserve: reserving a graph for ubatch with n_tokens =    2, n_seqs =  2, n_outputs =    2
graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  2, n_outputs =  512
llama_context:      CUDA0 compute buffer size =   100.04 MiB
llama_context:      CUDA1 compute buffer size =    96.04 MiB
llama_context:      CUDA2 compute buffer size =    96.04 MiB
llama_context:      CUDA3 compute buffer size =    96.04 MiB
llama_context:      CUDA4 compute buffer size =    96.04 MiB
llama_context:      CUDA5 compute buffer size =    96.04 MiB
llama_context:      CUDA6 compute buffer size =    96.04 MiB
llama_context:      CUDA7 compute buffer size =   348.80 MiB
llama_context:  CUDA_Host compute buffer size =    36.05 MiB
llama_context: graph nodes  = 1043
llama_context: graph splits = 9
llama_context: constructing llama_context
llama_context: n_seq_max     = 2
llama_context: n_ctx         = 8192
llama_context: n_ctx_seq     = 4096
llama_context: n_batch       = 2048
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = enabled
llama_context: kv_unified    = false
llama_context: freq_base     = 1000000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_seq (4096) < n_ctx_train (32768) -- the full capacity of the model will not be utilized
set_abort_callback: call
llama_context:  CUDA_Host  output buffer size =     1.16 MiB
llama_kv_cache: layer   0: dev = CUDA0
llama_kv_cache: layer   1: dev = CUDA0
llama_kv_cache: layer   2: dev = CUDA0
llama_kv_cache: layer   3: dev = CUDA0
llama_kv_cache: layer   4: dev = CUDA1
llama_kv_cache: layer   5: dev = CUDA1
llama_kv_cache: layer   6: dev = CUDA1
llama_kv_cache: layer   7: dev = CUDA1
llama_kv_cache: layer   8: dev = CUDA2
llama_kv_cache: layer   9: dev = CUDA2
llama_kv_cache: layer  10: dev = CUDA2
llama_kv_cache: layer  11: dev = CUDA3
llama_kv_cache: layer  12: dev = CUDA3
llama_kv_cache: layer  13: dev = CUDA3
llama_kv_cache: layer  14: dev = CUDA3
llama_kv_cache: layer  15: dev = CUDA4
llama_kv_cache: layer  16: dev = CUDA4
llama_kv_cache: layer  17: dev = CUDA4
llama_kv_cache: layer  18: dev = CUDA4
llama_kv_cache: layer  19: dev = CUDA5
llama_kv_cache: layer  20: dev = CUDA5
llama_kv_cache: layer  21: dev = CUDA5
llama_kv_cache: layer  22: dev = CUDA6
llama_kv_cache: layer  23: dev = CUDA6
llama_kv_cache: layer  24: dev = CUDA6
llama_kv_cache: layer  25: dev = CUDA6
llama_kv_cache: layer  26: dev = CUDA7
llama_kv_cache: layer  27: dev = CUDA7
llama_kv_cache:      CUDA0 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA1 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA2 KV buffer size =    96.00 MiB
llama_kv_cache:      CUDA3 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA4 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA5 KV buffer size =    96.00 MiB
llama_kv_cache:      CUDA6 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA7 KV buffer size =    64.00 MiB
llama_kv_cache: size =  896.00 MiB (  4096 cells,  28 layers,  2/2 seqs), K (f16):  448.00 MiB, V (f16):  448.00 MiB
llama_context: enumerating backends
llama_context: backend_ptrs.size() = 9
llama_context: max_nodes = 2488
llama_context: pipeline parallelism enabled (n_copies=4)
llama_context: reserving full memory module
llama_context: worst-case: n_tokens = 512, n_seqs = 2, n_outputs = 2
graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  2, n_outputs =  512
graph_reserve: reserving a graph for ubatch with n_tokens =    2, n_seqs =  2, n_outputs =    2
graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  2, n_outputs =  512
llama_context:      CUDA0 compute buffer size =   100.04 MiB
llama_context:      CUDA1 compute buffer size =    96.04 MiB
llama_context:      CUDA2 compute buffer size =    96.04 MiB
llama_context:      CUDA3 compute buffer size =    96.04 MiB
llama_context:      CUDA4 compute buffer size =    96.04 MiB
llama_context:      CUDA5 compute buffer size =    96.04 MiB
llama_context:      CUDA6 compute buffer size =    96.04 MiB
llama_context:      CUDA7 compute buffer size =   348.80 MiB
llama_context:  CUDA_Host compute buffer size =    36.05 MiB
llama_context: graph nodes  = 1043
llama_context: graph splits = 9
llama_context: constructing llama_context
llama_context: n_seq_max     = 2
llama_context: n_ctx         = 8192
llama_context: n_ctx_seq     = 4096
llama_context: n_batch       = 2048
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = enabled
llama_context: kv_unified    = false
llama_context: freq_base     = 1000000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_seq (4096) < n_ctx_train (32768) -- the full capacity of the model will not be utilized
set_abort_callback: call
llama_context:  CUDA_Host  output buffer size =     1.16 MiB
llama_kv_cache: layer   0: dev = CUDA0
llama_kv_cache: layer   1: dev = CUDA0
llama_kv_cache: layer   2: dev = CUDA0
llama_kv_cache: layer   3: dev = CUDA0
llama_kv_cache: layer   4: dev = CUDA1
llama_kv_cache: layer   5: dev = CUDA1
llama_kv_cache: layer   6: dev = CUDA1
llama_kv_cache: layer   7: dev = CUDA1
llama_kv_cache: layer   8: dev = CUDA2
llama_kv_cache: layer   9: dev = CUDA2
llama_kv_cache: layer  10: dev = CUDA2
llama_kv_cache: layer  11: dev = CUDA3
llama_kv_cache: layer  12: dev = CUDA3
llama_kv_cache: layer  13: dev = CUDA3
llama_kv_cache: layer  14: dev = CUDA3
llama_kv_cache: layer  15: dev = CUDA4
llama_kv_cache: layer  16: dev = CUDA4
llama_kv_cache: layer  17: dev = CUDA4
llama_kv_cache: layer  18: dev = CUDA4
llama_kv_cache: layer  19: dev = CUDA5
llama_kv_cache: layer  20: dev = CUDA5
llama_kv_cache: layer  21: dev = CUDA5
llama_kv_cache: layer  22: dev = CUDA6
llama_kv_cache: layer  23: dev = CUDA6
llama_kv_cache: layer  24: dev = CUDA6
llama_kv_cache: layer  25: dev = CUDA6
llama_kv_cache: layer  26: dev = CUDA7
llama_kv_cache: layer  27: dev = CUDA7
llama_kv_cache:      CUDA0 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA1 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA2 KV buffer size =    96.00 MiB
llama_kv_cache:      CUDA3 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA4 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA5 KV buffer size =    96.00 MiB
llama_kv_cache:      CUDA6 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA7 KV buffer size =    64.00 MiB
llama_kv_cache: size =  896.00 MiB (  4096 cells,  28 layers,  2/2 seqs), K (f16):  448.00 MiB, V (f16):  448.00 MiB
llama_context: enumerating backends
llama_context: backend_ptrs.size() = 9
llama_context: max_nodes = 2488
llama_context: pipeline parallelism enabled (n_copies=4)
llama_context: reserving full memory module
llama_context: worst-case: n_tokens = 512, n_seqs = 2, n_outputs = 2
graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  2, n_outputs =  512
graph_reserve: reserving a graph for ubatch with n_tokens =    2, n_seqs =  2, n_outputs =    2
graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  2, n_outputs =  512
llama_context:      CUDA0 compute buffer size =   100.04 MiB
llama_context:      CUDA1 compute buffer size =    96.04 MiB
llama_context:      CUDA2 compute buffer size =    96.04 MiB
llama_context:      CUDA3 compute buffer size =    96.04 MiB
llama_context:      CUDA4 compute buffer size =    96.04 MiB
llama_context:      CUDA5 compute buffer size =    96.04 MiB
llama_context:      CUDA6 compute buffer size =    96.04 MiB
llama_context:      CUDA7 compute buffer size =   348.80 MiB
llama_context:  CUDA_Host compute buffer size =    36.05 MiB
llama_context: graph nodes  = 1043
llama_context: graph splits = 9
llama_context: constructing llama_context
llama_context: n_seq_max     = 2
llama_context: n_ctx         = 8192
llama_context: n_ctx_seq     = 4096
llama_context: n_batch       = 2048
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = enabled
llama_context: kv_unified    = false
llama_context: freq_base     = 1000000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_seq (4096) < n_ctx_train (32768) -- the full capacity of the model will not be utilized
set_abort_callback: call
llama_context:  CUDA_Host  output buffer size =     1.16 MiB
llama_kv_cache: layer   0: dev = CUDA0
llama_kv_cache: layer   1: dev = CUDA0
llama_kv_cache: layer   2: dev = CUDA0
llama_kv_cache: layer   3: dev = CUDA0
llama_kv_cache: layer   4: dev = CUDA1
llama_kv_cache: layer   5: dev = CUDA1
llama_kv_cache: layer   6: dev = CUDA1
llama_kv_cache: layer   7: dev = CUDA1
llama_kv_cache: layer   8: dev = CUDA2
llama_kv_cache: layer   9: dev = CUDA2
llama_kv_cache: layer  10: dev = CUDA2
llama_kv_cache: layer  11: dev = CUDA3
llama_kv_cache: layer  12: dev = CUDA3
llama_kv_cache: layer  13: dev = CUDA3
llama_kv_cache: layer  14: dev = CUDA3
llama_kv_cache: layer  15: dev = CUDA4
llama_kv_cache: layer  16: dev = CUDA4
llama_kv_cache: layer  17: dev = CUDA4
llama_kv_cache: layer  18: dev = CUDA4
llama_kv_cache: layer  19: dev = CUDA5
llama_kv_cache: layer  20: dev = CUDA5
llama_kv_cache: layer  21: dev = CUDA5
llama_kv_cache: layer  22: dev = CUDA6
llama_kv_cache: layer  23: dev = CUDA6
llama_kv_cache: layer  24: dev = CUDA6
llama_kv_cache: layer  25: dev = CUDA6
llama_kv_cache: layer  26: dev = CUDA7
llama_kv_cache: layer  27: dev = CUDA7
llama_kv_cache:      CUDA0 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA1 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA2 KV buffer size =    96.00 MiB
llama_kv_cache:      CUDA3 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA4 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA5 KV buffer size =    96.00 MiB
llama_kv_cache:      CUDA6 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA7 KV buffer size =    64.00 MiB
llama_kv_cache: size =  896.00 MiB (  4096 cells,  28 layers,  2/2 seqs), K (f16):  448.00 MiB, V (f16):  448.00 MiB
llama_context: enumerating backends
llama_context: backend_ptrs.size() = 9
llama_context: max_nodes = 2488
llama_context: pipeline parallelism enabled (n_copies=4)
llama_context: reserving full memory module
llama_context: worst-case: n_tokens = 512, n_seqs = 2, n_outputs = 2
graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  2, n_outputs =  512
graph_reserve: reserving a graph for ubatch with n_tokens =    2, n_seqs =  2, n_outputs =    2
graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  2, n_outputs =  512
llama_context:      CUDA0 compute buffer size =   100.04 MiB
llama_context:      CUDA1 compute buffer size =    96.04 MiB
llama_context:      CUDA2 compute buffer size =    96.04 MiB
llama_context:      CUDA3 compute buffer size =    96.04 MiB
llama_context:      CUDA4 compute buffer size =    96.04 MiB
llama_context:      CUDA5 compute buffer size =    96.04 MiB
llama_context:      CUDA6 compute buffer size =    96.04 MiB
llama_context:      CUDA7 compute buffer size =   348.80 MiB
llama_context:  CUDA_Host compute buffer size =    36.05 MiB
llama_context: graph nodes  = 1043
llama_context: graph splits = 9
llama_context: constructing llama_context
llama_context: n_seq_max     = 2
llama_context: n_ctx         = 8192
llama_context: n_ctx_seq     = 4096
llama_context: n_batch       = 2048
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = enabled
llama_context: kv_unified    = false
llama_context: freq_base     = 1000000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_seq (4096) < n_ctx_train (32768) -- the full capacity of the model will not be utilized
set_abort_callback: call
llama_context:  CUDA_Host  output buffer size =     1.16 MiB
llama_kv_cache: layer   0: dev = CUDA0
llama_kv_cache: layer   1: dev = CUDA0
llama_kv_cache: layer   2: dev = CUDA0
llama_kv_cache: layer   3: dev = CUDA0
llama_kv_cache: layer   4: dev = CUDA1
llama_kv_cache: layer   5: dev = CUDA1
llama_kv_cache: layer   6: dev = CUDA1
llama_kv_cache: layer   7: dev = CUDA1
llama_kv_cache: layer   8: dev = CUDA2
llama_kv_cache: layer   9: dev = CUDA2
llama_kv_cache: layer  10: dev = CUDA2
llama_kv_cache: layer  11: dev = CUDA3
llama_kv_cache: layer  12: dev = CUDA3
llama_kv_cache: layer  13: dev = CUDA3
llama_kv_cache: layer  14: dev = CUDA3
llama_kv_cache: layer  15: dev = CUDA4
llama_kv_cache: layer  16: dev = CUDA4
llama_kv_cache: layer  17: dev = CUDA4
llama_kv_cache: layer  18: dev = CUDA4
llama_kv_cache: layer  19: dev = CUDA5
llama_kv_cache: layer  20: dev = CUDA5
llama_kv_cache: layer  21: dev = CUDA5
llama_kv_cache: layer  22: dev = CUDA6
llama_kv_cache: layer  23: dev = CUDA6
llama_kv_cache: layer  24: dev = CUDA6
llama_kv_cache: layer  25: dev = CUDA6
llama_kv_cache: layer  26: dev = CUDA7
llama_kv_cache: layer  27: dev = CUDA7
llama_kv_cache:      CUDA0 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA1 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA2 KV buffer size =    96.00 MiB
llama_kv_cache:      CUDA3 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA4 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA5 KV buffer size =    96.00 MiB
llama_kv_cache:      CUDA6 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA7 KV buffer size =    64.00 MiB
llama_kv_cache: size =  896.00 MiB (  4096 cells,  28 layers,  2/2 seqs), K (f16):  448.00 MiB, V (f16):  448.00 MiB
llama_context: enumerating backends
llama_context: backend_ptrs.size() = 9
llama_context: max_nodes = 2488
llama_context: pipeline parallelism enabled (n_copies=4)
llama_context: reserving full memory module
llama_context: worst-case: n_tokens = 512, n_seqs = 2, n_outputs = 2
graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  2, n_outputs =  512
graph_reserve: reserving a graph for ubatch with n_tokens =    2, n_seqs =  2, n_outputs =    2
graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  2, n_outputs =  512
llama_context:      CUDA0 compute buffer size =   100.04 MiB
llama_context:      CUDA1 compute buffer size =    96.04 MiB
llama_context:      CUDA2 compute buffer size =    96.04 MiB
llama_context:      CUDA3 compute buffer size =    96.04 MiB
llama_context:      CUDA4 compute buffer size =    96.04 MiB
llama_context:      CUDA5 compute buffer size =    96.04 MiB
llama_context:      CUDA6 compute buffer size =    96.04 MiB
llama_context:      CUDA7 compute buffer size =   348.80 MiB
llama_context:  CUDA_Host compute buffer size =    36.05 MiB
llama_context: graph nodes  = 1043
llama_context: graph splits = 9
llama_context: constructing llama_context
llama_context: n_seq_max     = 2
llama_context: n_ctx         = 8192
llama_context: n_ctx_seq     = 4096
llama_context: n_batch       = 2048
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = enabled
llama_context: kv_unified    = false
llama_context: freq_base     = 1000000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_seq (4096) < n_ctx_train (32768) -- the full capacity of the model will not be utilized
set_abort_callback: call
llama_context:  CUDA_Host  output buffer size =     1.16 MiB
llama_kv_cache: layer   0: dev = CUDA0
llama_kv_cache: layer   1: dev = CUDA0
llama_kv_cache: layer   2: dev = CUDA0
llama_kv_cache: layer   3: dev = CUDA0
llama_kv_cache: layer   4: dev = CUDA1
llama_kv_cache: layer   5: dev = CUDA1
llama_kv_cache: layer   6: dev = CUDA1
llama_kv_cache: layer   7: dev = CUDA1
llama_kv_cache: layer   8: dev = CUDA2
llama_kv_cache: layer   9: dev = CUDA2
llama_kv_cache: layer  10: dev = CUDA2
llama_kv_cache: layer  11: dev = CUDA3
llama_kv_cache: layer  12: dev = CUDA3
llama_kv_cache: layer  13: dev = CUDA3
llama_kv_cache: layer  14: dev = CUDA3
llama_kv_cache: layer  15: dev = CUDA4
llama_kv_cache: layer  16: dev = CUDA4
llama_kv_cache: layer  17: dev = CUDA4
llama_kv_cache: layer  18: dev = CUDA4
llama_kv_cache: layer  19: dev = CUDA5
llama_kv_cache: layer  20: dev = CUDA5
llama_kv_cache: layer  21: dev = CUDA5
llama_kv_cache: layer  22: dev = CUDA6
llama_kv_cache: layer  23: dev = CUDA6
llama_kv_cache: layer  24: dev = CUDA6
llama_kv_cache: layer  25: dev = CUDA6
llama_kv_cache: layer  26: dev = CUDA7
llama_kv_cache: layer  27: dev = CUDA7
llama_kv_cache:      CUDA0 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA1 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA2 KV buffer size =    96.00 MiB
llama_kv_cache:      CUDA3 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA4 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA5 KV buffer size =    96.00 MiB
llama_kv_cache:      CUDA6 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA7 KV buffer size =    64.00 MiB
llama_kv_cache: size =  896.00 MiB (  4096 cells,  28 layers,  2/2 seqs), K (f16):  448.00 MiB, V (f16):  448.00 MiB
llama_context: enumerating backends
llama_context: backend_ptrs.size() = 9
llama_context: max_nodes = 2488
llama_context: pipeline parallelism enabled (n_copies=4)
llama_context: reserving full memory module
llama_context: worst-case: n_tokens = 512, n_seqs = 2, n_outputs = 2
graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  2, n_outputs =  512
graph_reserve: reserving a graph for ubatch with n_tokens =    2, n_seqs =  2, n_outputs =    2
graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  2, n_outputs =  512
llama_context:      CUDA0 compute buffer size =   100.04 MiB
llama_context:      CUDA1 compute buffer size =    96.04 MiB
llama_context:      CUDA2 compute buffer size =    96.04 MiB
llama_context:      CUDA3 compute buffer size =    96.04 MiB
llama_context:      CUDA4 compute buffer size =    96.04 MiB
llama_context:      CUDA5 compute buffer size =    96.04 MiB
llama_context:      CUDA6 compute buffer size =    96.04 MiB
llama_context:      CUDA7 compute buffer size =   348.80 MiB
llama_context:  CUDA_Host compute buffer size =    36.05 MiB
llama_context: graph nodes  = 1043
llama_context: graph splits = 9
llama_context: constructing llama_context
llama_context: n_seq_max     = 2
llama_context: n_ctx         = 8192
llama_context: n_ctx_seq     = 4096
llama_context: n_batch       = 2048
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = enabled
llama_context: kv_unified    = false
llama_context: freq_base     = 1000000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_seq (4096) < n_ctx_train (32768) -- the full capacity of the model will not be utilized
set_abort_callback: call
llama_context:  CUDA_Host  output buffer size =     1.16 MiB
llama_kv_cache: layer   0: dev = CUDA0
llama_kv_cache: layer   1: dev = CUDA0
llama_kv_cache: layer   2: dev = CUDA0
llama_kv_cache: layer   3: dev = CUDA0
llama_kv_cache: layer   4: dev = CUDA1
llama_kv_cache: layer   5: dev = CUDA1
llama_kv_cache: layer   6: dev = CUDA1
llama_kv_cache: layer   7: dev = CUDA1
llama_kv_cache: layer   8: dev = CUDA2
llama_kv_cache: layer   9: dev = CUDA2
llama_kv_cache: layer  10: dev = CUDA2
llama_kv_cache: layer  11: dev = CUDA3
llama_kv_cache: layer  12: dev = CUDA3
llama_kv_cache: layer  13: dev = CUDA3
llama_kv_cache: layer  14: dev = CUDA3
llama_kv_cache: layer  15: dev = CUDA4
llama_kv_cache: layer  16: dev = CUDA4
llama_kv_cache: layer  17: dev = CUDA4
llama_kv_cache: layer  18: dev = CUDA4
llama_kv_cache: layer  19: dev = CUDA5
llama_kv_cache: layer  20: dev = CUDA5
llama_kv_cache: layer  21: dev = CUDA5
llama_kv_cache: layer  22: dev = CUDA6
llama_kv_cache: layer  23: dev = CUDA6
llama_kv_cache: layer  24: dev = CUDA6
llama_kv_cache: layer  25: dev = CUDA6
llama_kv_cache: layer  26: dev = CUDA7
llama_kv_cache: layer  27: dev = CUDA7
llama_kv_cache:      CUDA0 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA1 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA2 KV buffer size =    96.00 MiB
llama_kv_cache:      CUDA3 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA4 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA5 KV buffer size =    96.00 MiB
llama_kv_cache:      CUDA6 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA7 KV buffer size =    64.00 MiB
llama_kv_cache: size =  896.00 MiB (  4096 cells,  28 layers,  2/2 seqs), K (f16):  448.00 MiB, V (f16):  448.00 MiB
llama_context: enumerating backends
llama_context: backend_ptrs.size() = 9
llama_context: max_nodes = 2488
llama_context: pipeline parallelism enabled (n_copies=4)
llama_context: reserving full memory module
llama_context: worst-case: n_tokens = 512, n_seqs = 2, n_outputs = 2
graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  2, n_outputs =  512
graph_reserve: reserving a graph for ubatch with n_tokens =    2, n_seqs =  2, n_outputs =    2
graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  2, n_outputs =  512
llama_context:      CUDA0 compute buffer size =   100.04 MiB
llama_context:      CUDA1 compute buffer size =    96.04 MiB
llama_context:      CUDA2 compute buffer size =    96.04 MiB
llama_context:      CUDA3 compute buffer size =    96.04 MiB
llama_context:      CUDA4 compute buffer size =    96.04 MiB
llama_context:      CUDA5 compute buffer size =    96.04 MiB
llama_context:      CUDA6 compute buffer size =    96.04 MiB
llama_context:      CUDA7 compute buffer size =   348.80 MiB
llama_context:  CUDA_Host compute buffer size =    36.05 MiB
llama_context: graph nodes  = 1043
llama_context: graph splits = 9
llama_context: constructing llama_context
llama_context: n_seq_max     = 2
llama_context: n_ctx         = 8192
llama_context: n_ctx_seq     = 4096
llama_context: n_batch       = 2048
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = enabled
llama_context: kv_unified    = false
llama_context: freq_base     = 1000000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_seq (4096) < n_ctx_train (32768) -- the full capacity of the model will not be utilized
set_abort_callback: call
llama_context:  CUDA_Host  output buffer size =     1.16 MiB
llama_kv_cache: layer   0: dev = CUDA0
llama_kv_cache: layer   1: dev = CUDA0
llama_kv_cache: layer   2: dev = CUDA0
llama_kv_cache: layer   3: dev = CUDA0
llama_kv_cache: layer   4: dev = CUDA1
llama_kv_cache: layer   5: dev = CUDA1
llama_kv_cache: layer   6: dev = CUDA1
llama_kv_cache: layer   7: dev = CUDA1
llama_kv_cache: layer   8: dev = CUDA2
llama_kv_cache: layer   9: dev = CUDA2
llama_kv_cache: layer  10: dev = CUDA2
llama_kv_cache: layer  11: dev = CUDA3
llama_kv_cache: layer  12: dev = CUDA3
llama_kv_cache: layer  13: dev = CUDA3
llama_kv_cache: layer  14: dev = CUDA3
llama_kv_cache: layer  15: dev = CUDA4
llama_kv_cache: layer  16: dev = CUDA4
llama_kv_cache: layer  17: dev = CUDA4
llama_kv_cache: layer  18: dev = CUDA4
llama_kv_cache: layer  19: dev = CUDA5
llama_kv_cache: layer  20: dev = CUDA5
llama_kv_cache: layer  21: dev = CUDA5
llama_kv_cache: layer  22: dev = CUDA6
llama_kv_cache: layer  23: dev = CUDA6
llama_kv_cache: layer  24: dev = CUDA6
llama_kv_cache: layer  25: dev = CUDA6
llama_kv_cache: layer  26: dev = CUDA7
llama_kv_cache: layer  27: dev = CUDA7
llama_kv_cache:      CUDA0 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA1 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA2 KV buffer size =    96.00 MiB
llama_kv_cache:      CUDA3 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA4 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA5 KV buffer size =    96.00 MiB
llama_kv_cache:      CUDA6 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA7 KV buffer size =    64.00 MiB
llama_kv_cache: size =  896.00 MiB (  4096 cells,  28 layers,  2/2 seqs), K (f16):  448.00 MiB, V (f16):  448.00 MiB
llama_context: enumerating backends
llama_context: backend_ptrs.size() = 9
llama_context: max_nodes = 2488
llama_context: pipeline parallelism enabled (n_copies=4)
llama_context: reserving full memory module
llama_context: worst-case: n_tokens = 512, n_seqs = 2, n_outputs = 2
graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  2, n_outputs =  512
graph_reserve: reserving a graph for ubatch with n_tokens =    2, n_seqs =  2, n_outputs =    2
graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  2, n_outputs =  512
llama_context:      CUDA0 compute buffer size =   100.04 MiB
llama_context:      CUDA1 compute buffer size =    96.04 MiB
llama_context:      CUDA2 compute buffer size =    96.04 MiB
llama_context:      CUDA3 compute buffer size =    96.04 MiB
llama_context:      CUDA4 compute buffer size =    96.04 MiB
llama_context:      CUDA5 compute buffer size =    96.04 MiB
llama_context:      CUDA6 compute buffer size =    96.04 MiB
llama_context:      CUDA7 compute buffer size =   348.80 MiB
llama_context:  CUDA_Host compute buffer size =    36.05 MiB
llama_context: graph nodes  = 1043
llama_context: graph splits = 9
llama_context: constructing llama_context
llama_context: n_seq_max     = 2
llama_context: n_ctx         = 8192
llama_context: n_ctx_seq     = 4096
llama_context: n_batch       = 2048
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = enabled
llama_context: kv_unified    = false
llama_context: freq_base     = 1000000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_seq (4096) < n_ctx_train (32768) -- the full capacity of the model will not be utilized
set_abort_callback: call
llama_context:  CUDA_Host  output buffer size =     1.16 MiB
llama_kv_cache: layer   0: dev = CUDA0
llama_kv_cache: layer   1: dev = CUDA0
llama_kv_cache: layer   2: dev = CUDA0
llama_kv_cache: layer   3: dev = CUDA0
llama_kv_cache: layer   4: dev = CUDA1
llama_kv_cache: layer   5: dev = CUDA1
llama_kv_cache: layer   6: dev = CUDA1
llama_kv_cache: layer   7: dev = CUDA1
llama_kv_cache: layer   8: dev = CUDA2
llama_kv_cache: layer   9: dev = CUDA2
llama_kv_cache: layer  10: dev = CUDA2
llama_kv_cache: layer  11: dev = CUDA3
llama_kv_cache: layer  12: dev = CUDA3
llama_kv_cache: layer  13: dev = CUDA3
llama_kv_cache: layer  14: dev = CUDA3
llama_kv_cache: layer  15: dev = CUDA4
llama_kv_cache: layer  16: dev = CUDA4
llama_kv_cache: layer  17: dev = CUDA4
llama_kv_cache: layer  18: dev = CUDA4
llama_kv_cache: layer  19: dev = CUDA5
llama_kv_cache: layer  20: dev = CUDA5
llama_kv_cache: layer  21: dev = CUDA5
llama_kv_cache: layer  22: dev = CUDA6
llama_kv_cache: layer  23: dev = CUDA6
llama_kv_cache: layer  24: dev = CUDA6
llama_kv_cache: layer  25: dev = CUDA6
llama_kv_cache: layer  26: dev = CUDA7
llama_kv_cache: layer  27: dev = CUDA7
llama_kv_cache:      CUDA0 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA1 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA2 KV buffer size =    96.00 MiB
llama_kv_cache:      CUDA3 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA4 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA5 KV buffer size =    96.00 MiB
llama_kv_cache:      CUDA6 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA7 KV buffer size =    64.00 MiB
llama_kv_cache: size =  896.00 MiB (  4096 cells,  28 layers,  2/2 seqs), K (f16):  448.00 MiB, V (f16):  448.00 MiB
llama_context: enumerating backends
llama_context: backend_ptrs.size() = 9
llama_context: max_nodes = 2488
llama_context: pipeline parallelism enabled (n_copies=4)
llama_context: reserving full memory module
llama_context: worst-case: n_tokens = 512, n_seqs = 2, n_outputs = 2
graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  2, n_outputs =  512
graph_reserve: reserving a graph for ubatch with n_tokens =    2, n_seqs =  2, n_outputs =    2
graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  2, n_outputs =  512
llama_context:      CUDA0 compute buffer size =   100.04 MiB
llama_context:      CUDA1 compute buffer size =    96.04 MiB
llama_context:      CUDA2 compute buffer size =    96.04 MiB
llama_context:      CUDA3 compute buffer size =    96.04 MiB
llama_context:      CUDA4 compute buffer size =    96.04 MiB
llama_context:      CUDA5 compute buffer size =    96.04 MiB
llama_context:      CUDA6 compute buffer size =    96.04 MiB
llama_context:      CUDA7 compute buffer size =   348.80 MiB
llama_context:  CUDA_Host compute buffer size =    36.05 MiB
llama_context: graph nodes  = 1043
llama_context: graph splits = 9
llama_context: constructing llama_context
llama_context: n_seq_max     = 2
llama_context: n_ctx         = 8192
llama_context: n_ctx_seq     = 4096
llama_context: n_batch       = 2048
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = enabled
llama_context: kv_unified    = false
llama_context: freq_base     = 1000000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_seq (4096) < n_ctx_train (32768) -- the full capacity of the model will not be utilized
set_abort_callback: call
llama_context:  CUDA_Host  output buffer size =     1.16 MiB
llama_kv_cache: layer   0: dev = CUDA0
llama_kv_cache: layer   1: dev = CUDA0
llama_kv_cache: layer   2: dev = CUDA0
llama_kv_cache: layer   3: dev = CUDA0
llama_kv_cache: layer   4: dev = CUDA1
llama_kv_cache: layer   5: dev = CUDA1
llama_kv_cache: layer   6: dev = CUDA1
llama_kv_cache: layer   7: dev = CUDA1
llama_kv_cache: layer   8: dev = CUDA2
llama_kv_cache: layer   9: dev = CUDA2
llama_kv_cache: layer  10: dev = CUDA2
llama_kv_cache: layer  11: dev = CUDA3
llama_kv_cache: layer  12: dev = CUDA3
llama_kv_cache: layer  13: dev = CUDA3
llama_kv_cache: layer  14: dev = CUDA3
llama_kv_cache: layer  15: dev = CUDA4
llama_kv_cache: layer  16: dev = CUDA4
llama_kv_cache: layer  17: dev = CUDA4
llama_kv_cache: layer  18: dev = CUDA4
llama_kv_cache: layer  19: dev = CUDA5
llama_kv_cache: layer  20: dev = CUDA5
llama_kv_cache: layer  21: dev = CUDA5
llama_kv_cache: layer  22: dev = CUDA6
llama_kv_cache: layer  23: dev = CUDA6
llama_kv_cache: layer  24: dev = CUDA6
llama_kv_cache: layer  25: dev = CUDA6
llama_kv_cache: layer  26: dev = CUDA7
llama_kv_cache: layer  27: dev = CUDA7
llama_kv_cache:      CUDA0 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA1 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA2 KV buffer size =    96.00 MiB
llama_kv_cache:      CUDA3 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA4 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA5 KV buffer size =    96.00 MiB
llama_kv_cache:      CUDA6 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA7 KV buffer size =    64.00 MiB
llama_kv_cache: size =  896.00 MiB (  4096 cells,  28 layers,  2/2 seqs), K (f16):  448.00 MiB, V (f16):  448.00 MiB
llama_context: enumerating backends
llama_context: backend_ptrs.size() = 9
llama_context: max_nodes = 2488
llama_context: pipeline parallelism enabled (n_copies=4)
llama_context: reserving full memory module
llama_context: worst-case: n_tokens = 512, n_seqs = 2, n_outputs = 2
graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  2, n_outputs =  512
graph_reserve: reserving a graph for ubatch with n_tokens =    2, n_seqs =  2, n_outputs =    2
graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  2, n_outputs =  512
llama_context:      CUDA0 compute buffer size =   100.04 MiB
llama_context:      CUDA1 compute buffer size =    96.04 MiB
llama_context:      CUDA2 compute buffer size =    96.04 MiB
llama_context:      CUDA3 compute buffer size =    96.04 MiB
llama_context:      CUDA4 compute buffer size =    96.04 MiB
llama_context:      CUDA5 compute buffer size =    96.04 MiB
llama_context:      CUDA6 compute buffer size =    96.04 MiB
llama_context:      CUDA7 compute buffer size =   348.80 MiB
llama_context:  CUDA_Host compute buffer size =    36.05 MiB
llama_context: graph nodes  = 1043
llama_context: graph splits = 9
llama_context: constructing llama_context
llama_context: n_seq_max     = 2
llama_context: n_ctx         = 8192
llama_context: n_ctx_seq     = 4096
llama_context: n_batch       = 2048
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = enabled
llama_context: kv_unified    = false
llama_context: freq_base     = 1000000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_seq (4096) < n_ctx_train (32768) -- the full capacity of the model will not be utilized
set_abort_callback: call
llama_context:  CUDA_Host  output buffer size =     1.16 MiB
llama_kv_cache: layer   0: dev = CUDA0
llama_kv_cache: layer   1: dev = CUDA0
llama_kv_cache: layer   2: dev = CUDA0
llama_kv_cache: layer   3: dev = CUDA0
llama_kv_cache: layer   4: dev = CUDA1
llama_kv_cache: layer   5: dev = CUDA1
llama_kv_cache: layer   6: dev = CUDA1
llama_kv_cache: layer   7: dev = CUDA1
llama_kv_cache: layer   8: dev = CUDA2
llama_kv_cache: layer   9: dev = CUDA2
llama_kv_cache: layer  10: dev = CUDA2
llama_kv_cache: layer  11: dev = CUDA3
llama_kv_cache: layer  12: dev = CUDA3
llama_kv_cache: layer  13: dev = CUDA3
llama_kv_cache: layer  14: dev = CUDA3
llama_kv_cache: layer  15: dev = CUDA4
llama_kv_cache: layer  16: dev = CUDA4
llama_kv_cache: layer  17: dev = CUDA4
llama_kv_cache: layer  18: dev = CUDA4
llama_kv_cache: layer  19: dev = CUDA5
llama_kv_cache: layer  20: dev = CUDA5
llama_kv_cache: layer  21: dev = CUDA5
llama_kv_cache: layer  22: dev = CUDA6
llama_kv_cache: layer  23: dev = CUDA6
llama_kv_cache: layer  24: dev = CUDA6
llama_kv_cache: layer  25: dev = CUDA6
llama_kv_cache: layer  26: dev = CUDA7
llama_kv_cache: layer  27: dev = CUDA7
llama_kv_cache:      CUDA0 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA1 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA2 KV buffer size =    96.00 MiB
llama_kv_cache:      CUDA3 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA4 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA5 KV buffer size =    96.00 MiB
llama_kv_cache:      CUDA6 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA7 KV buffer size =    64.00 MiB
llama_kv_cache: size =  896.00 MiB (  4096 cells,  28 layers,  2/2 seqs), K (f16):  448.00 MiB, V (f16):  448.00 MiB
llama_context: enumerating backends
llama_context: backend_ptrs.size() = 9
llama_context: max_nodes = 2488
llama_context: pipeline parallelism enabled (n_copies=4)
llama_context: reserving full memory module
llama_context: worst-case: n_tokens = 512, n_seqs = 2, n_outputs = 2
graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  2, n_outputs =  512
graph_reserve: reserving a graph for ubatch with n_tokens =    2, n_seqs =  2, n_outputs =    2
graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  2, n_outputs =  512
llama_context:      CUDA0 compute buffer size =   100.04 MiB
llama_context:      CUDA1 compute buffer size =    96.04 MiB
llama_context:      CUDA2 compute buffer size =    96.04 MiB
llama_context:      CUDA3 compute buffer size =    96.04 MiB
llama_context:      CUDA4 compute buffer size =    96.04 MiB
llama_context:      CUDA5 compute buffer size =    96.04 MiB
llama_context:      CUDA6 compute buffer size =    96.04 MiB
llama_context:      CUDA7 compute buffer size =   348.80 MiB
llama_context:  CUDA_Host compute buffer size =    36.05 MiB
llama_context: graph nodes  = 1043
llama_context: graph splits = 9
llama_context: constructing llama_context
llama_context: n_seq_max     = 2
llama_context: n_ctx         = 8192
llama_context: n_ctx_seq     = 4096
llama_context: n_batch       = 2048
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = enabled
llama_context: kv_unified    = false
llama_context: freq_base     = 1000000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_seq (4096) < n_ctx_train (32768) -- the full capacity of the model will not be utilized
set_abort_callback: call
llama_context:  CUDA_Host  output buffer size =     1.16 MiB
llama_kv_cache: layer   0: dev = CUDA0
llama_kv_cache: layer   1: dev = CUDA0
llama_kv_cache: layer   2: dev = CUDA0
llama_kv_cache: layer   3: dev = CUDA0
llama_kv_cache: layer   4: dev = CUDA1
llama_kv_cache: layer   5: dev = CUDA1
llama_kv_cache: layer   6: dev = CUDA1
llama_kv_cache: layer   7: dev = CUDA1
llama_kv_cache: layer   8: dev = CUDA2
llama_kv_cache: layer   9: dev = CUDA2
llama_kv_cache: layer  10: dev = CUDA2
llama_kv_cache: layer  11: dev = CUDA3
llama_kv_cache: layer  12: dev = CUDA3
llama_kv_cache: layer  13: dev = CUDA3
llama_kv_cache: layer  14: dev = CUDA3
llama_kv_cache: layer  15: dev = CUDA4
llama_kv_cache: layer  16: dev = CUDA4
llama_kv_cache: layer  17: dev = CUDA4
llama_kv_cache: layer  18: dev = CUDA4
llama_kv_cache: layer  19: dev = CUDA5
llama_kv_cache: layer  20: dev = CUDA5
llama_kv_cache: layer  21: dev = CUDA5
llama_kv_cache: layer  22: dev = CUDA6
llama_kv_cache: layer  23: dev = CUDA6
llama_kv_cache: layer  24: dev = CUDA6
llama_kv_cache: layer  25: dev = CUDA6
llama_kv_cache: layer  26: dev = CUDA7
llama_kv_cache: layer  27: dev = CUDA7
llama_kv_cache:      CUDA0 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA1 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA2 KV buffer size =    96.00 MiB
llama_kv_cache:      CUDA3 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA4 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA5 KV buffer size =    96.00 MiB
llama_kv_cache:      CUDA6 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA7 KV buffer size =    64.00 MiB
llama_kv_cache: size =  896.00 MiB (  4096 cells,  28 layers,  2/2 seqs), K (f16):  448.00 MiB, V (f16):  448.00 MiB
llama_context: enumerating backends
llama_context: backend_ptrs.size() = 9
llama_context: max_nodes = 2488
llama_context: pipeline parallelism enabled (n_copies=4)
llama_context: reserving full memory module
llama_context: worst-case: n_tokens = 512, n_seqs = 2, n_outputs = 2
graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  2, n_outputs =  512
graph_reserve: reserving a graph for ubatch with n_tokens =    2, n_seqs =  2, n_outputs =    2
graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  2, n_outputs =  512
llama_context:      CUDA0 compute buffer size =   100.04 MiB
llama_context:      CUDA1 compute buffer size =    96.04 MiB
llama_context:      CUDA2 compute buffer size =    96.04 MiB
llama_context:      CUDA3 compute buffer size =    96.04 MiB
llama_context:      CUDA4 compute buffer size =    96.04 MiB
llama_context:      CUDA5 compute buffer size =    96.04 MiB
llama_context:      CUDA6 compute buffer size =    96.04 MiB
llama_context:      CUDA7 compute buffer size =   348.80 MiB
llama_context:  CUDA_Host compute buffer size =    36.05 MiB
llama_context: graph nodes  = 1043
llama_context: graph splits = 9
llama_context: constructing llama_context
llama_context: n_seq_max     = 2
llama_context: n_ctx         = 8192
llama_context: n_ctx_seq     = 4096
llama_context: n_batch       = 2048
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = enabled
llama_context: kv_unified    = false
llama_context: freq_base     = 1000000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_seq (4096) < n_ctx_train (32768) -- the full capacity of the model will not be utilized
set_abort_callback: call
llama_context:  CUDA_Host  output buffer size =     1.16 MiB
llama_kv_cache: layer   0: dev = CUDA0
llama_kv_cache: layer   1: dev = CUDA0
llama_kv_cache: layer   2: dev = CUDA0
llama_kv_cache: layer   3: dev = CUDA0
llama_kv_cache: layer   4: dev = CUDA1
llama_kv_cache: layer   5: dev = CUDA1
llama_kv_cache: layer   6: dev = CUDA1
llama_kv_cache: layer   7: dev = CUDA1
llama_kv_cache: layer   8: dev = CUDA2
llama_kv_cache: layer   9: dev = CUDA2
llama_kv_cache: layer  10: dev = CUDA2
llama_kv_cache: layer  11: dev = CUDA3
llama_kv_cache: layer  12: dev = CUDA3
llama_kv_cache: layer  13: dev = CUDA3
llama_kv_cache: layer  14: dev = CUDA3
llama_kv_cache: layer  15: dev = CUDA4
llama_kv_cache: layer  16: dev = CUDA4
llama_kv_cache: layer  17: dev = CUDA4
llama_kv_cache: layer  18: dev = CUDA4
llama_kv_cache: layer  19: dev = CUDA5
llama_kv_cache: layer  20: dev = CUDA5
llama_kv_cache: layer  21: dev = CUDA5
llama_kv_cache: layer  22: dev = CUDA6
llama_kv_cache: layer  23: dev = CUDA6
llama_kv_cache: layer  24: dev = CUDA6
llama_kv_cache: layer  25: dev = CUDA6
llama_kv_cache: layer  26: dev = CUDA7
llama_kv_cache: layer  27: dev = CUDA7
llama_kv_cache:      CUDA0 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA1 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA2 KV buffer size =    96.00 MiB
llama_kv_cache:      CUDA3 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA4 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA5 KV buffer size =    96.00 MiB
llama_kv_cache:      CUDA6 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA7 KV buffer size =    64.00 MiB
llama_kv_cache: size =  896.00 MiB (  4096 cells,  28 layers,  2/2 seqs), K (f16):  448.00 MiB, V (f16):  448.00 MiB
llama_context: enumerating backends
llama_context: backend_ptrs.size() = 9
llama_context: max_nodes = 2488
llama_context: pipeline parallelism enabled (n_copies=4)
llama_context: reserving full memory module
llama_context: worst-case: n_tokens = 512, n_seqs = 2, n_outputs = 2
graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  2, n_outputs =  512
graph_reserve: reserving a graph for ubatch with n_tokens =    2, n_seqs =  2, n_outputs =    2
graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  2, n_outputs =  512
llama_context:      CUDA0 compute buffer size =   100.04 MiB
llama_context:      CUDA1 compute buffer size =    96.04 MiB
llama_context:      CUDA2 compute buffer size =    96.04 MiB
llama_context:      CUDA3 compute buffer size =    96.04 MiB
llama_context:      CUDA4 compute buffer size =    96.04 MiB
llama_context:      CUDA5 compute buffer size =    96.04 MiB
llama_context:      CUDA6 compute buffer size =    96.04 MiB
llama_context:      CUDA7 compute buffer size =   348.80 MiB
llama_context:  CUDA_Host compute buffer size =    36.05 MiB
llama_context: graph nodes  = 1043
llama_context: graph splits = 9
llama_context: constructing llama_context
llama_context: n_seq_max     = 2
llama_context: n_ctx         = 8192
llama_context: n_ctx_seq     = 4096
llama_context: n_batch       = 2048
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = enabled
llama_context: kv_unified    = false
llama_context: freq_base     = 1000000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_seq (4096) < n_ctx_train (32768) -- the full capacity of the model will not be utilized
set_abort_callback: call
llama_context:  CUDA_Host  output buffer size =     1.16 MiB
llama_kv_cache: layer   0: dev = CUDA0
llama_kv_cache: layer   1: dev = CUDA0
llama_kv_cache: layer   2: dev = CUDA0
llama_kv_cache: layer   3: dev = CUDA0
llama_kv_cache: layer   4: dev = CUDA1
llama_kv_cache: layer   5: dev = CUDA1
llama_kv_cache: layer   6: dev = CUDA1
llama_kv_cache: layer   7: dev = CUDA1
llama_kv_cache: layer   8: dev = CUDA2
llama_kv_cache: layer   9: dev = CUDA2
llama_kv_cache: layer  10: dev = CUDA2
llama_kv_cache: layer  11: dev = CUDA3
llama_kv_cache: layer  12: dev = CUDA3
llama_kv_cache: layer  13: dev = CUDA3
llama_kv_cache: layer  14: dev = CUDA3
llama_kv_cache: layer  15: dev = CUDA4
llama_kv_cache: layer  16: dev = CUDA4
llama_kv_cache: layer  17: dev = CUDA4
llama_kv_cache: layer  18: dev = CUDA4
llama_kv_cache: layer  19: dev = CUDA5
llama_kv_cache: layer  20: dev = CUDA5
llama_kv_cache: layer  21: dev = CUDA5
llama_kv_cache: layer  22: dev = CUDA6
llama_kv_cache: layer  23: dev = CUDA6
llama_kv_cache: layer  24: dev = CUDA6
llama_kv_cache: layer  25: dev = CUDA6
llama_kv_cache: layer  26: dev = CUDA7
llama_kv_cache: layer  27: dev = CUDA7
llama_kv_cache:      CUDA0 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA1 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA2 KV buffer size =    96.00 MiB
llama_kv_cache:      CUDA3 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA4 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA5 KV buffer size =    96.00 MiB
llama_kv_cache:      CUDA6 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA7 KV buffer size =    64.00 MiB
llama_kv_cache: size =  896.00 MiB (  4096 cells,  28 layers,  2/2 seqs), K (f16):  448.00 MiB, V (f16):  448.00 MiB
llama_context: enumerating backends
llama_context: backend_ptrs.size() = 9
llama_context: max_nodes = 2488
llama_context: pipeline parallelism enabled (n_copies=4)
llama_context: reserving full memory module
llama_context: worst-case: n_tokens = 512, n_seqs = 2, n_outputs = 2
graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  2, n_outputs =  512
graph_reserve: reserving a graph for ubatch with n_tokens =    2, n_seqs =  2, n_outputs =    2
graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  2, n_outputs =  512
llama_context:      CUDA0 compute buffer size =   100.04 MiB
llama_context:      CUDA1 compute buffer size =    96.04 MiB
llama_context:      CUDA2 compute buffer size =    96.04 MiB
llama_context:      CUDA3 compute buffer size =    96.04 MiB
llama_context:      CUDA4 compute buffer size =    96.04 MiB
llama_context:      CUDA5 compute buffer size =    96.04 MiB
llama_context:      CUDA6 compute buffer size =    96.04 MiB
llama_context:      CUDA7 compute buffer size =   348.80 MiB
llama_context:  CUDA_Host compute buffer size =    36.05 MiB
llama_context: graph nodes  = 1043
llama_context: graph splits = 9
llama_context: constructing llama_context
llama_context: n_seq_max     = 2
llama_context: n_ctx         = 8192
llama_context: n_ctx_seq     = 4096
llama_context: n_batch       = 2048
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = enabled
llama_context: kv_unified    = false
llama_context: freq_base     = 1000000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_seq (4096) < n_ctx_train (32768) -- the full capacity of the model will not be utilized
set_abort_callback: call
llama_context:  CUDA_Host  output buffer size =     1.16 MiB
llama_kv_cache: layer   0: dev = CUDA0
llama_kv_cache: layer   1: dev = CUDA0
llama_kv_cache: layer   2: dev = CUDA0
llama_kv_cache: layer   3: dev = CUDA0
llama_kv_cache: layer   4: dev = CUDA1
llama_kv_cache: layer   5: dev = CUDA1
llama_kv_cache: layer   6: dev = CUDA1
llama_kv_cache: layer   7: dev = CUDA1
llama_kv_cache: layer   8: dev = CUDA2
llama_kv_cache: layer   9: dev = CUDA2
llama_kv_cache: layer  10: dev = CUDA2
llama_kv_cache: layer  11: dev = CUDA3
llama_kv_cache: layer  12: dev = CUDA3
llama_kv_cache: layer  13: dev = CUDA3
llama_kv_cache: layer  14: dev = CUDA3
llama_kv_cache: layer  15: dev = CUDA4
llama_kv_cache: layer  16: dev = CUDA4
llama_kv_cache: layer  17: dev = CUDA4
llama_kv_cache: layer  18: dev = CUDA4
llama_kv_cache: layer  19: dev = CUDA5
llama_kv_cache: layer  20: dev = CUDA5
llama_kv_cache: layer  21: dev = CUDA5
llama_kv_cache: layer  22: dev = CUDA6
llama_kv_cache: layer  23: dev = CUDA6
llama_kv_cache: layer  24: dev = CUDA6
llama_kv_cache: layer  25: dev = CUDA6
llama_kv_cache: layer  26: dev = CUDA7
llama_kv_cache: layer  27: dev = CUDA7
llama_kv_cache:      CUDA0 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA1 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA2 KV buffer size =    96.00 MiB
llama_kv_cache:      CUDA3 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA4 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA5 KV buffer size =    96.00 MiB
llama_kv_cache:      CUDA6 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA7 KV buffer size =    64.00 MiB
llama_kv_cache: size =  896.00 MiB (  4096 cells,  28 layers,  2/2 seqs), K (f16):  448.00 MiB, V (f16):  448.00 MiB
llama_context: enumerating backends
llama_context: backend_ptrs.size() = 9
llama_context: max_nodes = 2488
llama_context: pipeline parallelism enabled (n_copies=4)
llama_context: reserving full memory module
llama_context: worst-case: n_tokens = 512, n_seqs = 2, n_outputs = 2
graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  2, n_outputs =  512
graph_reserve: reserving a graph for ubatch with n_tokens =    2, n_seqs =  2, n_outputs =    2
graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  2, n_outputs =  512
llama_context:      CUDA0 compute buffer size =   100.04 MiB
llama_context:      CUDA1 compute buffer size =    96.04 MiB
llama_context:      CUDA2 compute buffer size =    96.04 MiB
llama_context:      CUDA3 compute buffer size =    96.04 MiB
llama_context:      CUDA4 compute buffer size =    96.04 MiB
llama_context:      CUDA5 compute buffer size =    96.04 MiB
llama_context:      CUDA6 compute buffer size =    96.04 MiB
llama_context:      CUDA7 compute buffer size =   348.80 MiB
llama_context:  CUDA_Host compute buffer size =    36.05 MiB
llama_context: graph nodes  = 1043
llama_context: graph splits = 9
llama_context: constructing llama_context
llama_context: n_seq_max     = 2
llama_context: n_ctx         = 8192
llama_context: n_ctx_seq     = 4096
llama_context: n_batch       = 2048
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = enabled
llama_context: kv_unified    = false
llama_context: freq_base     = 1000000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_seq (4096) < n_ctx_train (32768) -- the full capacity of the model will not be utilized
set_abort_callback: call
llama_context:  CUDA_Host  output buffer size =     1.16 MiB
llama_kv_cache: layer   0: dev = CUDA0
llama_kv_cache: layer   1: dev = CUDA0
llama_kv_cache: layer   2: dev = CUDA0
llama_kv_cache: layer   3: dev = CUDA0
llama_kv_cache: layer   4: dev = CUDA1
llama_kv_cache: layer   5: dev = CUDA1
llama_kv_cache: layer   6: dev = CUDA1
llama_kv_cache: layer   7: dev = CUDA1
llama_kv_cache: layer   8: dev = CUDA2
llama_kv_cache: layer   9: dev = CUDA2
llama_kv_cache: layer  10: dev = CUDA2
llama_kv_cache: layer  11: dev = CUDA3
llama_kv_cache: layer  12: dev = CUDA3
llama_kv_cache: layer  13: dev = CUDA3
llama_kv_cache: layer  14: dev = CUDA3
llama_kv_cache: layer  15: dev = CUDA4
llama_kv_cache: layer  16: dev = CUDA4
llama_kv_cache: layer  17: dev = CUDA4
llama_kv_cache: layer  18: dev = CUDA4
llama_kv_cache: layer  19: dev = CUDA5
llama_kv_cache: layer  20: dev = CUDA5
llama_kv_cache: layer  21: dev = CUDA5
llama_kv_cache: layer  22: dev = CUDA6
llama_kv_cache: layer  23: dev = CUDA6
llama_kv_cache: layer  24: dev = CUDA6
llama_kv_cache: layer  25: dev = CUDA6
llama_kv_cache: layer  26: dev = CUDA7
llama_kv_cache: layer  27: dev = CUDA7
llama_kv_cache:      CUDA0 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA1 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA2 KV buffer size =    96.00 MiB
llama_kv_cache:      CUDA3 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA4 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA5 KV buffer size =    96.00 MiB
llama_kv_cache:      CUDA6 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA7 KV buffer size =    64.00 MiB
llama_kv_cache: size =  896.00 MiB (  4096 cells,  28 layers,  2/2 seqs), K (f16):  448.00 MiB, V (f16):  448.00 MiB
llama_context: enumerating backends
llama_context: backend_ptrs.size() = 9
llama_context: max_nodes = 2488
llama_context: pipeline parallelism enabled (n_copies=4)
llama_context: reserving full memory module
llama_context: worst-case: n_tokens = 512, n_seqs = 2, n_outputs = 2
graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  2, n_outputs =  512
graph_reserve: reserving a graph for ubatch with n_tokens =    2, n_seqs =  2, n_outputs =    2
graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  2, n_outputs =  512
llama_context:      CUDA0 compute buffer size =   100.04 MiB
llama_context:      CUDA1 compute buffer size =    96.04 MiB
llama_context:      CUDA2 compute buffer size =    96.04 MiB
llama_context:      CUDA3 compute buffer size =    96.04 MiB
llama_context:      CUDA4 compute buffer size =    96.04 MiB
llama_context:      CUDA5 compute buffer size =    96.04 MiB
llama_context:      CUDA6 compute buffer size =    96.04 MiB
llama_context:      CUDA7 compute buffer size =   348.80 MiB
llama_context:  CUDA_Host compute buffer size =    36.05 MiB
llama_context: graph nodes  = 1043
llama_context: graph splits = 9
llama_context: constructing llama_context
llama_context: n_seq_max     = 2
llama_context: n_ctx         = 8192
llama_context: n_ctx_seq     = 4096
llama_context: n_batch       = 2048
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = enabled
llama_context: kv_unified    = false
llama_context: freq_base     = 1000000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_seq (4096) < n_ctx_train (32768) -- the full capacity of the model will not be utilized
set_abort_callback: call
llama_context:  CUDA_Host  output buffer size =     1.16 MiB
llama_kv_cache: layer   0: dev = CUDA0
llama_kv_cache: layer   1: dev = CUDA0
llama_kv_cache: layer   2: dev = CUDA0
llama_kv_cache: layer   3: dev = CUDA0
llama_kv_cache: layer   4: dev = CUDA1
llama_kv_cache: layer   5: dev = CUDA1
llama_kv_cache: layer   6: dev = CUDA1
llama_kv_cache: layer   7: dev = CUDA1
llama_kv_cache: layer   8: dev = CUDA2
llama_kv_cache: layer   9: dev = CUDA2
llama_kv_cache: layer  10: dev = CUDA2
llama_kv_cache: layer  11: dev = CUDA3
llama_kv_cache: layer  12: dev = CUDA3
llama_kv_cache: layer  13: dev = CUDA3
llama_kv_cache: layer  14: dev = CUDA3
llama_kv_cache: layer  15: dev = CUDA4
llama_kv_cache: layer  16: dev = CUDA4
llama_kv_cache: layer  17: dev = CUDA4
llama_kv_cache: layer  18: dev = CUDA4
llama_kv_cache: layer  19: dev = CUDA5
llama_kv_cache: layer  20: dev = CUDA5
llama_kv_cache: layer  21: dev = CUDA5
llama_kv_cache: layer  22: dev = CUDA6
llama_kv_cache: layer  23: dev = CUDA6
llama_kv_cache: layer  24: dev = CUDA6
llama_kv_cache: layer  25: dev = CUDA6
llama_kv_cache: layer  26: dev = CUDA7
llama_kv_cache: layer  27: dev = CUDA7
llama_kv_cache:      CUDA0 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA1 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA2 KV buffer size =    96.00 MiB
llama_kv_cache:      CUDA3 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA4 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA5 KV buffer size =    96.00 MiB
llama_kv_cache:      CUDA6 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA7 KV buffer size =    64.00 MiB
llama_kv_cache: size =  896.00 MiB (  4096 cells,  28 layers,  2/2 seqs), K (f16):  448.00 MiB, V (f16):  448.00 MiB
llama_context: enumerating backends
llama_context: backend_ptrs.size() = 9
llama_context: max_nodes = 2488
llama_context: pipeline parallelism enabled (n_copies=4)
llama_context: reserving full memory module
llama_context: worst-case: n_tokens = 512, n_seqs = 2, n_outputs = 2
graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  2, n_outputs =  512
graph_reserve: reserving a graph for ubatch with n_tokens =    2, n_seqs =  2, n_outputs =    2
graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  2, n_outputs =  512
llama_context:      CUDA0 compute buffer size =   100.04 MiB
llama_context:      CUDA1 compute buffer size =    96.04 MiB
llama_context:      CUDA2 compute buffer size =    96.04 MiB
llama_context:      CUDA3 compute buffer size =    96.04 MiB
llama_context:      CUDA4 compute buffer size =    96.04 MiB
llama_context:      CUDA5 compute buffer size =    96.04 MiB
llama_context:      CUDA6 compute buffer size =    96.04 MiB
llama_context:      CUDA7 compute buffer size =   348.80 MiB
llama_context:  CUDA_Host compute buffer size =    36.05 MiB
llama_context: graph nodes  = 1043
llama_context: graph splits = 9
completed in 791.81 ms
================================================================================
Warmup completed!
================================================================================


================================================================================
Running test 1/16: Baseline (block=8, steps=8)
================================================================================

Run 1/3:

Run 2/3:

Run 3/3:

Average across 3 runs:

Wall Time: 2718.86 ms
Tokens Generated: 149
Throughput: 54.80 tokens/sec

Top 10 Time-Consuming Sections:
Section                                  Total (ms)   Avg (ms)     Calls   
--------------------------------------------------------------------------------
generation_phase                         2651.82      2651.82      1       
token_sampling                           1853.60      14.19        130     
telemetry_cpu_sampling                   1853.47      14.18        130     
telemetry_cpu_loop                       1853.34      14.18        130     
llama_decode                             694.87       5.32         130     
denoising_step_0                         347.19       20.42        17      
denoising_step_1                         329.23       19.37        17      
denoising_step_2                         328.79       19.34        17      
denoising_step_4                         312.51       19.53        16      
denoising_step_5                         310.20       19.39        16      

================================================================================
Running test 2/16: Baseline (block=4, steps=4)
================================================================================

Run 1/3:

Run 2/3:

Run 3/3:

Average across 3 runs:

Wall Time: 1872.81 ms
Tokens Generated: 149
Throughput: 79.56 tokens/sec

Top 10 Time-Consuming Sections:
Section                                  Total (ms)   Avg (ms)     Calls   
--------------------------------------------------------------------------------
generation_phase                         1802.03      1802.03      1       
token_sampling                           1285.22      9.81         131     
telemetry_cpu_sampling                   1285.14      9.81         131     
telemetry_cpu_loop                       1285.06      9.81         131     
denoising_step_0                         482.29       14.61        33      
denoising_step_1                         405.88       12.30        33      
denoising_step_2                         405.42       12.29        33      
llama_decode                             399.89       3.05         131     
denoising_step_3                         393.18       11.91        33      
finalize_block                           114.89       3.48         33      

================================================================================
Running test 3/16: Baseline (block=8, steps=4)
================================================================================

Run 1/3:

Run 2/3:

Run 3/3:

Average across 3 runs:

Wall Time: 1468.01 ms
Tokens Generated: 149
Throughput: 101.50 tokens/sec

Top 10 Time-Consuming Sections:
Section                                  Total (ms)   Avg (ms)     Calls   
--------------------------------------------------------------------------------
generation_phase                         1402.01      1402.01      1       
token_sampling                           942.00       14.27        66      
telemetry_cpu_sampling                   941.94       14.27        66      
telemetry_cpu_loop                       941.89       14.27        66      
llama_decode                             358.56       5.43         66      
denoising_step_0                         348.58       20.50        17      
denoising_step_1                         330.82       19.46        17      
denoising_step_2                         311.43       18.32        17      
denoising_step_3                         310.80       19.43        16      
finalize_block                           100.08       5.89         17      

================================================================================
Running test 4/16: Baseline (block=8, steps=8) + GPU
================================================================================

Run 1/3:

Run 2/3:

Run 3/3:

Average across 3 runs:

Wall Time: 1323.85 ms
Tokens Generated: 149
Throughput: 112.55 tokens/sec

Top 10 Time-Consuming Sections:
Section                                  Total (ms)   Avg (ms)     Calls   
--------------------------------------------------------------------------------
generation_phase                         1241.72      1241.72      1       
llama_decode                             587.11       5.62         104     
token_sampling                           544.69       5.28         104     
telemetry_gpu_invoke                     341.26       3.31         104     
denoising_step_0                         204.05       12.00        17      
telemetry_gpu_stage_cpu_post             167.49       1.63         104     
denoising_step_1                         157.20       9.25         17      
telemetry_gpu_logit_pack                 139.82       1.35         104     
denoising_step_2                         137.87       8.84         14      
denoising_step_3                         127.62       9.58         13      

================================================================================
Running test 5/16: Baseline (block=4, steps=4) + GPU
================================================================================

Run 1/3:

Run 2/3:

Run 3/3:

Average across 3 runs:

Wall Time: 1278.84 ms
Tokens Generated: 149
Throughput: 116.51 tokens/sec

Top 10 Time-Consuming Sections:
Section                                  Total (ms)   Avg (ms)     Calls   
--------------------------------------------------------------------------------
generation_phase                         1199.49      1199.49      1       
token_sampling                           682.46       5.22         130     
telemetry_gpu_logit_pack                 444.31       3.40         130     
llama_decode                             397.01       3.04         130     
denoising_step_0                         322.08       9.76         33      
denoising_step_1                         257.11       7.79         33      
denoising_step_2                         256.14       7.76         33      
denoising_step_3                         246.07       7.46         33      
telemetry_gpu_invoke                     209.69       1.60         130     
telemetry_gpu_success                    130.67       1.00         130     

================================================================================
Running test 6/16: Baseline (block=8, steps=4) + GPU
================================================================================

Run 1/3:

Run 2/3:

Run 3/3:

Average across 3 runs:

Wall Time: 827.30 ms
Tokens Generated: 149
Throughput: 180.10 tokens/sec

Top 10 Time-Consuming Sections:
Section                                  Total (ms)   Avg (ms)     Calls   
--------------------------------------------------------------------------------
generation_phase                         745.47       745.47       1       
llama_decode                             329.52       5.65         58      
token_sampling                           308.53       5.30         58      
denoising_step_0                         200.59       11.80        17      
telemetry_gpu_invoke                     192.07       3.30         58      
denoising_step_1                         159.96       9.41         17      
denoising_step_2                         141.33       9.22         15      
denoising_step_3                         137.52       10.13        13      
finalize_block                           105.43       6.20         17      
telemetry_gpu_stage_cpu_post             95.45        1.64         58      

================================================================================
Running test 7/16: Iterative Refinement (block=4, steps=2, rounds=4)
================================================================================
llama_context: constructing llama_context
llama_context: n_seq_max     = 2
llama_context: n_ctx         = 8192
llama_context: n_ctx_seq     = 4096
llama_context: n_batch       = 2048
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = enabled
llama_context: kv_unified    = false
llama_context: freq_base     = 1000000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_seq (4096) < n_ctx_train (32768) -- the full capacity of the model will not be utilized
set_abort_callback: call
llama_context:  CUDA_Host  output buffer size =     1.16 MiB
llama_kv_cache: layer   0: dev = CUDA0
llama_kv_cache: layer   1: dev = CUDA0
llama_kv_cache: layer   2: dev = CUDA0
llama_kv_cache: layer   3: dev = CUDA0
llama_kv_cache: layer   4: dev = CUDA1
llama_kv_cache: layer   5: dev = CUDA1
llama_kv_cache: layer   6: dev = CUDA1
llama_kv_cache: layer   7: dev = CUDA1
llama_kv_cache: layer   8: dev = CUDA2
llama_kv_cache: layer   9: dev = CUDA2
llama_kv_cache: layer  10: dev = CUDA2
llama_kv_cache: layer  11: dev = CUDA3
llama_kv_cache: layer  12: dev = CUDA3
llama_kv_cache: layer  13: dev = CUDA3
llama_kv_cache: layer  14: dev = CUDA3
llama_kv_cache: layer  15: dev = CUDA4
llama_kv_cache: layer  16: dev = CUDA4
llama_kv_cache: layer  17: dev = CUDA4
llama_kv_cache: layer  18: dev = CUDA4
llama_kv_cache: layer  19: dev = CUDA5
llama_kv_cache: layer  20: dev = CUDA5
llama_kv_cache: layer  21: dev = CUDA5
llama_kv_cache: layer  22: dev = CUDA6
llama_kv_cache: layer  23: dev = CUDA6
llama_kv_cache: layer  24: dev = CUDA6
llama_kv_cache: layer  25: dev = CUDA6
llama_kv_cache: layer  26: dev = CUDA7
llama_kv_cache: layer  27: dev = CUDA7
llama_kv_cache:      CUDA0 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA1 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA2 KV buffer size =    96.00 MiB
llama_kv_cache:      CUDA3 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA4 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA5 KV buffer size =    96.00 MiB
llama_kv_cache:      CUDA6 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA7 KV buffer size =    64.00 MiB
llama_kv_cache: size =  896.00 MiB (  4096 cells,  28 layers,  2/2 seqs), K (f16):  448.00 MiB, V (f16):  448.00 MiB
llama_context: enumerating backends
llama_context: backend_ptrs.size() = 9
llama_context: max_nodes = 2488
llama_context: pipeline parallelism enabled (n_copies=4)
llama_context: reserving full memory module
llama_context: worst-case: n_tokens = 512, n_seqs = 2, n_outputs = 2
graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  2, n_outputs =  512
graph_reserve: reserving a graph for ubatch with n_tokens =    2, n_seqs =  2, n_outputs =    2
graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  2, n_outputs =  512
llama_context:      CUDA0 compute buffer size =   100.04 MiB
llama_context:      CUDA1 compute buffer size =    96.04 MiB
llama_context:      CUDA2 compute buffer size =    96.04 MiB
llama_context:      CUDA3 compute buffer size =    96.04 MiB
llama_context:      CUDA4 compute buffer size =    96.04 MiB
llama_context:      CUDA5 compute buffer size =    96.04 MiB
llama_context:      CUDA6 compute buffer size =    96.04 MiB
llama_context:      CUDA7 compute buffer size =   348.80 MiB
llama_context:  CUDA_Host compute buffer size =    36.05 MiB
llama_context: graph nodes  = 1043
llama_context: graph splits = 9
llama_context: constructing llama_context
llama_context: n_seq_max     = 2
llama_context: n_ctx         = 8192
llama_context: n_ctx_seq     = 4096
llama_context: n_batch       = 2048
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = enabled
llama_context: kv_unified    = false
llama_context: freq_base     = 1000000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_seq (4096) < n_ctx_train (32768) -- the full capacity of the model will not be utilized
set_abort_callback: call
llama_context:  CUDA_Host  output buffer size =     1.16 MiB
llama_kv_cache: layer   0: dev = CUDA0
llama_kv_cache: layer   1: dev = CUDA0
llama_kv_cache: layer   2: dev = CUDA0
llama_kv_cache: layer   3: dev = CUDA0
llama_kv_cache: layer   4: dev = CUDA1
llama_kv_cache: layer   5: dev = CUDA1
llama_kv_cache: layer   6: dev = CUDA1
llama_kv_cache: layer   7: dev = CUDA1
llama_kv_cache: layer   8: dev = CUDA2
llama_kv_cache: layer   9: dev = CUDA2
llama_kv_cache: layer  10: dev = CUDA2
llama_kv_cache: layer  11: dev = CUDA3
llama_kv_cache: layer  12: dev = CUDA3
llama_kv_cache: layer  13: dev = CUDA3
llama_kv_cache: layer  14: dev = CUDA3
llama_kv_cache: layer  15: dev = CUDA4
llama_kv_cache: layer  16: dev = CUDA4
llama_kv_cache: layer  17: dev = CUDA4
llama_kv_cache: layer  18: dev = CUDA4
llama_kv_cache: layer  19: dev = CUDA5
llama_kv_cache: layer  20: dev = CUDA5
llama_kv_cache: layer  21: dev = CUDA5
llama_kv_cache: layer  22: dev = CUDA6
llama_kv_cache: layer  23: dev = CUDA6
llama_kv_cache: layer  24: dev = CUDA6
llama_kv_cache: layer  25: dev = CUDA6
llama_kv_cache: layer  26: dev = CUDA7
llama_kv_cache: layer  27: dev = CUDA7
llama_kv_cache:      CUDA0 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA1 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA2 KV buffer size =    96.00 MiB
llama_kv_cache:      CUDA3 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA4 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA5 KV buffer size =    96.00 MiB
llama_kv_cache:      CUDA6 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA7 KV buffer size =    64.00 MiB
llama_kv_cache: size =  896.00 MiB (  4096 cells,  28 layers,  2/2 seqs), K (f16):  448.00 MiB, V (f16):  448.00 MiB
llama_context: enumerating backends
llama_context: backend_ptrs.size() = 9
llama_context: max_nodes = 2488
llama_context: pipeline parallelism enabled (n_copies=4)
llama_context: reserving full memory module
llama_context: worst-case: n_tokens = 512, n_seqs = 2, n_outputs = 2
graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  2, n_outputs =  512
graph_reserve: reserving a graph for ubatch with n_tokens =    2, n_seqs =  2, n_outputs =    2
graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  2, n_outputs =  512
llama_context:      CUDA0 compute buffer size =   100.04 MiB
llama_context:      CUDA1 compute buffer size =    96.04 MiB
llama_context:      CUDA2 compute buffer size =    96.04 MiB
llama_context:      CUDA3 compute buffer size =    96.04 MiB
llama_context:      CUDA4 compute buffer size =    96.04 MiB
llama_context:      CUDA5 compute buffer size =    96.04 MiB
llama_context:      CUDA6 compute buffer size =    96.04 MiB
llama_context:      CUDA7 compute buffer size =   348.80 MiB
llama_context:  CUDA_Host compute buffer size =    36.05 MiB
llama_context: graph nodes  = 1043
llama_context: graph splits = 9
llama_context: constructing llama_context
llama_context: n_seq_max     = 2
llama_context: n_ctx         = 8192
llama_context: n_ctx_seq     = 4096
llama_context: n_batch       = 2048
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = enabled
llama_context: kv_unified    = false
llama_context: freq_base     = 1000000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_seq (4096) < n_ctx_train (32768) -- the full capacity of the model will not be utilized
set_abort_callback: call
llama_context:  CUDA_Host  output buffer size =     1.16 MiB
llama_kv_cache: layer   0: dev = CUDA0
llama_kv_cache: layer   1: dev = CUDA0
llama_kv_cache: layer   2: dev = CUDA0
llama_kv_cache: layer   3: dev = CUDA0
llama_kv_cache: layer   4: dev = CUDA1
llama_kv_cache: layer   5: dev = CUDA1
llama_kv_cache: layer   6: dev = CUDA1
llama_kv_cache: layer   7: dev = CUDA1
llama_kv_cache: layer   8: dev = CUDA2
llama_kv_cache: layer   9: dev = CUDA2
llama_kv_cache: layer  10: dev = CUDA2
llama_kv_cache: layer  11: dev = CUDA3
llama_kv_cache: layer  12: dev = CUDA3
llama_kv_cache: layer  13: dev = CUDA3
llama_kv_cache: layer  14: dev = CUDA3
llama_kv_cache: layer  15: dev = CUDA4
llama_kv_cache: layer  16: dev = CUDA4
llama_kv_cache: layer  17: dev = CUDA4
llama_kv_cache: layer  18: dev = CUDA4
llama_kv_cache: layer  19: dev = CUDA5
llama_kv_cache: layer  20: dev = CUDA5
llama_kv_cache: layer  21: dev = CUDA5
llama_kv_cache: layer  22: dev = CUDA6
llama_kv_cache: layer  23: dev = CUDA6
llama_kv_cache: layer  24: dev = CUDA6
llama_kv_cache: layer  25: dev = CUDA6
llama_kv_cache: layer  26: dev = CUDA7
llama_kv_cache: layer  27: dev = CUDA7
llama_kv_cache:      CUDA0 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA1 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA2 KV buffer size =    96.00 MiB
llama_kv_cache:      CUDA3 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA4 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA5 KV buffer size =    96.00 MiB
llama_kv_cache:      CUDA6 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA7 KV buffer size =    64.00 MiB
llama_kv_cache: size =  896.00 MiB (  4096 cells,  28 layers,  2/2 seqs), K (f16):  448.00 MiB, V (f16):  448.00 MiB
llama_context: enumerating backends
llama_context: backend_ptrs.size() = 9
llama_context: max_nodes = 2488
llama_context: pipeline parallelism enabled (n_copies=4)
llama_context: reserving full memory module
llama_context: worst-case: n_tokens = 512, n_seqs = 2, n_outputs = 2
graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  2, n_outputs =  512
graph_reserve: reserving a graph for ubatch with n_tokens =    2, n_seqs =  2, n_outputs =    2
graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  2, n_outputs =  512
llama_context:      CUDA0 compute buffer size =   100.04 MiB
llama_context:      CUDA1 compute buffer size =    96.04 MiB
llama_context:      CUDA2 compute buffer size =    96.04 MiB
llama_context:      CUDA3 compute buffer size =    96.04 MiB
llama_context:      CUDA4 compute buffer size =    96.04 MiB
llama_context:      CUDA5 compute buffer size =    96.04 MiB
llama_context:      CUDA6 compute buffer size =    96.04 MiB
llama_context:      CUDA7 compute buffer size =   348.80 MiB
llama_context:  CUDA_Host compute buffer size =    36.05 MiB
llama_context: graph nodes  = 1043
llama_context: graph splits = 9
llama_context: constructing llama_context
llama_context: n_seq_max     = 2
llama_context: n_ctx         = 8192
llama_context: n_ctx_seq     = 4096
llama_context: n_batch       = 2048
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = enabled
llama_context: kv_unified    = false
llama_context: freq_base     = 1000000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_seq (4096) < n_ctx_train (32768) -- the full capacity of the model will not be utilized
set_abort_callback: call
llama_context:  CUDA_Host  output buffer size =     1.16 MiB
llama_kv_cache: layer   0: dev = CUDA0
llama_kv_cache: layer   1: dev = CUDA0
llama_kv_cache: layer   2: dev = CUDA0
llama_kv_cache: layer   3: dev = CUDA0
llama_kv_cache: layer   4: dev = CUDA1
llama_kv_cache: layer   5: dev = CUDA1
llama_kv_cache: layer   6: dev = CUDA1
llama_kv_cache: layer   7: dev = CUDA1
llama_kv_cache: layer   8: dev = CUDA2
llama_kv_cache: layer   9: dev = CUDA2
llama_kv_cache: layer  10: dev = CUDA2
llama_kv_cache: layer  11: dev = CUDA3
llama_kv_cache: layer  12: dev = CUDA3
llama_kv_cache: layer  13: dev = CUDA3
llama_kv_cache: layer  14: dev = CUDA3
llama_kv_cache: layer  15: dev = CUDA4
llama_kv_cache: layer  16: dev = CUDA4
llama_kv_cache: layer  17: dev = CUDA4
llama_kv_cache: layer  18: dev = CUDA4
llama_kv_cache: layer  19: dev = CUDA5
llama_kv_cache: layer  20: dev = CUDA5
llama_kv_cache: layer  21: dev = CUDA5
llama_kv_cache: layer  22: dev = CUDA6
llama_kv_cache: layer  23: dev = CUDA6
llama_kv_cache: layer  24: dev = CUDA6
llama_kv_cache: layer  25: dev = CUDA6
llama_kv_cache: layer  26: dev = CUDA7
llama_kv_cache: layer  27: dev = CUDA7
llama_kv_cache:      CUDA0 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA1 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA2 KV buffer size =    96.00 MiB
llama_kv_cache:      CUDA3 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA4 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA5 KV buffer size =    96.00 MiB
llama_kv_cache:      CUDA6 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA7 KV buffer size =    64.00 MiB
llama_kv_cache: size =  896.00 MiB (  4096 cells,  28 layers,  2/2 seqs), K (f16):  448.00 MiB, V (f16):  448.00 MiB
llama_context: enumerating backends
llama_context: backend_ptrs.size() = 9
llama_context: max_nodes = 2488
llama_context: pipeline parallelism enabled (n_copies=4)
llama_context: reserving full memory module
llama_context: worst-case: n_tokens = 512, n_seqs = 2, n_outputs = 2
graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  2, n_outputs =  512
graph_reserve: reserving a graph for ubatch with n_tokens =    2, n_seqs =  2, n_outputs =    2
graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  2, n_outputs =  512
llama_context:      CUDA0 compute buffer size =   100.04 MiB
llama_context:      CUDA1 compute buffer size =    96.04 MiB
llama_context:      CUDA2 compute buffer size =    96.04 MiB
llama_context:      CUDA3 compute buffer size =    96.04 MiB
llama_context:      CUDA4 compute buffer size =    96.04 MiB
llama_context:      CUDA5 compute buffer size =    96.04 MiB
llama_context:      CUDA6 compute buffer size =    96.04 MiB
llama_context:      CUDA7 compute buffer size =   348.80 MiB
llama_context:  CUDA_Host compute buffer size =    36.05 MiB
llama_context: graph nodes  = 1043
llama_context: graph splits = 9
llama_context: constructing llama_context
llama_context: n_seq_max     = 2
llama_context: n_ctx         = 8192
llama_context: n_ctx_seq     = 4096
llama_context: n_batch       = 2048
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = enabled
llama_context: kv_unified    = false
llama_context: freq_base     = 1000000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_seq (4096) < n_ctx_train (32768) -- the full capacity of the model will not be utilized
set_abort_callback: call
llama_context:  CUDA_Host  output buffer size =     1.16 MiB
llama_kv_cache: layer   0: dev = CUDA0
llama_kv_cache: layer   1: dev = CUDA0
llama_kv_cache: layer   2: dev = CUDA0
llama_kv_cache: layer   3: dev = CUDA0
llama_kv_cache: layer   4: dev = CUDA1
llama_kv_cache: layer   5: dev = CUDA1
llama_kv_cache: layer   6: dev = CUDA1
llama_kv_cache: layer   7: dev = CUDA1
llama_kv_cache: layer   8: dev = CUDA2
llama_kv_cache: layer   9: dev = CUDA2
llama_kv_cache: layer  10: dev = CUDA2
llama_kv_cache: layer  11: dev = CUDA3
llama_kv_cache: layer  12: dev = CUDA3
llama_kv_cache: layer  13: dev = CUDA3
llama_kv_cache: layer  14: dev = CUDA3
llama_kv_cache: layer  15: dev = CUDA4
llama_kv_cache: layer  16: dev = CUDA4
llama_kv_cache: layer  17: dev = CUDA4
llama_kv_cache: layer  18: dev = CUDA4
llama_kv_cache: layer  19: dev = CUDA5
llama_kv_cache: layer  20: dev = CUDA5
llama_kv_cache: layer  21: dev = CUDA5
llama_kv_cache: layer  22: dev = CUDA6
llama_kv_cache: layer  23: dev = CUDA6
llama_kv_cache: layer  24: dev = CUDA6
llama_kv_cache: layer  25: dev = CUDA6
llama_kv_cache: layer  26: dev = CUDA7
llama_kv_cache: layer  27: dev = CUDA7
llama_kv_cache:      CUDA0 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA1 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA2 KV buffer size =    96.00 MiB
llama_kv_cache:      CUDA3 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA4 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA5 KV buffer size =    96.00 MiB
llama_kv_cache:      CUDA6 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA7 KV buffer size =    64.00 MiB
llama_kv_cache: size =  896.00 MiB (  4096 cells,  28 layers,  2/2 seqs), K (f16):  448.00 MiB, V (f16):  448.00 MiB
llama_context: enumerating backends
llama_context: backend_ptrs.size() = 9
llama_context: max_nodes = 2488
llama_context: pipeline parallelism enabled (n_copies=4)
llama_context: reserving full memory module
llama_context: worst-case: n_tokens = 512, n_seqs = 2, n_outputs = 2
graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  2, n_outputs =  512
graph_reserve: reserving a graph for ubatch with n_tokens =    2, n_seqs =  2, n_outputs =    2
graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  2, n_outputs =  512
llama_context:      CUDA0 compute buffer size =   100.04 MiB
llama_context:      CUDA1 compute buffer size =    96.04 MiB
llama_context:      CUDA2 compute buffer size =    96.04 MiB
llama_context:      CUDA3 compute buffer size =    96.04 MiB
llama_context:      CUDA4 compute buffer size =    96.04 MiB
llama_context:      CUDA5 compute buffer size =    96.04 MiB
llama_context:      CUDA6 compute buffer size =    96.04 MiB
llama_context:      CUDA7 compute buffer size =   348.80 MiB
llama_context:  CUDA_Host compute buffer size =    36.05 MiB
llama_context: graph nodes  = 1043
llama_context: graph splits = 9
llama_context: constructing llama_context
llama_context: n_seq_max     = 2
llama_context: n_ctx         = 8192
llama_context: n_ctx_seq     = 4096
llama_context: n_batch       = 2048
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = enabled
llama_context: kv_unified    = false
llama_context: freq_base     = 1000000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_seq (4096) < n_ctx_train (32768) -- the full capacity of the model will not be utilized
set_abort_callback: call
llama_context:  CUDA_Host  output buffer size =     1.16 MiB
llama_kv_cache: layer   0: dev = CUDA0
llama_kv_cache: layer   1: dev = CUDA0
llama_kv_cache: layer   2: dev = CUDA0
llama_kv_cache: layer   3: dev = CUDA0
llama_kv_cache: layer   4: dev = CUDA1
llama_kv_cache: layer   5: dev = CUDA1
llama_kv_cache: layer   6: dev = CUDA1
llama_kv_cache: layer   7: dev = CUDA1
llama_kv_cache: layer   8: dev = CUDA2
llama_kv_cache: layer   9: dev = CUDA2
llama_kv_cache: layer  10: dev = CUDA2
llama_kv_cache: layer  11: dev = CUDA3
llama_kv_cache: layer  12: dev = CUDA3
llama_kv_cache: layer  13: dev = CUDA3
llama_kv_cache: layer  14: dev = CUDA3
llama_kv_cache: layer  15: dev = CUDA4
llama_kv_cache: layer  16: dev = CUDA4
llama_kv_cache: layer  17: dev = CUDA4
llama_kv_cache: layer  18: dev = CUDA4
llama_kv_cache: layer  19: dev = CUDA5
llama_kv_cache: layer  20: dev = CUDA5
llama_kv_cache: layer  21: dev = CUDA5
llama_kv_cache: layer  22: dev = CUDA6
llama_kv_cache: layer  23: dev = CUDA6
llama_kv_cache: layer  24: dev = CUDA6
llama_kv_cache: layer  25: dev = CUDA6
llama_kv_cache: layer  26: dev = CUDA7
llama_kv_cache: layer  27: dev = CUDA7
llama_kv_cache:      CUDA0 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA1 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA2 KV buffer size =    96.00 MiB
llama_kv_cache:      CUDA3 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA4 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA5 KV buffer size =    96.00 MiB
llama_kv_cache:      CUDA6 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA7 KV buffer size =    64.00 MiB
llama_kv_cache: size =  896.00 MiB (  4096 cells,  28 layers,  2/2 seqs), K (f16):  448.00 MiB, V (f16):  448.00 MiB
llama_context: enumerating backends
llama_context: backend_ptrs.size() = 9
llama_context: max_nodes = 2488
llama_context: pipeline parallelism enabled (n_copies=4)
llama_context: reserving full memory module
llama_context: worst-case: n_tokens = 512, n_seqs = 2, n_outputs = 2
graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  2, n_outputs =  512
graph_reserve: reserving a graph for ubatch with n_tokens =    2, n_seqs =  2, n_outputs =    2
graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  2, n_outputs =  512
llama_context:      CUDA0 compute buffer size =   100.04 MiB
llama_context:      CUDA1 compute buffer size =    96.04 MiB
llama_context:      CUDA2 compute buffer size =    96.04 MiB
llama_context:      CUDA3 compute buffer size =    96.04 MiB
llama_context:      CUDA4 compute buffer size =    96.04 MiB
llama_context:      CUDA5 compute buffer size =    96.04 MiB
llama_context:      CUDA6 compute buffer size =    96.04 MiB
llama_context:      CUDA7 compute buffer size =   348.80 MiB
llama_context:  CUDA_Host compute buffer size =    36.05 MiB
llama_context: graph nodes  = 1043
llama_context: graph splits = 9
llama_context: constructing llama_context
llama_context: n_seq_max     = 2
llama_context: n_ctx         = 8192
llama_context: n_ctx_seq     = 4096
llama_context: n_batch       = 2048
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = enabled
llama_context: kv_unified    = false
llama_context: freq_base     = 1000000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_seq (4096) < n_ctx_train (32768) -- the full capacity of the model will not be utilized
set_abort_callback: call
llama_context:  CUDA_Host  output buffer size =     1.16 MiB
llama_kv_cache: layer   0: dev = CUDA0
llama_kv_cache: layer   1: dev = CUDA0
llama_kv_cache: layer   2: dev = CUDA0
llama_kv_cache: layer   3: dev = CUDA0
llama_kv_cache: layer   4: dev = CUDA1
llama_kv_cache: layer   5: dev = CUDA1
llama_kv_cache: layer   6: dev = CUDA1
llama_kv_cache: layer   7: dev = CUDA1
llama_kv_cache: layer   8: dev = CUDA2
llama_kv_cache: layer   9: dev = CUDA2
llama_kv_cache: layer  10: dev = CUDA2
llama_kv_cache: layer  11: dev = CUDA3
llama_kv_cache: layer  12: dev = CUDA3
llama_kv_cache: layer  13: dev = CUDA3
llama_kv_cache: layer  14: dev = CUDA3
llama_kv_cache: layer  15: dev = CUDA4
llama_kv_cache: layer  16: dev = CUDA4
llama_kv_cache: layer  17: dev = CUDA4
llama_kv_cache: layer  18: dev = CUDA4
llama_kv_cache: layer  19: dev = CUDA5
llama_kv_cache: layer  20: dev = CUDA5
llama_kv_cache: layer  21: dev = CUDA5
llama_kv_cache: layer  22: dev = CUDA6
llama_kv_cache: layer  23: dev = CUDA6
llama_kv_cache: layer  24: dev = CUDA6
llama_kv_cache: layer  25: dev = CUDA6
llama_kv_cache: layer  26: dev = CUDA7
llama_kv_cache: layer  27: dev = CUDA7
llama_kv_cache:      CUDA0 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA1 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA2 KV buffer size =    96.00 MiB
llama_kv_cache:      CUDA3 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA4 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA5 KV buffer size =    96.00 MiB
llama_kv_cache:      CUDA6 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA7 KV buffer size =    64.00 MiB
llama_kv_cache: size =  896.00 MiB (  4096 cells,  28 layers,  2/2 seqs), K (f16):  448.00 MiB, V (f16):  448.00 MiB
llama_context: enumerating backends
llama_context: backend_ptrs.size() = 9
llama_context: max_nodes = 2488
llama_context: pipeline parallelism enabled (n_copies=4)
llama_context: reserving full memory module
llama_context: worst-case: n_tokens = 512, n_seqs = 2, n_outputs = 2
graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  2, n_outputs =  512
graph_reserve: reserving a graph for ubatch with n_tokens =    2, n_seqs =  2, n_outputs =    2
graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  2, n_outputs =  512
llama_context:      CUDA0 compute buffer size =   100.04 MiB
llama_context:      CUDA1 compute buffer size =    96.04 MiB
llama_context:      CUDA2 compute buffer size =    96.04 MiB
llama_context:      CUDA3 compute buffer size =    96.04 MiB
llama_context:      CUDA4 compute buffer size =    96.04 MiB
llama_context:      CUDA5 compute buffer size =    96.04 MiB
llama_context:      CUDA6 compute buffer size =    96.04 MiB
llama_context:      CUDA7 compute buffer size =   348.80 MiB
llama_context:  CUDA_Host compute buffer size =    36.05 MiB
llama_context: graph nodes  = 1043
llama_context: graph splits = 9
llama_context: constructing llama_context
llama_context: n_seq_max     = 2
llama_context: n_ctx         = 8192
llama_context: n_ctx_seq     = 4096
llama_context: n_batch       = 2048
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = enabled
llama_context: kv_unified    = false
llama_context: freq_base     = 1000000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_seq (4096) < n_ctx_train (32768) -- the full capacity of the model will not be utilized
set_abort_callback: call
llama_context:  CUDA_Host  output buffer size =     1.16 MiB
llama_kv_cache: layer   0: dev = CUDA0
llama_kv_cache: layer   1: dev = CUDA0
llama_kv_cache: layer   2: dev = CUDA0
llama_kv_cache: layer   3: dev = CUDA0
llama_kv_cache: layer   4: dev = CUDA1
llama_kv_cache: layer   5: dev = CUDA1
llama_kv_cache: layer   6: dev = CUDA1
llama_kv_cache: layer   7: dev = CUDA1
llama_kv_cache: layer   8: dev = CUDA2
llama_kv_cache: layer   9: dev = CUDA2
llama_kv_cache: layer  10: dev = CUDA2
llama_kv_cache: layer  11: dev = CUDA3
llama_kv_cache: layer  12: dev = CUDA3
llama_kv_cache: layer  13: dev = CUDA3
llama_kv_cache: layer  14: dev = CUDA3
llama_kv_cache: layer  15: dev = CUDA4
llama_kv_cache: layer  16: dev = CUDA4
llama_kv_cache: layer  17: dev = CUDA4
llama_kv_cache: layer  18: dev = CUDA4
llama_kv_cache: layer  19: dev = CUDA5
llama_kv_cache: layer  20: dev = CUDA5
llama_kv_cache: layer  21: dev = CUDA5
llama_kv_cache: layer  22: dev = CUDA6
llama_kv_cache: layer  23: dev = CUDA6
llama_kv_cache: layer  24: dev = CUDA6
llama_kv_cache: layer  25: dev = CUDA6
llama_kv_cache: layer  26: dev = CUDA7
llama_kv_cache: layer  27: dev = CUDA7
llama_kv_cache:      CUDA0 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA1 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA2 KV buffer size =    96.00 MiB
llama_kv_cache:      CUDA3 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA4 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA5 KV buffer size =    96.00 MiB
llama_kv_cache:      CUDA6 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA7 KV buffer size =    64.00 MiB
llama_kv_cache: size =  896.00 MiB (  4096 cells,  28 layers,  2/2 seqs), K (f16):  448.00 MiB, V (f16):  448.00 MiB
llama_context: enumerating backends
llama_context: backend_ptrs.size() = 9
llama_context: max_nodes = 2488
llama_context: pipeline parallelism enabled (n_copies=4)
llama_context: reserving full memory module
llama_context: worst-case: n_tokens = 512, n_seqs = 2, n_outputs = 2
graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  2, n_outputs =  512
graph_reserve: reserving a graph for ubatch with n_tokens =    2, n_seqs =  2, n_outputs =    2
graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  2, n_outputs =  512
llama_context:      CUDA0 compute buffer size =   100.04 MiB
llama_context:      CUDA1 compute buffer size =    96.04 MiB
llama_context:      CUDA2 compute buffer size =    96.04 MiB
llama_context:      CUDA3 compute buffer size =    96.04 MiB
llama_context:      CUDA4 compute buffer size =    96.04 MiB
llama_context:      CUDA5 compute buffer size =    96.04 MiB
llama_context:      CUDA6 compute buffer size =    96.04 MiB
llama_context:      CUDA7 compute buffer size =   348.80 MiB
llama_context:  CUDA_Host compute buffer size =    36.05 MiB
llama_context: graph nodes  = 1043
llama_context: graph splits = 9
llama_context: constructing llama_context
llama_context: n_seq_max     = 2
llama_context: n_ctx         = 8192
llama_context: n_ctx_seq     = 4096
llama_context: n_batch       = 2048
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = enabled
llama_context: kv_unified    = false
llama_context: freq_base     = 1000000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_seq (4096) < n_ctx_train (32768) -- the full capacity of the model will not be utilized
set_abort_callback: call
llama_context:  CUDA_Host  output buffer size =     1.16 MiB
llama_kv_cache: layer   0: dev = CUDA0
llama_kv_cache: layer   1: dev = CUDA0
llama_kv_cache: layer   2: dev = CUDA0
llama_kv_cache: layer   3: dev = CUDA0
llama_kv_cache: layer   4: dev = CUDA1
llama_kv_cache: layer   5: dev = CUDA1
llama_kv_cache: layer   6: dev = CUDA1
llama_kv_cache: layer   7: dev = CUDA1
llama_kv_cache: layer   8: dev = CUDA2
llama_kv_cache: layer   9: dev = CUDA2
llama_kv_cache: layer  10: dev = CUDA2
llama_kv_cache: layer  11: dev = CUDA3
llama_kv_cache: layer  12: dev = CUDA3
llama_kv_cache: layer  13: dev = CUDA3
llama_kv_cache: layer  14: dev = CUDA3
llama_kv_cache: layer  15: dev = CUDA4
llama_kv_cache: layer  16: dev = CUDA4
llama_kv_cache: layer  17: dev = CUDA4
llama_kv_cache: layer  18: dev = CUDA4
llama_kv_cache: layer  19: dev = CUDA5
llama_kv_cache: layer  20: dev = CUDA5
llama_kv_cache: layer  21: dev = CUDA5
llama_kv_cache: layer  22: dev = CUDA6
llama_kv_cache: layer  23: dev = CUDA6
llama_kv_cache: layer  24: dev = CUDA6
llama_kv_cache: layer  25: dev = CUDA6
llama_kv_cache: layer  26: dev = CUDA7
llama_kv_cache: layer  27: dev = CUDA7
llama_kv_cache:      CUDA0 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA1 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA2 KV buffer size =    96.00 MiB
llama_kv_cache:      CUDA3 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA4 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA5 KV buffer size =    96.00 MiB
llama_kv_cache:      CUDA6 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA7 KV buffer size =    64.00 MiB
llama_kv_cache: size =  896.00 MiB (  4096 cells,  28 layers,  2/2 seqs), K (f16):  448.00 MiB, V (f16):  448.00 MiB
llama_context: enumerating backends
llama_context: backend_ptrs.size() = 9
llama_context: max_nodes = 2488
llama_context: pipeline parallelism enabled (n_copies=4)
llama_context: reserving full memory module
llama_context: worst-case: n_tokens = 512, n_seqs = 2, n_outputs = 2
graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  2, n_outputs =  512
graph_reserve: reserving a graph for ubatch with n_tokens =    2, n_seqs =  2, n_outputs =    2
graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  2, n_outputs =  512
llama_context:      CUDA0 compute buffer size =   100.04 MiB
llama_context:      CUDA1 compute buffer size =    96.04 MiB
llama_context:      CUDA2 compute buffer size =    96.04 MiB
llama_context:      CUDA3 compute buffer size =    96.04 MiB
llama_context:      CUDA4 compute buffer size =    96.04 MiB
llama_context:      CUDA5 compute buffer size =    96.04 MiB
llama_context:      CUDA6 compute buffer size =    96.04 MiB
llama_context:      CUDA7 compute buffer size =   348.80 MiB
llama_context:  CUDA_Host compute buffer size =    36.05 MiB
llama_context: graph nodes  = 1043
llama_context: graph splits = 9
llama_context: constructing llama_context
llama_context: n_seq_max     = 2
llama_context: n_ctx         = 8192
llama_context: n_ctx_seq     = 4096
llama_context: n_batch       = 2048
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = enabled
llama_context: kv_unified    = false
llama_context: freq_base     = 1000000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_seq (4096) < n_ctx_train (32768) -- the full capacity of the model will not be utilized
set_abort_callback: call
llama_context:  CUDA_Host  output buffer size =     1.16 MiB
llama_kv_cache: layer   0: dev = CUDA0
llama_kv_cache: layer   1: dev = CUDA0
llama_kv_cache: layer   2: dev = CUDA0
llama_kv_cache: layer   3: dev = CUDA0
llama_kv_cache: layer   4: dev = CUDA1
llama_kv_cache: layer   5: dev = CUDA1
llama_kv_cache: layer   6: dev = CUDA1
llama_kv_cache: layer   7: dev = CUDA1
llama_kv_cache: layer   8: dev = CUDA2
llama_kv_cache: layer   9: dev = CUDA2
llama_kv_cache: layer  10: dev = CUDA2
llama_kv_cache: layer  11: dev = CUDA3
llama_kv_cache: layer  12: dev = CUDA3
llama_kv_cache: layer  13: dev = CUDA3
llama_kv_cache: layer  14: dev = CUDA3
llama_kv_cache: layer  15: dev = CUDA4
llama_kv_cache: layer  16: dev = CUDA4
llama_kv_cache: layer  17: dev = CUDA4
llama_kv_cache: layer  18: dev = CUDA4
llama_kv_cache: layer  19: dev = CUDA5
llama_kv_cache: layer  20: dev = CUDA5
llama_kv_cache: layer  21: dev = CUDA5
llama_kv_cache: layer  22: dev = CUDA6
llama_kv_cache: layer  23: dev = CUDA6
llama_kv_cache: layer  24: dev = CUDA6
llama_kv_cache: layer  25: dev = CUDA6
llama_kv_cache: layer  26: dev = CUDA7
llama_kv_cache: layer  27: dev = CUDA7
llama_kv_cache:      CUDA0 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA1 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA2 KV buffer size =    96.00 MiB
llama_kv_cache:      CUDA3 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA4 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA5 KV buffer size =    96.00 MiB
llama_kv_cache:      CUDA6 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA7 KV buffer size =    64.00 MiB
llama_kv_cache: size =  896.00 MiB (  4096 cells,  28 layers,  2/2 seqs), K (f16):  448.00 MiB, V (f16):  448.00 MiB
llama_context: enumerating backends
llama_context: backend_ptrs.size() = 9
llama_context: max_nodes = 2488
llama_context: pipeline parallelism enabled (n_copies=4)
llama_context: reserving full memory module
llama_context: worst-case: n_tokens = 512, n_seqs = 2, n_outputs = 2
graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  2, n_outputs =  512
graph_reserve: reserving a graph for ubatch with n_tokens =    2, n_seqs =  2, n_outputs =    2
graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  2, n_outputs =  512
llama_context:      CUDA0 compute buffer size =   100.04 MiB
llama_context:      CUDA1 compute buffer size =    96.04 MiB
llama_context:      CUDA2 compute buffer size =    96.04 MiB
llama_context:      CUDA3 compute buffer size =    96.04 MiB
llama_context:      CUDA4 compute buffer size =    96.04 MiB
llama_context:      CUDA5 compute buffer size =    96.04 MiB
llama_context:      CUDA6 compute buffer size =    96.04 MiB
llama_context:      CUDA7 compute buffer size =   348.80 MiB
llama_context:  CUDA_Host compute buffer size =    36.05 MiB
llama_context: graph nodes  = 1043
llama_context: graph splits = 9
llama_context: constructing llama_context
llama_context: n_seq_max     = 2
llama_context: n_ctx         = 8192
llama_context: n_ctx_seq     = 4096
llama_context: n_batch       = 2048
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = enabled
llama_context: kv_unified    = false
llama_context: freq_base     = 1000000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_seq (4096) < n_ctx_train (32768) -- the full capacity of the model will not be utilized
set_abort_callback: call
llama_context:  CUDA_Host  output buffer size =     1.16 MiB
llama_kv_cache: layer   0: dev = CUDA0
llama_kv_cache: layer   1: dev = CUDA0
llama_kv_cache: layer   2: dev = CUDA0
llama_kv_cache: layer   3: dev = CUDA0
llama_kv_cache: layer   4: dev = CUDA1
llama_kv_cache: layer   5: dev = CUDA1
llama_kv_cache: layer   6: dev = CUDA1
llama_kv_cache: layer   7: dev = CUDA1
llama_kv_cache: layer   8: dev = CUDA2
llama_kv_cache: layer   9: dev = CUDA2
llama_kv_cache: layer  10: dev = CUDA2
llama_kv_cache: layer  11: dev = CUDA3
llama_kv_cache: layer  12: dev = CUDA3
llama_kv_cache: layer  13: dev = CUDA3
llama_kv_cache: layer  14: dev = CUDA3
llama_kv_cache: layer  15: dev = CUDA4
llama_kv_cache: layer  16: dev = CUDA4
llama_kv_cache: layer  17: dev = CUDA4
llama_kv_cache: layer  18: dev = CUDA4
llama_kv_cache: layer  19: dev = CUDA5
llama_kv_cache: layer  20: dev = CUDA5
llama_kv_cache: layer  21: dev = CUDA5
llama_kv_cache: layer  22: dev = CUDA6
llama_kv_cache: layer  23: dev = CUDA6
llama_kv_cache: layer  24: dev = CUDA6
llama_kv_cache: layer  25: dev = CUDA6
llama_kv_cache: layer  26: dev = CUDA7
llama_kv_cache: layer  27: dev = CUDA7
llama_kv_cache:      CUDA0 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA1 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA2 KV buffer size =    96.00 MiB
llama_kv_cache:      CUDA3 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA4 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA5 KV buffer size =    96.00 MiB
llama_kv_cache:      CUDA6 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA7 KV buffer size =    64.00 MiB
llama_kv_cache: size =  896.00 MiB (  4096 cells,  28 layers,  2/2 seqs), K (f16):  448.00 MiB, V (f16):  448.00 MiB
llama_context: enumerating backends
llama_context: backend_ptrs.size() = 9
llama_context: max_nodes = 2488
llama_context: pipeline parallelism enabled (n_copies=4)
llama_context: reserving full memory module
llama_context: worst-case: n_tokens = 512, n_seqs = 2, n_outputs = 2
graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  2, n_outputs =  512
graph_reserve: reserving a graph for ubatch with n_tokens =    2, n_seqs =  2, n_outputs =    2
graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  2, n_outputs =  512
llama_context:      CUDA0 compute buffer size =   100.04 MiB
llama_context:      CUDA1 compute buffer size =    96.04 MiB
llama_context:      CUDA2 compute buffer size =    96.04 MiB
llama_context:      CUDA3 compute buffer size =    96.04 MiB
llama_context:      CUDA4 compute buffer size =    96.04 MiB
llama_context:      CUDA5 compute buffer size =    96.04 MiB
llama_context:      CUDA6 compute buffer size =    96.04 MiB
llama_context:      CUDA7 compute buffer size =   348.80 MiB
llama_context:  CUDA_Host compute buffer size =    36.05 MiB
llama_context: graph nodes  = 1043
llama_context: graph splits = 9
llama_context: constructing llama_context
llama_context: n_seq_max     = 2
llama_context: n_ctx         = 8192
llama_context: n_ctx_seq     = 4096
llama_context: n_batch       = 2048
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = enabled
llama_context: kv_unified    = false
llama_context: freq_base     = 1000000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_seq (4096) < n_ctx_train (32768) -- the full capacity of the model will not be utilized
set_abort_callback: call
llama_context:  CUDA_Host  output buffer size =     1.16 MiB
llama_kv_cache: layer   0: dev = CUDA0
llama_kv_cache: layer   1: dev = CUDA0
llama_kv_cache: layer   2: dev = CUDA0
llama_kv_cache: layer   3: dev = CUDA0
llama_kv_cache: layer   4: dev = CUDA1
llama_kv_cache: layer   5: dev = CUDA1
llama_kv_cache: layer   6: dev = CUDA1
llama_kv_cache: layer   7: dev = CUDA1
llama_kv_cache: layer   8: dev = CUDA2
llama_kv_cache: layer   9: dev = CUDA2
llama_kv_cache: layer  10: dev = CUDA2
llama_kv_cache: layer  11: dev = CUDA3
llama_kv_cache: layer  12: dev = CUDA3
llama_kv_cache: layer  13: dev = CUDA3
llama_kv_cache: layer  14: dev = CUDA3
llama_kv_cache: layer  15: dev = CUDA4
llama_kv_cache: layer  16: dev = CUDA4
llama_kv_cache: layer  17: dev = CUDA4
llama_kv_cache: layer  18: dev = CUDA4
llama_kv_cache: layer  19: dev = CUDA5
llama_kv_cache: layer  20: dev = CUDA5
llama_kv_cache: layer  21: dev = CUDA5
llama_kv_cache: layer  22: dev = CUDA6
llama_kv_cache: layer  23: dev = CUDA6
llama_kv_cache: layer  24: dev = CUDA6
llama_kv_cache: layer  25: dev = CUDA6
llama_kv_cache: layer  26: dev = CUDA7
llama_kv_cache: layer  27: dev = CUDA7
llama_kv_cache:      CUDA0 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA1 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA2 KV buffer size =    96.00 MiB
llama_kv_cache:      CUDA3 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA4 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA5 KV buffer size =    96.00 MiB
llama_kv_cache:      CUDA6 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA7 KV buffer size =    64.00 MiB
llama_kv_cache: size =  896.00 MiB (  4096 cells,  28 layers,  2/2 seqs), K (f16):  448.00 MiB, V (f16):  448.00 MiB
llama_context: enumerating backends
llama_context: backend_ptrs.size() = 9
llama_context: max_nodes = 2488
llama_context: pipeline parallelism enabled (n_copies=4)
llama_context: reserving full memory module
llama_context: worst-case: n_tokens = 512, n_seqs = 2, n_outputs = 2
graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  2, n_outputs =  512
graph_reserve: reserving a graph for ubatch with n_tokens =    2, n_seqs =  2, n_outputs =    2
graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  2, n_outputs =  512
llama_context:      CUDA0 compute buffer size =   100.04 MiB
llama_context:      CUDA1 compute buffer size =    96.04 MiB
llama_context:      CUDA2 compute buffer size =    96.04 MiB
llama_context:      CUDA3 compute buffer size =    96.04 MiB
llama_context:      CUDA4 compute buffer size =    96.04 MiB
llama_context:      CUDA5 compute buffer size =    96.04 MiB
llama_context:      CUDA6 compute buffer size =    96.04 MiB
llama_context:      CUDA7 compute buffer size =   348.80 MiB
llama_context:  CUDA_Host compute buffer size =    36.05 MiB
llama_context: graph nodes  = 1043
llama_context: graph splits = 9
llama_context: constructing llama_context
llama_context: n_seq_max     = 2
llama_context: n_ctx         = 8192
llama_context: n_ctx_seq     = 4096
llama_context: n_batch       = 2048
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = enabled
llama_context: kv_unified    = false
llama_context: freq_base     = 1000000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_seq (4096) < n_ctx_train (32768) -- the full capacity of the model will not be utilized
set_abort_callback: call
llama_context:  CUDA_Host  output buffer size =     1.16 MiB
llama_kv_cache: layer   0: dev = CUDA0
llama_kv_cache: layer   1: dev = CUDA0
llama_kv_cache: layer   2: dev = CUDA0
llama_kv_cache: layer   3: dev = CUDA0
llama_kv_cache: layer   4: dev = CUDA1
llama_kv_cache: layer   5: dev = CUDA1
llama_kv_cache: layer   6: dev = CUDA1
llama_kv_cache: layer   7: dev = CUDA1
llama_kv_cache: layer   8: dev = CUDA2
llama_kv_cache: layer   9: dev = CUDA2
llama_kv_cache: layer  10: dev = CUDA2
llama_kv_cache: layer  11: dev = CUDA3
llama_kv_cache: layer  12: dev = CUDA3
llama_kv_cache: layer  13: dev = CUDA3
llama_kv_cache: layer  14: dev = CUDA3
llama_kv_cache: layer  15: dev = CUDA4
llama_kv_cache: layer  16: dev = CUDA4
llama_kv_cache: layer  17: dev = CUDA4
llama_kv_cache: layer  18: dev = CUDA4
llama_kv_cache: layer  19: dev = CUDA5
llama_kv_cache: layer  20: dev = CUDA5
llama_kv_cache: layer  21: dev = CUDA5
llama_kv_cache: layer  22: dev = CUDA6
llama_kv_cache: layer  23: dev = CUDA6
llama_kv_cache: layer  24: dev = CUDA6
llama_kv_cache: layer  25: dev = CUDA6
llama_kv_cache: layer  26: dev = CUDA7
llama_kv_cache: layer  27: dev = CUDA7
llama_kv_cache:      CUDA0 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA1 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA2 KV buffer size =    96.00 MiB
llama_kv_cache:      CUDA3 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA4 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA5 KV buffer size =    96.00 MiB
llama_kv_cache:      CUDA6 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA7 KV buffer size =    64.00 MiB
llama_kv_cache: size =  896.00 MiB (  4096 cells,  28 layers,  2/2 seqs), K (f16):  448.00 MiB, V (f16):  448.00 MiB
llama_context: enumerating backends
llama_context: backend_ptrs.size() = 9
llama_context: max_nodes = 2488
llama_context: pipeline parallelism enabled (n_copies=4)
llama_context: reserving full memory module
llama_context: worst-case: n_tokens = 512, n_seqs = 2, n_outputs = 2
graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  2, n_outputs =  512
graph_reserve: reserving a graph for ubatch with n_tokens =    2, n_seqs =  2, n_outputs =    2
graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  2, n_outputs =  512
llama_context:      CUDA0 compute buffer size =   100.04 MiB
llama_context:      CUDA1 compute buffer size =    96.04 MiB
llama_context:      CUDA2 compute buffer size =    96.04 MiB
llama_context:      CUDA3 compute buffer size =    96.04 MiB
llama_context:      CUDA4 compute buffer size =    96.04 MiB
llama_context:      CUDA5 compute buffer size =    96.04 MiB
llama_context:      CUDA6 compute buffer size =    96.04 MiB
llama_context:      CUDA7 compute buffer size =   348.80 MiB
llama_context:  CUDA_Host compute buffer size =    36.05 MiB
llama_context: graph nodes  = 1043
llama_context: graph splits = 9
llama_context: constructing llama_context
llama_context: n_seq_max     = 2
llama_context: n_ctx         = 8192
llama_context: n_ctx_seq     = 4096
llama_context: n_batch       = 2048
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = enabled
llama_context: kv_unified    = false
llama_context: freq_base     = 1000000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_seq (4096) < n_ctx_train (32768) -- the full capacity of the model will not be utilized
set_abort_callback: call
llama_context:  CUDA_Host  output buffer size =     1.16 MiB
llama_kv_cache: layer   0: dev = CUDA0
llama_kv_cache: layer   1: dev = CUDA0
llama_kv_cache: layer   2: dev = CUDA0
llama_kv_cache: layer   3: dev = CUDA0
llama_kv_cache: layer   4: dev = CUDA1
llama_kv_cache: layer   5: dev = CUDA1
llama_kv_cache: layer   6: dev = CUDA1
llama_kv_cache: layer   7: dev = CUDA1
llama_kv_cache: layer   8: dev = CUDA2
llama_kv_cache: layer   9: dev = CUDA2
llama_kv_cache: layer  10: dev = CUDA2
llama_kv_cache: layer  11: dev = CUDA3
llama_kv_cache: layer  12: dev = CUDA3
llama_kv_cache: layer  13: dev = CUDA3
llama_kv_cache: layer  14: dev = CUDA3
llama_kv_cache: layer  15: dev = CUDA4
llama_kv_cache: layer  16: dev = CUDA4
llama_kv_cache: layer  17: dev = CUDA4
llama_kv_cache: layer  18: dev = CUDA4
llama_kv_cache: layer  19: dev = CUDA5
llama_kv_cache: layer  20: dev = CUDA5
llama_kv_cache: layer  21: dev = CUDA5
llama_kv_cache: layer  22: dev = CUDA6
llama_kv_cache: layer  23: dev = CUDA6
llama_kv_cache: layer  24: dev = CUDA6
llama_kv_cache: layer  25: dev = CUDA6
llama_kv_cache: layer  26: dev = CUDA7
llama_kv_cache: layer  27: dev = CUDA7
llama_kv_cache:      CUDA0 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA1 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA2 KV buffer size =    96.00 MiB
llama_kv_cache:      CUDA3 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA4 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA5 KV buffer size =    96.00 MiB
llama_kv_cache:      CUDA6 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA7 KV buffer size =    64.00 MiB
llama_kv_cache: size =  896.00 MiB (  4096 cells,  28 layers,  2/2 seqs), K (f16):  448.00 MiB, V (f16):  448.00 MiB
llama_context: enumerating backends
llama_context: backend_ptrs.size() = 9
llama_context: max_nodes = 2488
llama_context: pipeline parallelism enabled (n_copies=4)
llama_context: reserving full memory module
llama_context: worst-case: n_tokens = 512, n_seqs = 2, n_outputs = 2
graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  2, n_outputs =  512
graph_reserve: reserving a graph for ubatch with n_tokens =    2, n_seqs =  2, n_outputs =    2
graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  2, n_outputs =  512
llama_context:      CUDA0 compute buffer size =   100.04 MiB
llama_context:      CUDA1 compute buffer size =    96.04 MiB
llama_context:      CUDA2 compute buffer size =    96.04 MiB
llama_context:      CUDA3 compute buffer size =    96.04 MiB
llama_context:      CUDA4 compute buffer size =    96.04 MiB
llama_context:      CUDA5 compute buffer size =    96.04 MiB
llama_context:      CUDA6 compute buffer size =    96.04 MiB
llama_context:      CUDA7 compute buffer size =   348.80 MiB
llama_context:  CUDA_Host compute buffer size =    36.05 MiB
llama_context: graph nodes  = 1043
llama_context: graph splits = 9
llama_context: constructing llama_context
llama_context: n_seq_max     = 2
llama_context: n_ctx         = 8192
llama_context: n_ctx_seq     = 4096
llama_context: n_batch       = 2048
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = enabled
llama_context: kv_unified    = false
llama_context: freq_base     = 1000000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_seq (4096) < n_ctx_train (32768) -- the full capacity of the model will not be utilized
set_abort_callback: call
llama_context:  CUDA_Host  output buffer size =     1.16 MiB
llama_kv_cache: layer   0: dev = CUDA0
llama_kv_cache: layer   1: dev = CUDA0
llama_kv_cache: layer   2: dev = CUDA0
llama_kv_cache: layer   3: dev = CUDA0
llama_kv_cache: layer   4: dev = CUDA1
llama_kv_cache: layer   5: dev = CUDA1
llama_kv_cache: layer   6: dev = CUDA1
llama_kv_cache: layer   7: dev = CUDA1
llama_kv_cache: layer   8: dev = CUDA2
llama_kv_cache: layer   9: dev = CUDA2
llama_kv_cache: layer  10: dev = CUDA2
llama_kv_cache: layer  11: dev = CUDA3
llama_kv_cache: layer  12: dev = CUDA3
llama_kv_cache: layer  13: dev = CUDA3
llama_kv_cache: layer  14: dev = CUDA3
llama_kv_cache: layer  15: dev = CUDA4
llama_kv_cache: layer  16: dev = CUDA4
llama_kv_cache: layer  17: dev = CUDA4
llama_kv_cache: layer  18: dev = CUDA4
llama_kv_cache: layer  19: dev = CUDA5
llama_kv_cache: layer  20: dev = CUDA5
llama_kv_cache: layer  21: dev = CUDA5
llama_kv_cache: layer  22: dev = CUDA6
llama_kv_cache: layer  23: dev = CUDA6
llama_kv_cache: layer  24: dev = CUDA6
llama_kv_cache: layer  25: dev = CUDA6
llama_kv_cache: layer  26: dev = CUDA7
llama_kv_cache: layer  27: dev = CUDA7
llama_kv_cache:      CUDA0 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA1 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA2 KV buffer size =    96.00 MiB
llama_kv_cache:      CUDA3 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA4 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA5 KV buffer size =    96.00 MiB
llama_kv_cache:      CUDA6 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA7 KV buffer size =    64.00 MiB
llama_kv_cache: size =  896.00 MiB (  4096 cells,  28 layers,  2/2 seqs), K (f16):  448.00 MiB, V (f16):  448.00 MiB
llama_context: enumerating backends
llama_context: backend_ptrs.size() = 9
llama_context: max_nodes = 2488
llama_context: pipeline parallelism enabled (n_copies=4)
llama_context: reserving full memory module
llama_context: worst-case: n_tokens = 512, n_seqs = 2, n_outputs = 2
graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  2, n_outputs =  512
graph_reserve: reserving a graph for ubatch with n_tokens =    2, n_seqs =  2, n_outputs =    2
graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  2, n_outputs =  512
llama_context:      CUDA0 compute buffer size =   100.04 MiB
llama_context:      CUDA1 compute buffer size =    96.04 MiB
llama_context:      CUDA2 compute buffer size =    96.04 MiB
llama_context:      CUDA3 compute buffer size =    96.04 MiB
llama_context:      CUDA4 compute buffer size =    96.04 MiB
llama_context:      CUDA5 compute buffer size =    96.04 MiB
llama_context:      CUDA6 compute buffer size =    96.04 MiB
llama_context:      CUDA7 compute buffer size =   348.80 MiB
llama_context:  CUDA_Host compute buffer size =    36.05 MiB
llama_context: graph nodes  = 1043
llama_context: graph splits = 9
llama_context: constructing llama_context
llama_context: n_seq_max     = 2
llama_context: n_ctx         = 8192
llama_context: n_ctx_seq     = 4096
llama_context: n_batch       = 2048
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = enabled
llama_context: kv_unified    = false
llama_context: freq_base     = 1000000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_seq (4096) < n_ctx_train (32768) -- the full capacity of the model will not be utilized
set_abort_callback: call
llama_context:  CUDA_Host  output buffer size =     1.16 MiB
llama_kv_cache: layer   0: dev = CUDA0
llama_kv_cache: layer   1: dev = CUDA0
llama_kv_cache: layer   2: dev = CUDA0
llama_kv_cache: layer   3: dev = CUDA0
llama_kv_cache: layer   4: dev = CUDA1
llama_kv_cache: layer   5: dev = CUDA1
llama_kv_cache: layer   6: dev = CUDA1
llama_kv_cache: layer   7: dev = CUDA1
llama_kv_cache: layer   8: dev = CUDA2
llama_kv_cache: layer   9: dev = CUDA2
llama_kv_cache: layer  10: dev = CUDA2
llama_kv_cache: layer  11: dev = CUDA3
llama_kv_cache: layer  12: dev = CUDA3
llama_kv_cache: layer  13: dev = CUDA3
llama_kv_cache: layer  14: dev = CUDA3
llama_kv_cache: layer  15: dev = CUDA4
llama_kv_cache: layer  16: dev = CUDA4
llama_kv_cache: layer  17: dev = CUDA4
llama_kv_cache: layer  18: dev = CUDA4
llama_kv_cache: layer  19: dev = CUDA5
llama_kv_cache: layer  20: dev = CUDA5
llama_kv_cache: layer  21: dev = CUDA5
llama_kv_cache: layer  22: dev = CUDA6
llama_kv_cache: layer  23: dev = CUDA6
llama_kv_cache: layer  24: dev = CUDA6
llama_kv_cache: layer  25: dev = CUDA6
llama_kv_cache: layer  26: dev = CUDA7
llama_kv_cache: layer  27: dev = CUDA7
llama_kv_cache:      CUDA0 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA1 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA2 KV buffer size =    96.00 MiB
llama_kv_cache:      CUDA3 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA4 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA5 KV buffer size =    96.00 MiB
llama_kv_cache:      CUDA6 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA7 KV buffer size =    64.00 MiB
llama_kv_cache: size =  896.00 MiB (  4096 cells,  28 layers,  2/2 seqs), K (f16):  448.00 MiB, V (f16):  448.00 MiB
llama_context: enumerating backends
llama_context: backend_ptrs.size() = 9
llama_context: max_nodes = 2488
llama_context: pipeline parallelism enabled (n_copies=4)
llama_context: reserving full memory module
llama_context: worst-case: n_tokens = 512, n_seqs = 2, n_outputs = 2
graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  2, n_outputs =  512
graph_reserve: reserving a graph for ubatch with n_tokens =    2, n_seqs =  2, n_outputs =    2
graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  2, n_outputs =  512
llama_context:      CUDA0 compute buffer size =   100.04 MiB
llama_context:      CUDA1 compute buffer size =    96.04 MiB
llama_context:      CUDA2 compute buffer size =    96.04 MiB
llama_context:      CUDA3 compute buffer size =    96.04 MiB
llama_context:      CUDA4 compute buffer size =    96.04 MiB
llama_context:      CUDA5 compute buffer size =    96.04 MiB
llama_context:      CUDA6 compute buffer size =    96.04 MiB
llama_context:      CUDA7 compute buffer size =   348.80 MiB
llama_context:  CUDA_Host compute buffer size =    36.05 MiB
llama_context: graph nodes  = 1043
llama_context: graph splits = 9
llama_context: constructing llama_context
llama_context: n_seq_max     = 2
llama_context: n_ctx         = 8192
llama_context: n_ctx_seq     = 4096
llama_context: n_batch       = 2048
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = enabled
llama_context: kv_unified    = false
llama_context: freq_base     = 1000000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_seq (4096) < n_ctx_train (32768) -- the full capacity of the model will not be utilized
set_abort_callback: call
llama_context:  CUDA_Host  output buffer size =     1.16 MiB
llama_kv_cache: layer   0: dev = CUDA0
llama_kv_cache: layer   1: dev = CUDA0
llama_kv_cache: layer   2: dev = CUDA0
llama_kv_cache: layer   3: dev = CUDA0
llama_kv_cache: layer   4: dev = CUDA1
llama_kv_cache: layer   5: dev = CUDA1
llama_kv_cache: layer   6: dev = CUDA1
llama_kv_cache: layer   7: dev = CUDA1
llama_kv_cache: layer   8: dev = CUDA2
llama_kv_cache: layer   9: dev = CUDA2
llama_kv_cache: layer  10: dev = CUDA2
llama_kv_cache: layer  11: dev = CUDA3
llama_kv_cache: layer  12: dev = CUDA3
llama_kv_cache: layer  13: dev = CUDA3
llama_kv_cache: layer  14: dev = CUDA3
llama_kv_cache: layer  15: dev = CUDA4
llama_kv_cache: layer  16: dev = CUDA4
llama_kv_cache: layer  17: dev = CUDA4
llama_kv_cache: layer  18: dev = CUDA4
llama_kv_cache: layer  19: dev = CUDA5
llama_kv_cache: layer  20: dev = CUDA5
llama_kv_cache: layer  21: dev = CUDA5
llama_kv_cache: layer  22: dev = CUDA6
llama_kv_cache: layer  23: dev = CUDA6
llama_kv_cache: layer  24: dev = CUDA6
llama_kv_cache: layer  25: dev = CUDA6
llama_kv_cache: layer  26: dev = CUDA7
llama_kv_cache: layer  27: dev = CUDA7
llama_kv_cache:      CUDA0 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA1 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA2 KV buffer size =    96.00 MiB
llama_kv_cache:      CUDA3 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA4 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA5 KV buffer size =    96.00 MiB
llama_kv_cache:      CUDA6 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA7 KV buffer size =    64.00 MiB
llama_kv_cache: size =  896.00 MiB (  4096 cells,  28 layers,  2/2 seqs), K (f16):  448.00 MiB, V (f16):  448.00 MiB
llama_context: enumerating backends
llama_context: backend_ptrs.size() = 9
llama_context: max_nodes = 2488
llama_context: pipeline parallelism enabled (n_copies=4)
llama_context: reserving full memory module
llama_context: worst-case: n_tokens = 512, n_seqs = 2, n_outputs = 2
graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  2, n_outputs =  512
graph_reserve: reserving a graph for ubatch with n_tokens =    2, n_seqs =  2, n_outputs =    2
graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  2, n_outputs =  512
llama_context:      CUDA0 compute buffer size =   100.04 MiB
llama_context:      CUDA1 compute buffer size =    96.04 MiB
llama_context:      CUDA2 compute buffer size =    96.04 MiB
llama_context:      CUDA3 compute buffer size =    96.04 MiB
llama_context:      CUDA4 compute buffer size =    96.04 MiB
llama_context:      CUDA5 compute buffer size =    96.04 MiB
llama_context:      CUDA6 compute buffer size =    96.04 MiB
llama_context:      CUDA7 compute buffer size =   348.80 MiB
llama_context:  CUDA_Host compute buffer size =    36.05 MiB
llama_context: graph nodes  = 1043
llama_context: graph splits = 9
llama_context: constructing llama_context
llama_context: n_seq_max     = 2
llama_context: n_ctx         = 8192
llama_context: n_ctx_seq     = 4096
llama_context: n_batch       = 2048
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = enabled
llama_context: kv_unified    = false
llama_context: freq_base     = 1000000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_seq (4096) < n_ctx_train (32768) -- the full capacity of the model will not be utilized
set_abort_callback: call
llama_context:  CUDA_Host  output buffer size =     1.16 MiB
llama_kv_cache: layer   0: dev = CUDA0
llama_kv_cache: layer   1: dev = CUDA0
llama_kv_cache: layer   2: dev = CUDA0
llama_kv_cache: layer   3: dev = CUDA0
llama_kv_cache: layer   4: dev = CUDA1
llama_kv_cache: layer   5: dev = CUDA1
llama_kv_cache: layer   6: dev = CUDA1
llama_kv_cache: layer   7: dev = CUDA1
llama_kv_cache: layer   8: dev = CUDA2
llama_kv_cache: layer   9: dev = CUDA2
llama_kv_cache: layer  10: dev = CUDA2
llama_kv_cache: layer  11: dev = CUDA3
llama_kv_cache: layer  12: dev = CUDA3
llama_kv_cache: layer  13: dev = CUDA3
llama_kv_cache: layer  14: dev = CUDA3
llama_kv_cache: layer  15: dev = CUDA4
llama_kv_cache: layer  16: dev = CUDA4
llama_kv_cache: layer  17: dev = CUDA4
llama_kv_cache: layer  18: dev = CUDA4
llama_kv_cache: layer  19: dev = CUDA5
llama_kv_cache: layer  20: dev = CUDA5
llama_kv_cache: layer  21: dev = CUDA5
llama_kv_cache: layer  22: dev = CUDA6
llama_kv_cache: layer  23: dev = CUDA6
llama_kv_cache: layer  24: dev = CUDA6
llama_kv_cache: layer  25: dev = CUDA6
llama_kv_cache: layer  26: dev = CUDA7
llama_kv_cache: layer  27: dev = CUDA7
llama_kv_cache:      CUDA0 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA1 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA2 KV buffer size =    96.00 MiB
llama_kv_cache:      CUDA3 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA4 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA5 KV buffer size =    96.00 MiB
llama_kv_cache:      CUDA6 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA7 KV buffer size =    64.00 MiB
llama_kv_cache: size =  896.00 MiB (  4096 cells,  28 layers,  2/2 seqs), K (f16):  448.00 MiB, V (f16):  448.00 MiB
llama_context: enumerating backends
llama_context: backend_ptrs.size() = 9
llama_context: max_nodes = 2488
llama_context: pipeline parallelism enabled (n_copies=4)
llama_context: reserving full memory module
llama_context: worst-case: n_tokens = 512, n_seqs = 2, n_outputs = 2
graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  2, n_outputs =  512
graph_reserve: reserving a graph for ubatch with n_tokens =    2, n_seqs =  2, n_outputs =    2
graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  2, n_outputs =  512
llama_context:      CUDA0 compute buffer size =   100.04 MiB
llama_context:      CUDA1 compute buffer size =    96.04 MiB
llama_context:      CUDA2 compute buffer size =    96.04 MiB
llama_context:      CUDA3 compute buffer size =    96.04 MiB
llama_context:      CUDA4 compute buffer size =    96.04 MiB
llama_context:      CUDA5 compute buffer size =    96.04 MiB
llama_context:      CUDA6 compute buffer size =    96.04 MiB
llama_context:      CUDA7 compute buffer size =   348.80 MiB
llama_context:  CUDA_Host compute buffer size =    36.05 MiB
llama_context: graph nodes  = 1043
llama_context: graph splits = 9
llama_context: constructing llama_context
llama_context: n_seq_max     = 2
llama_context: n_ctx         = 8192
llama_context: n_ctx_seq     = 4096
llama_context: n_batch       = 2048
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = enabled
llama_context: kv_unified    = false
llama_context: freq_base     = 1000000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_seq (4096) < n_ctx_train (32768) -- the full capacity of the model will not be utilized
set_abort_callback: call
llama_context:  CUDA_Host  output buffer size =     1.16 MiB
llama_kv_cache: layer   0: dev = CUDA0
llama_kv_cache: layer   1: dev = CUDA0
llama_kv_cache: layer   2: dev = CUDA0
llama_kv_cache: layer   3: dev = CUDA0
llama_kv_cache: layer   4: dev = CUDA1
llama_kv_cache: layer   5: dev = CUDA1
llama_kv_cache: layer   6: dev = CUDA1
llama_kv_cache: layer   7: dev = CUDA1
llama_kv_cache: layer   8: dev = CUDA2
llama_kv_cache: layer   9: dev = CUDA2
llama_kv_cache: layer  10: dev = CUDA2
llama_kv_cache: layer  11: dev = CUDA3
llama_kv_cache: layer  12: dev = CUDA3
llama_kv_cache: layer  13: dev = CUDA3
llama_kv_cache: layer  14: dev = CUDA3
llama_kv_cache: layer  15: dev = CUDA4
llama_kv_cache: layer  16: dev = CUDA4
llama_kv_cache: layer  17: dev = CUDA4
llama_kv_cache: layer  18: dev = CUDA4
llama_kv_cache: layer  19: dev = CUDA5
llama_kv_cache: layer  20: dev = CUDA5
llama_kv_cache: layer  21: dev = CUDA5
llama_kv_cache: layer  22: dev = CUDA6
llama_kv_cache: layer  23: dev = CUDA6
llama_kv_cache: layer  24: dev = CUDA6
llama_kv_cache: layer  25: dev = CUDA6
llama_kv_cache: layer  26: dev = CUDA7
llama_kv_cache: layer  27: dev = CUDA7
llama_kv_cache:      CUDA0 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA1 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA2 KV buffer size =    96.00 MiB
llama_kv_cache:      CUDA3 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA4 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA5 KV buffer size =    96.00 MiB
llama_kv_cache:      CUDA6 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA7 KV buffer size =    64.00 MiB
llama_kv_cache: size =  896.00 MiB (  4096 cells,  28 layers,  2/2 seqs), K (f16):  448.00 MiB, V (f16):  448.00 MiB
llama_context: enumerating backends
llama_context: backend_ptrs.size() = 9
llama_context: max_nodes = 2488
llama_context: pipeline parallelism enabled (n_copies=4)
llama_context: reserving full memory module
llama_context: worst-case: n_tokens = 512, n_seqs = 2, n_outputs = 2
graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  2, n_outputs =  512
graph_reserve: reserving a graph for ubatch with n_tokens =    2, n_seqs =  2, n_outputs =    2
graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  2, n_outputs =  512
llama_context:      CUDA0 compute buffer size =   100.04 MiB
llama_context:      CUDA1 compute buffer size =    96.04 MiB
llama_context:      CUDA2 compute buffer size =    96.04 MiB
llama_context:      CUDA3 compute buffer size =    96.04 MiB
llama_context:      CUDA4 compute buffer size =    96.04 MiB
llama_context:      CUDA5 compute buffer size =    96.04 MiB
llama_context:      CUDA6 compute buffer size =    96.04 MiB
llama_context:      CUDA7 compute buffer size =   348.80 MiB
llama_context:  CUDA_Host compute buffer size =    36.05 MiB
llama_context: graph nodes  = 1043
llama_context: graph splits = 9
llama_context: constructing llama_context
llama_context: n_seq_max     = 2
llama_context: n_ctx         = 8192
llama_context: n_ctx_seq     = 4096
llama_context: n_batch       = 2048
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = enabled
llama_context: kv_unified    = false
llama_context: freq_base     = 1000000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_seq (4096) < n_ctx_train (32768) -- the full capacity of the model will not be utilized
set_abort_callback: call
llama_context:  CUDA_Host  output buffer size =     1.16 MiB
llama_kv_cache: layer   0: dev = CUDA0
llama_kv_cache: layer   1: dev = CUDA0
llama_kv_cache: layer   2: dev = CUDA0
llama_kv_cache: layer   3: dev = CUDA0
llama_kv_cache: layer   4: dev = CUDA1
llama_kv_cache: layer   5: dev = CUDA1
llama_kv_cache: layer   6: dev = CUDA1
llama_kv_cache: layer   7: dev = CUDA1
llama_kv_cache: layer   8: dev = CUDA2
llama_kv_cache: layer   9: dev = CUDA2
llama_kv_cache: layer  10: dev = CUDA2
llama_kv_cache: layer  11: dev = CUDA3
llama_kv_cache: layer  12: dev = CUDA3
llama_kv_cache: layer  13: dev = CUDA3
llama_kv_cache: layer  14: dev = CUDA3
llama_kv_cache: layer  15: dev = CUDA4
llama_kv_cache: layer  16: dev = CUDA4
llama_kv_cache: layer  17: dev = CUDA4
llama_kv_cache: layer  18: dev = CUDA4
llama_kv_cache: layer  19: dev = CUDA5
llama_kv_cache: layer  20: dev = CUDA5
llama_kv_cache: layer  21: dev = CUDA5
llama_kv_cache: layer  22: dev = CUDA6
llama_kv_cache: layer  23: dev = CUDA6
llama_kv_cache: layer  24: dev = CUDA6
llama_kv_cache: layer  25: dev = CUDA6
llama_kv_cache: layer  26: dev = CUDA7
llama_kv_cache: layer  27: dev = CUDA7
llama_kv_cache:      CUDA0 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA1 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA2 KV buffer size =    96.00 MiB
llama_kv_cache:      CUDA3 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA4 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA5 KV buffer size =    96.00 MiB
llama_kv_cache:      CUDA6 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA7 KV buffer size =    64.00 MiB
llama_kv_cache: size =  896.00 MiB (  4096 cells,  28 layers,  2/2 seqs), K (f16):  448.00 MiB, V (f16):  448.00 MiB
llama_context: enumerating backends
llama_context: backend_ptrs.size() = 9
llama_context: max_nodes = 2488
llama_context: pipeline parallelism enabled (n_copies=4)
llama_context: reserving full memory module
llama_context: worst-case: n_tokens = 512, n_seqs = 2, n_outputs = 2
graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  2, n_outputs =  512
graph_reserve: reserving a graph for ubatch with n_tokens =    2, n_seqs =  2, n_outputs =    2
graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  2, n_outputs =  512
llama_context:      CUDA0 compute buffer size =   100.04 MiB
llama_context:      CUDA1 compute buffer size =    96.04 MiB
llama_context:      CUDA2 compute buffer size =    96.04 MiB
llama_context:      CUDA3 compute buffer size =    96.04 MiB
llama_context:      CUDA4 compute buffer size =    96.04 MiB
llama_context:      CUDA5 compute buffer size =    96.04 MiB
llama_context:      CUDA6 compute buffer size =    96.04 MiB
llama_context:      CUDA7 compute buffer size =   348.80 MiB
llama_context:  CUDA_Host compute buffer size =    36.05 MiB
llama_context: graph nodes  = 1043
llama_context: graph splits = 9
llama_context: constructing llama_context
llama_context: n_seq_max     = 2
llama_context: n_ctx         = 8192
llama_context: n_ctx_seq     = 4096
llama_context: n_batch       = 2048
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = enabled
llama_context: kv_unified    = false
llama_context: freq_base     = 1000000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_seq (4096) < n_ctx_train (32768) -- the full capacity of the model will not be utilized
set_abort_callback: call
llama_context:  CUDA_Host  output buffer size =     1.16 MiB
llama_kv_cache: layer   0: dev = CUDA0
llama_kv_cache: layer   1: dev = CUDA0
llama_kv_cache: layer   2: dev = CUDA0
llama_kv_cache: layer   3: dev = CUDA0
llama_kv_cache: layer   4: dev = CUDA1
llama_kv_cache: layer   5: dev = CUDA1
llama_kv_cache: layer   6: dev = CUDA1
llama_kv_cache: layer   7: dev = CUDA1
llama_kv_cache: layer   8: dev = CUDA2
llama_kv_cache: layer   9: dev = CUDA2
llama_kv_cache: layer  10: dev = CUDA2
llama_kv_cache: layer  11: dev = CUDA3
llama_kv_cache: layer  12: dev = CUDA3
llama_kv_cache: layer  13: dev = CUDA3
llama_kv_cache: layer  14: dev = CUDA3
llama_kv_cache: layer  15: dev = CUDA4
llama_kv_cache: layer  16: dev = CUDA4
llama_kv_cache: layer  17: dev = CUDA4
llama_kv_cache: layer  18: dev = CUDA4
llama_kv_cache: layer  19: dev = CUDA5
llama_kv_cache: layer  20: dev = CUDA5
llama_kv_cache: layer  21: dev = CUDA5
llama_kv_cache: layer  22: dev = CUDA6
llama_kv_cache: layer  23: dev = CUDA6
llama_kv_cache: layer  24: dev = CUDA6
llama_kv_cache: layer  25: dev = CUDA6
llama_kv_cache: layer  26: dev = CUDA7
llama_kv_cache: layer  27: dev = CUDA7
llama_kv_cache:      CUDA0 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA1 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA2 KV buffer size =    96.00 MiB
llama_kv_cache:      CUDA3 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA4 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA5 KV buffer size =    96.00 MiB
llama_kv_cache:      CUDA6 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA7 KV buffer size =    64.00 MiB
llama_kv_cache: size =  896.00 MiB (  4096 cells,  28 layers,  2/2 seqs), K (f16):  448.00 MiB, V (f16):  448.00 MiB
llama_context: enumerating backends
llama_context: backend_ptrs.size() = 9
llama_context: max_nodes = 2488
llama_context: pipeline parallelism enabled (n_copies=4)
llama_context: reserving full memory module
llama_context: worst-case: n_tokens = 512, n_seqs = 2, n_outputs = 2
graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  2, n_outputs =  512
graph_reserve: reserving a graph for ubatch with n_tokens =    2, n_seqs =  2, n_outputs =    2
graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  2, n_outputs =  512
llama_context:      CUDA0 compute buffer size =   100.04 MiB
llama_context:      CUDA1 compute buffer size =    96.04 MiB
llama_context:      CUDA2 compute buffer size =    96.04 MiB
llama_context:      CUDA3 compute buffer size =    96.04 MiB
llama_context:      CUDA4 compute buffer size =    96.04 MiB
llama_context:      CUDA5 compute buffer size =    96.04 MiB
llama_context:      CUDA6 compute buffer size =    96.04 MiB
llama_context:      CUDA7 compute buffer size =   348.80 MiB
llama_context:  CUDA_Host compute buffer size =    36.05 MiB
llama_context: graph nodes  = 1043
llama_context: graph splits = 9

Run 1/3:

Run 2/3:

Run 3/3:

Average across 3 runs:

Wall Time: 1056.42 ms
Tokens Generated: 149
Throughput: 141.04 tokens/sec

Top 10 Time-Consuming Sections:
Section                                  Total (ms)   Avg (ms)     Calls   
--------------------------------------------------------------------------------
generation_phase                         990.04       990.04       1       
token_sampling                           666.51       10.10        66      
telemetry_cpu_sampling                   666.47       10.10        66      
telemetry_cpu_loop                       666.42       10.10        66      
denoising_step_0                         471.00       14.27        33      
denoising_step_1                         405.09       12.28        33      
llama_decode                             208.75       3.16         66      
finalize_block                           113.72       3.45         33      
block_5                                  33.19        33.19        1       
block_8                                  30.04        30.04        1       

================================================================================
Running test 8/16: Iterative Refinement (block=8, steps=3, rounds=3)
================================================================================

Run 1/3:

Run 2/3:

Run 3/3:

Average across 3 runs:

Wall Time: 1135.77 ms
Tokens Generated: 149
Throughput: 131.19 tokens/sec

Top 10 Time-Consuming Sections:
Section                                  Total (ms)   Avg (ms)     Calls   
--------------------------------------------------------------------------------
generation_phase                         1069.18      1069.18      1       
token_sampling                           698.00       14.24        49      
telemetry_cpu_sampling                   697.96       14.24        49      
telemetry_cpu_loop                       697.91       14.24        49      
denoising_step_0                         347.77       20.46        17      
denoising_step_1                         310.63       18.27        17      
denoising_step_2                         310.43       19.40        16      
llama_decode                             270.04       5.51         49      
finalize_block                           100.11       5.89         17      
block_13                                 65.11        65.11        1       

================================================================================
Running test 9/16: Iterative Refinement (block=8, steps=3, rounds=4)
================================================================================

Run 1/3:

Run 2/3:

Run 3/3:

Average across 3 runs:

Wall Time: 1142.76 ms
Tokens Generated: 149
Throughput: 130.39 tokens/sec

Top 10 Time-Consuming Sections:
Section                                  Total (ms)   Avg (ms)     Calls   
--------------------------------------------------------------------------------
generation_phase                         1077.33      1077.33      1       
token_sampling                           704.14       14.37        49      
telemetry_cpu_sampling                   704.10       14.37        49      
telemetry_cpu_loop                       704.05       14.37        49      
denoising_step_0                         349.46       20.56        17      
denoising_step_1                         315.06       18.53        17      
denoising_step_2                         311.94       19.50        16      
llama_decode                             271.54       5.54         49      
finalize_block                           100.60       5.92         17      
block_15                                 68.23        68.23        1       

================================================================================
Running test 10/16: Iterative Refinement (block=4, steps=3, rounds=3)
================================================================================

Run 1/3:

Run 2/3:

Run 3/3:

Average across 3 runs:

Wall Time: 1448.97 ms
Tokens Generated: 149
Throughput: 102.83 tokens/sec

Top 10 Time-Consuming Sections:
Section                                  Total (ms)   Avg (ms)     Calls   
--------------------------------------------------------------------------------
generation_phase                         1382.80      1382.80      1       
token_sampling                           970.08       9.90         98      
telemetry_cpu_sampling                   970.02       9.90         98      
telemetry_cpu_loop                       969.95       9.90         98      
denoising_step_0                         471.11       14.28        33      
denoising_step_1                         404.78       12.27        33      
denoising_step_2                         392.63       11.90        33      
llama_decode                             297.23       3.03         98      
finalize_block                           113.99       3.45         33      
block_20                                 43.10        43.10        1       

================================================================================
Running test 11/16: Iterative Refinement (block=8, steps=4, rounds=3)
================================================================================

Run 1/3:

Run 2/3:

Run 3/3:

Average across 3 runs:

Wall Time: 1453.42 ms
Tokens Generated: 149
Throughput: 102.52 tokens/sec

Top 10 Time-Consuming Sections:
Section                                  Total (ms)   Avg (ms)     Calls   
--------------------------------------------------------------------------------
generation_phase                         1388.85      1388.85      1       
token_sampling                           934.68       14.16        66      
telemetry_cpu_sampling                   934.63       14.16        66      
telemetry_cpu_loop                       934.55       14.16        66      
llama_decode                             354.05       5.36         66      
denoising_step_0                         345.17       20.30        17      
denoising_step_1                         327.74       19.28        17      
denoising_step_2                         308.62       18.15        17      
denoising_step_3                         308.18       19.26        16      
finalize_block                           98.84        5.81         17      

================================================================================
Running test 12/16: Iterative Refinement (block=4, steps=2, rounds=4) + GPU
================================================================================

Run 1/3:

Run 2/3:

Run 3/3:

Average across 3 runs:

Wall Time: 780.40 ms
Tokens Generated: 149
Throughput: 190.93 tokens/sec

Top 10 Time-Consuming Sections:
Section                                  Total (ms)   Avg (ms)     Calls   
--------------------------------------------------------------------------------
generation_phase                         702.79       702.79       1       
token_sampling                           376.08       5.70         66      
denoising_step_0                         326.97       9.91         33      
denoising_step_1                         259.92       7.88         33      
telemetry_gpu_logit_pack                 246.28       3.73         66      
llama_decode                             209.93       3.18         66      
finalize_block                           115.60       3.50         33      
telemetry_gpu_invoke                     114.71       1.74         66      
telemetry_gpu_success                    66.00        1.00         66      
telemetry_gpu_stage_cpu_post             53.66        0.81         66      

================================================================================
Running test 13/16: Iterative Refinement (block=8, steps=3, rounds=3) + GPU
================================================================================

Run 1/3:

Run 2/3:

Run 3/3:

Average across 3 runs:

Wall Time: 724.58 ms
Tokens Generated: 149
Throughput: 205.64 tokens/sec

Top 10 Time-Consuming Sections:
Section                                  Total (ms)   Avg (ms)     Calls   
llama_context: constructing llama_context
llama_context: n_seq_max     = 2
llama_context: n_ctx         = 8192
llama_context: n_ctx_seq     = 4096
llama_context: n_batch       = 2048
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = enabled
llama_context: kv_unified    = false
llama_context: freq_base     = 1000000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_seq (4096) < n_ctx_train (32768) -- the full capacity of the model will not be utilized
set_abort_callback: call
llama_context:  CUDA_Host  output buffer size =     1.16 MiB
llama_kv_cache: layer   0: dev = CUDA0
llama_kv_cache: layer   1: dev = CUDA0
llama_kv_cache: layer   2: dev = CUDA0
llama_kv_cache: layer   3: dev = CUDA0
llama_kv_cache: layer   4: dev = CUDA1
llama_kv_cache: layer   5: dev = CUDA1
llama_kv_cache: layer   6: dev = CUDA1
llama_kv_cache: layer   7: dev = CUDA1
llama_kv_cache: layer   8: dev = CUDA2
llama_kv_cache: layer   9: dev = CUDA2
llama_kv_cache: layer  10: dev = CUDA2
llama_kv_cache: layer  11: dev = CUDA3
llama_kv_cache: layer  12: dev = CUDA3
llama_kv_cache: layer  13: dev = CUDA3
llama_kv_cache: layer  14: dev = CUDA3
llama_kv_cache: layer  15: dev = CUDA4
llama_kv_cache: layer  16: dev = CUDA4
llama_kv_cache: layer  17: dev = CUDA4
llama_kv_cache: layer  18: dev = CUDA4
llama_kv_cache: layer  19: dev = CUDA5
llama_kv_cache: layer  20: dev = CUDA5
llama_kv_cache: layer  21: dev = CUDA5
llama_kv_cache: layer  22: dev = CUDA6
llama_kv_cache: layer  23: dev = CUDA6
llama_kv_cache: layer  24: dev = CUDA6
llama_kv_cache: layer  25: dev = CUDA6
llama_kv_cache: layer  26: dev = CUDA7
llama_kv_cache: layer  27: dev = CUDA7
llama_kv_cache:      CUDA0 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA1 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA2 KV buffer size =    96.00 MiB
llama_kv_cache:      CUDA3 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA4 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA5 KV buffer size =    96.00 MiB
llama_kv_cache:      CUDA6 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA7 KV buffer size =    64.00 MiB
llama_kv_cache: size =  896.00 MiB (  4096 cells,  28 layers,  2/2 seqs), K (f16):  448.00 MiB, V (f16):  448.00 MiB
llama_context: enumerating backends
llama_context: backend_ptrs.size() = 9
llama_context: max_nodes = 2488
llama_context: pipeline parallelism enabled (n_copies=4)
llama_context: reserving full memory module
llama_context: worst-case: n_tokens = 512, n_seqs = 2, n_outputs = 2
graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  2, n_outputs =  512
graph_reserve: reserving a graph for ubatch with n_tokens =    2, n_seqs =  2, n_outputs =    2
graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  2, n_outputs =  512
llama_context:      CUDA0 compute buffer size =   100.04 MiB
llama_context:      CUDA1 compute buffer size =    96.04 MiB
llama_context:      CUDA2 compute buffer size =    96.04 MiB
llama_context:      CUDA3 compute buffer size =    96.04 MiB
llama_context:      CUDA4 compute buffer size =    96.04 MiB
llama_context:      CUDA5 compute buffer size =    96.04 MiB
llama_context:      CUDA6 compute buffer size =    96.04 MiB
llama_context:      CUDA7 compute buffer size =   348.80 MiB
llama_context:  CUDA_Host compute buffer size =    36.05 MiB
llama_context: graph nodes  = 1043
llama_context: graph splits = 9
llama_context: constructing llama_context
llama_context: n_seq_max     = 2
llama_context: n_ctx         = 8192
llama_context: n_ctx_seq     = 4096
llama_context: n_batch       = 2048
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = enabled
llama_context: kv_unified    = false
llama_context: freq_base     = 1000000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_seq (4096) < n_ctx_train (32768) -- the full capacity of the model will not be utilized
set_abort_callback: call
llama_context:  CUDA_Host  output buffer size =     1.16 MiB
llama_kv_cache: layer   0: dev = CUDA0
llama_kv_cache: layer   1: dev = CUDA0
llama_kv_cache: layer   2: dev = CUDA0
llama_kv_cache: layer   3: dev = CUDA0
llama_kv_cache: layer   4: dev = CUDA1
llama_kv_cache: layer   5: dev = CUDA1
llama_kv_cache: layer   6: dev = CUDA1
llama_kv_cache: layer   7: dev = CUDA1
llama_kv_cache: layer   8: dev = CUDA2
llama_kv_cache: layer   9: dev = CUDA2
llama_kv_cache: layer  10: dev = CUDA2
llama_kv_cache: layer  11: dev = CUDA3
llama_kv_cache: layer  12: dev = CUDA3
llama_kv_cache: layer  13: dev = CUDA3
llama_kv_cache: layer  14: dev = CUDA3
llama_kv_cache: layer  15: dev = CUDA4
llama_kv_cache: layer  16: dev = CUDA4
llama_kv_cache: layer  17: dev = CUDA4
llama_kv_cache: layer  18: dev = CUDA4
llama_kv_cache: layer  19: dev = CUDA5
llama_kv_cache: layer  20: dev = CUDA5
llama_kv_cache: layer  21: dev = CUDA5
llama_kv_cache: layer  22: dev = CUDA6
llama_kv_cache: layer  23: dev = CUDA6
llama_kv_cache: layer  24: dev = CUDA6
llama_kv_cache: layer  25: dev = CUDA6
llama_kv_cache: layer  26: dev = CUDA7
llama_kv_cache: layer  27: dev = CUDA7
llama_kv_cache:      CUDA0 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA1 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA2 KV buffer size =    96.00 MiB
llama_kv_cache:      CUDA3 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA4 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA5 KV buffer size =    96.00 MiB
llama_kv_cache:      CUDA6 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA7 KV buffer size =    64.00 MiB
llama_kv_cache: size =  896.00 MiB (  4096 cells,  28 layers,  2/2 seqs), K (f16):  448.00 MiB, V (f16):  448.00 MiB
llama_context: enumerating backends
llama_context: backend_ptrs.size() = 9
llama_context: max_nodes = 2488
llama_context: pipeline parallelism enabled (n_copies=4)
llama_context: reserving full memory module
llama_context: worst-case: n_tokens = 512, n_seqs = 2, n_outputs = 2
graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  2, n_outputs =  512
graph_reserve: reserving a graph for ubatch with n_tokens =    2, n_seqs =  2, n_outputs =    2
graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  2, n_outputs =  512
llama_context:      CUDA0 compute buffer size =   100.04 MiB
llama_context:      CUDA1 compute buffer size =    96.04 MiB
llama_context:      CUDA2 compute buffer size =    96.04 MiB
llama_context:      CUDA3 compute buffer size =    96.04 MiB
llama_context:      CUDA4 compute buffer size =    96.04 MiB
llama_context:      CUDA5 compute buffer size =    96.04 MiB
llama_context:      CUDA6 compute buffer size =    96.04 MiB
llama_context:      CUDA7 compute buffer size =   348.80 MiB
llama_context:  CUDA_Host compute buffer size =    36.05 MiB
llama_context: graph nodes  = 1043
llama_context: graph splits = 9
llama_context: constructing llama_context
llama_context: n_seq_max     = 2
llama_context: n_ctx         = 8192
llama_context: n_ctx_seq     = 4096
llama_context: n_batch       = 2048
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = enabled
llama_context: kv_unified    = false
llama_context: freq_base     = 1000000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_seq (4096) < n_ctx_train (32768) -- the full capacity of the model will not be utilized
set_abort_callback: call
llama_context:  CUDA_Host  output buffer size =     1.16 MiB
llama_kv_cache: layer   0: dev = CUDA0
llama_kv_cache: layer   1: dev = CUDA0
llama_kv_cache: layer   2: dev = CUDA0
llama_kv_cache: layer   3: dev = CUDA0
llama_kv_cache: layer   4: dev = CUDA1
llama_kv_cache: layer   5: dev = CUDA1
llama_kv_cache: layer   6: dev = CUDA1
llama_kv_cache: layer   7: dev = CUDA1
llama_kv_cache: layer   8: dev = CUDA2
llama_kv_cache: layer   9: dev = CUDA2
llama_kv_cache: layer  10: dev = CUDA2
llama_kv_cache: layer  11: dev = CUDA3
llama_kv_cache: layer  12: dev = CUDA3
llama_kv_cache: layer  13: dev = CUDA3
llama_kv_cache: layer  14: dev = CUDA3
llama_kv_cache: layer  15: dev = CUDA4
llama_kv_cache: layer  16: dev = CUDA4
llama_kv_cache: layer  17: dev = CUDA4
llama_kv_cache: layer  18: dev = CUDA4
llama_kv_cache: layer  19: dev = CUDA5
llama_kv_cache: layer  20: dev = CUDA5
llama_kv_cache: layer  21: dev = CUDA5
llama_kv_cache: layer  22: dev = CUDA6
llama_kv_cache: layer  23: dev = CUDA6
llama_kv_cache: layer  24: dev = CUDA6
llama_kv_cache: layer  25: dev = CUDA6
llama_kv_cache: layer  26: dev = CUDA7
llama_kv_cache: layer  27: dev = CUDA7
llama_kv_cache:      CUDA0 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA1 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA2 KV buffer size =    96.00 MiB
llama_kv_cache:      CUDA3 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA4 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA5 KV buffer size =    96.00 MiB
llama_kv_cache:      CUDA6 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA7 KV buffer size =    64.00 MiB
llama_kv_cache: size =  896.00 MiB (  4096 cells,  28 layers,  2/2 seqs), K (f16):  448.00 MiB, V (f16):  448.00 MiB
llama_context: enumerating backends
llama_context: backend_ptrs.size() = 9
llama_context: max_nodes = 2488
llama_context: pipeline parallelism enabled (n_copies=4)
llama_context: reserving full memory module
llama_context: worst-case: n_tokens = 512, n_seqs = 2, n_outputs = 2
graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  2, n_outputs =  512
graph_reserve: reserving a graph for ubatch with n_tokens =    2, n_seqs =  2, n_outputs =    2
graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  2, n_outputs =  512
llama_context:      CUDA0 compute buffer size =   100.04 MiB
llama_context:      CUDA1 compute buffer size =    96.04 MiB
llama_context:      CUDA2 compute buffer size =    96.04 MiB
llama_context:      CUDA3 compute buffer size =    96.04 MiB
llama_context:      CUDA4 compute buffer size =    96.04 MiB
llama_context:      CUDA5 compute buffer size =    96.04 MiB
llama_context:      CUDA6 compute buffer size =    96.04 MiB
llama_context:      CUDA7 compute buffer size =   348.80 MiB
llama_context:  CUDA_Host compute buffer size =    36.05 MiB
llama_context: graph nodes  = 1043
llama_context: graph splits = 9
llama_context: constructing llama_context
llama_context: n_seq_max     = 2
llama_context: n_ctx         = 8192
llama_context: n_ctx_seq     = 4096
llama_context: n_batch       = 2048
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = enabled
llama_context: kv_unified    = false
llama_context: freq_base     = 1000000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_seq (4096) < n_ctx_train (32768) -- the full capacity of the model will not be utilized
set_abort_callback: call
llama_context:  CUDA_Host  output buffer size =     1.16 MiB
llama_kv_cache: layer   0: dev = CUDA0
llama_kv_cache: layer   1: dev = CUDA0
llama_kv_cache: layer   2: dev = CUDA0
llama_kv_cache: layer   3: dev = CUDA0
llama_kv_cache: layer   4: dev = CUDA1
llama_kv_cache: layer   5: dev = CUDA1
llama_kv_cache: layer   6: dev = CUDA1
llama_kv_cache: layer   7: dev = CUDA1
llama_kv_cache: layer   8: dev = CUDA2
llama_kv_cache: layer   9: dev = CUDA2
llama_kv_cache: layer  10: dev = CUDA2
llama_kv_cache: layer  11: dev = CUDA3
llama_kv_cache: layer  12: dev = CUDA3
llama_kv_cache: layer  13: dev = CUDA3
llama_kv_cache: layer  14: dev = CUDA3
llama_kv_cache: layer  15: dev = CUDA4
llama_kv_cache: layer  16: dev = CUDA4
llama_kv_cache: layer  17: dev = CUDA4
llama_kv_cache: layer  18: dev = CUDA4
llama_kv_cache: layer  19: dev = CUDA5
llama_kv_cache: layer  20: dev = CUDA5
llama_kv_cache: layer  21: dev = CUDA5
llama_kv_cache: layer  22: dev = CUDA6
llama_kv_cache: layer  23: dev = CUDA6
llama_kv_cache: layer  24: dev = CUDA6
llama_kv_cache: layer  25: dev = CUDA6
llama_kv_cache: layer  26: dev = CUDA7
llama_kv_cache: layer  27: dev = CUDA7
llama_kv_cache:      CUDA0 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA1 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA2 KV buffer size =    96.00 MiB
llama_kv_cache:      CUDA3 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA4 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA5 KV buffer size =    96.00 MiB
llama_kv_cache:      CUDA6 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA7 KV buffer size =    64.00 MiB
llama_kv_cache: size =  896.00 MiB (  4096 cells,  28 layers,  2/2 seqs), K (f16):  448.00 MiB, V (f16):  448.00 MiB
llama_context: enumerating backends
llama_context: backend_ptrs.size() = 9
llama_context: max_nodes = 2488
llama_context: pipeline parallelism enabled (n_copies=4)
llama_context: reserving full memory module
llama_context: worst-case: n_tokens = 512, n_seqs = 2, n_outputs = 2
graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  2, n_outputs =  512
graph_reserve: reserving a graph for ubatch with n_tokens =    2, n_seqs =  2, n_outputs =    2
graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  2, n_outputs =  512
llama_context:      CUDA0 compute buffer size =   100.04 MiB
llama_context:      CUDA1 compute buffer size =    96.04 MiB
llama_context:      CUDA2 compute buffer size =    96.04 MiB
llama_context:      CUDA3 compute buffer size =    96.04 MiB
llama_context:      CUDA4 compute buffer size =    96.04 MiB
llama_context:      CUDA5 compute buffer size =    96.04 MiB
llama_context:      CUDA6 compute buffer size =    96.04 MiB
llama_context:      CUDA7 compute buffer size =   348.80 MiB
llama_context:  CUDA_Host compute buffer size =    36.05 MiB
llama_context: graph nodes  = 1043
llama_context: graph splits = 9
llama_context: constructing llama_context
llama_context: n_seq_max     = 2
llama_context: n_ctx         = 8192
llama_context: n_ctx_seq     = 4096
llama_context: n_batch       = 2048
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = enabled
llama_context: kv_unified    = false
llama_context: freq_base     = 1000000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_seq (4096) < n_ctx_train (32768) -- the full capacity of the model will not be utilized
set_abort_callback: call
llama_context:  CUDA_Host  output buffer size =     1.16 MiB
llama_kv_cache: layer   0: dev = CUDA0
llama_kv_cache: layer   1: dev = CUDA0
llama_kv_cache: layer   2: dev = CUDA0
llama_kv_cache: layer   3: dev = CUDA0
llama_kv_cache: layer   4: dev = CUDA1
llama_kv_cache: layer   5: dev = CUDA1
llama_kv_cache: layer   6: dev = CUDA1
llama_kv_cache: layer   7: dev = CUDA1
llama_kv_cache: layer   8: dev = CUDA2
llama_kv_cache: layer   9: dev = CUDA2
llama_kv_cache: layer  10: dev = CUDA2
llama_kv_cache: layer  11: dev = CUDA3
llama_kv_cache: layer  12: dev = CUDA3
llama_kv_cache: layer  13: dev = CUDA3
llama_kv_cache: layer  14: dev = CUDA3
llama_kv_cache: layer  15: dev = CUDA4
llama_kv_cache: layer  16: dev = CUDA4
llama_kv_cache: layer  17: dev = CUDA4
llama_kv_cache: layer  18: dev = CUDA4
llama_kv_cache: layer  19: dev = CUDA5
llama_kv_cache: layer  20: dev = CUDA5
llama_kv_cache: layer  21: dev = CUDA5
llama_kv_cache: layer  22: dev = CUDA6
llama_kv_cache: layer  23: dev = CUDA6
llama_kv_cache: layer  24: dev = CUDA6
llama_kv_cache: layer  25: dev = CUDA6
llama_kv_cache: layer  26: dev = CUDA7
llama_kv_cache: layer  27: dev = CUDA7
llama_kv_cache:      CUDA0 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA1 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA2 KV buffer size =    96.00 MiB
llama_kv_cache:      CUDA3 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA4 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA5 KV buffer size =    96.00 MiB
llama_kv_cache:      CUDA6 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA7 KV buffer size =    64.00 MiB
llama_kv_cache: size =  896.00 MiB (  4096 cells,  28 layers,  2/2 seqs), K (f16):  448.00 MiB, V (f16):  448.00 MiB
llama_context: enumerating backends
llama_context: backend_ptrs.size() = 9
llama_context: max_nodes = 2488
llama_context: pipeline parallelism enabled (n_copies=4)
llama_context: reserving full memory module
llama_context: worst-case: n_tokens = 512, n_seqs = 2, n_outputs = 2
graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  2, n_outputs =  512
graph_reserve: reserving a graph for ubatch with n_tokens =    2, n_seqs =  2, n_outputs =    2
graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  2, n_outputs =  512
llama_context:      CUDA0 compute buffer size =   100.04 MiB
llama_context:      CUDA1 compute buffer size =    96.04 MiB
llama_context:      CUDA2 compute buffer size =    96.04 MiB
llama_context:      CUDA3 compute buffer size =    96.04 MiB
llama_context:      CUDA4 compute buffer size =    96.04 MiB
llama_context:      CUDA5 compute buffer size =    96.04 MiB
llama_context:      CUDA6 compute buffer size =    96.04 MiB
llama_context:      CUDA7 compute buffer size =   348.80 MiB
llama_context:  CUDA_Host compute buffer size =    36.05 MiB
llama_context: graph nodes  = 1043
llama_context: graph splits = 9
llama_context: constructing llama_context
llama_context: n_seq_max     = 2
llama_context: n_ctx         = 8192
llama_context: n_ctx_seq     = 4096
llama_context: n_batch       = 2048
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = enabled
llama_context: kv_unified    = false
llama_context: freq_base     = 1000000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_seq (4096) < n_ctx_train (32768) -- the full capacity of the model will not be utilized
set_abort_callback: call
llama_context:  CUDA_Host  output buffer size =     1.16 MiB
llama_kv_cache: layer   0: dev = CUDA0
llama_kv_cache: layer   1: dev = CUDA0
llama_kv_cache: layer   2: dev = CUDA0
llama_kv_cache: layer   3: dev = CUDA0
llama_kv_cache: layer   4: dev = CUDA1
llama_kv_cache: layer   5: dev = CUDA1
llama_kv_cache: layer   6: dev = CUDA1
llama_kv_cache: layer   7: dev = CUDA1
llama_kv_cache: layer   8: dev = CUDA2
llama_kv_cache: layer   9: dev = CUDA2
llama_kv_cache: layer  10: dev = CUDA2
llama_kv_cache: layer  11: dev = CUDA3
llama_kv_cache: layer  12: dev = CUDA3
llama_kv_cache: layer  13: dev = CUDA3
llama_kv_cache: layer  14: dev = CUDA3
llama_kv_cache: layer  15: dev = CUDA4
llama_kv_cache: layer  16: dev = CUDA4
llama_kv_cache: layer  17: dev = CUDA4
llama_kv_cache: layer  18: dev = CUDA4
llama_kv_cache: layer  19: dev = CUDA5
llama_kv_cache: layer  20: dev = CUDA5
llama_kv_cache: layer  21: dev = CUDA5
llama_kv_cache: layer  22: dev = CUDA6
llama_kv_cache: layer  23: dev = CUDA6
llama_kv_cache: layer  24: dev = CUDA6
llama_kv_cache: layer  25: dev = CUDA6
llama_kv_cache: layer  26: dev = CUDA7
llama_kv_cache: layer  27: dev = CUDA7
llama_kv_cache:      CUDA0 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA1 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA2 KV buffer size =    96.00 MiB
llama_kv_cache:      CUDA3 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA4 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA5 KV buffer size =    96.00 MiB
llama_kv_cache:      CUDA6 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA7 KV buffer size =    64.00 MiB
llama_kv_cache: size =  896.00 MiB (  4096 cells,  28 layers,  2/2 seqs), K (f16):  448.00 MiB, V (f16):  448.00 MiB
llama_context: enumerating backends
llama_context: backend_ptrs.size() = 9
llama_context: max_nodes = 2488
llama_context: pipeline parallelism enabled (n_copies=4)
llama_context: reserving full memory module
llama_context: worst-case: n_tokens = 512, n_seqs = 2, n_outputs = 2
graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  2, n_outputs =  512
graph_reserve: reserving a graph for ubatch with n_tokens =    2, n_seqs =  2, n_outputs =    2
graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  2, n_outputs =  512
llama_context:      CUDA0 compute buffer size =   100.04 MiB
llama_context:      CUDA1 compute buffer size =    96.04 MiB
llama_context:      CUDA2 compute buffer size =    96.04 MiB
llama_context:      CUDA3 compute buffer size =    96.04 MiB
llama_context:      CUDA4 compute buffer size =    96.04 MiB
llama_context:      CUDA5 compute buffer size =    96.04 MiB
llama_context:      CUDA6 compute buffer size =    96.04 MiB
llama_context:      CUDA7 compute buffer size =   348.80 MiB
llama_context:  CUDA_Host compute buffer size =    36.05 MiB
llama_context: graph nodes  = 1043
llama_context: graph splits = 9
llama_context: constructing llama_context
llama_context: n_seq_max     = 2
llama_context: n_ctx         = 8192
llama_context: n_ctx_seq     = 4096
llama_context: n_batch       = 2048
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = enabled
llama_context: kv_unified    = false
llama_context: freq_base     = 1000000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_seq (4096) < n_ctx_train (32768) -- the full capacity of the model will not be utilized
set_abort_callback: call
llama_context:  CUDA_Host  output buffer size =     1.16 MiB
llama_kv_cache: layer   0: dev = CUDA0
llama_kv_cache: layer   1: dev = CUDA0
llama_kv_cache: layer   2: dev = CUDA0
llama_kv_cache: layer   3: dev = CUDA0
llama_kv_cache: layer   4: dev = CUDA1
llama_kv_cache: layer   5: dev = CUDA1
llama_kv_cache: layer   6: dev = CUDA1
llama_kv_cache: layer   7: dev = CUDA1
llama_kv_cache: layer   8: dev = CUDA2
llama_kv_cache: layer   9: dev = CUDA2
llama_kv_cache: layer  10: dev = CUDA2
llama_kv_cache: layer  11: dev = CUDA3
llama_kv_cache: layer  12: dev = CUDA3
llama_kv_cache: layer  13: dev = CUDA3
llama_kv_cache: layer  14: dev = CUDA3
llama_kv_cache: layer  15: dev = CUDA4
llama_kv_cache: layer  16: dev = CUDA4
llama_kv_cache: layer  17: dev = CUDA4
llama_kv_cache: layer  18: dev = CUDA4
llama_kv_cache: layer  19: dev = CUDA5
llama_kv_cache: layer  20: dev = CUDA5
llama_kv_cache: layer  21: dev = CUDA5
llama_kv_cache: layer  22: dev = CUDA6
llama_kv_cache: layer  23: dev = CUDA6
llama_kv_cache: layer  24: dev = CUDA6
llama_kv_cache: layer  25: dev = CUDA6
llama_kv_cache: layer  26: dev = CUDA7
llama_kv_cache: layer  27: dev = CUDA7
llama_kv_cache:      CUDA0 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA1 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA2 KV buffer size =    96.00 MiB
llama_kv_cache:      CUDA3 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA4 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA5 KV buffer size =    96.00 MiB
llama_kv_cache:      CUDA6 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA7 KV buffer size =    64.00 MiB
llama_kv_cache: size =  896.00 MiB (  4096 cells,  28 layers,  2/2 seqs), K (f16):  448.00 MiB, V (f16):  448.00 MiB
llama_context: enumerating backends
llama_context: backend_ptrs.size() = 9
llama_context: max_nodes = 2488
llama_context: pipeline parallelism enabled (n_copies=4)
llama_context: reserving full memory module
llama_context: worst-case: n_tokens = 512, n_seqs = 2, n_outputs = 2
graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  2, n_outputs =  512
graph_reserve: reserving a graph for ubatch with n_tokens =    2, n_seqs =  2, n_outputs =    2
graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  2, n_outputs =  512
llama_context:      CUDA0 compute buffer size =   100.04 MiB
llama_context:      CUDA1 compute buffer size =    96.04 MiB
llama_context:      CUDA2 compute buffer size =    96.04 MiB
llama_context:      CUDA3 compute buffer size =    96.04 MiB
llama_context:      CUDA4 compute buffer size =    96.04 MiB
llama_context:      CUDA5 compute buffer size =    96.04 MiB
llama_context:      CUDA6 compute buffer size =    96.04 MiB
llama_context:      CUDA7 compute buffer size =   348.80 MiB
llama_context:  CUDA_Host compute buffer size =    36.05 MiB
llama_context: graph nodes  = 1043
llama_context: graph splits = 9
llama_context: constructing llama_context
llama_context: n_seq_max     = 2
llama_context: n_ctx         = 8192
llama_context: n_ctx_seq     = 4096
llama_context: n_batch       = 2048
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = enabled
llama_context: kv_unified    = false
llama_context: freq_base     = 1000000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_seq (4096) < n_ctx_train (32768) -- the full capacity of the model will not be utilized
set_abort_callback: call
llama_context:  CUDA_Host  output buffer size =     1.16 MiB
llama_kv_cache: layer   0: dev = CUDA0
llama_kv_cache: layer   1: dev = CUDA0
llama_kv_cache: layer   2: dev = CUDA0
llama_kv_cache: layer   3: dev = CUDA0
llama_kv_cache: layer   4: dev = CUDA1
llama_kv_cache: layer   5: dev = CUDA1
llama_kv_cache: layer   6: dev = CUDA1
llama_kv_cache: layer   7: dev = CUDA1
llama_kv_cache: layer   8: dev = CUDA2
llama_kv_cache: layer   9: dev = CUDA2
llama_kv_cache: layer  10: dev = CUDA2
llama_kv_cache: layer  11: dev = CUDA3
llama_kv_cache: layer  12: dev = CUDA3
llama_kv_cache: layer  13: dev = CUDA3
llama_kv_cache: layer  14: dev = CUDA3
llama_kv_cache: layer  15: dev = CUDA4
llama_kv_cache: layer  16: dev = CUDA4
llama_kv_cache: layer  17: dev = CUDA4
llama_kv_cache: layer  18: dev = CUDA4
llama_kv_cache: layer  19: dev = CUDA5
llama_kv_cache: layer  20: dev = CUDA5
llama_kv_cache: layer  21: dev = CUDA5
llama_kv_cache: layer  22: dev = CUDA6
llama_kv_cache: layer  23: dev = CUDA6
llama_kv_cache: layer  24: dev = CUDA6
llama_kv_cache: layer  25: dev = CUDA6
llama_kv_cache: layer  26: dev = CUDA7
llama_kv_cache: layer  27: dev = CUDA7
llama_kv_cache:      CUDA0 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA1 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA2 KV buffer size =    96.00 MiB
llama_kv_cache:      CUDA3 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA4 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA5 KV buffer size =    96.00 MiB
llama_kv_cache:      CUDA6 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA7 KV buffer size =    64.00 MiB
llama_kv_cache: size =  896.00 MiB (  4096 cells,  28 layers,  2/2 seqs), K (f16):  448.00 MiB, V (f16):  448.00 MiB
llama_context: enumerating backends
llama_context: backend_ptrs.size() = 9
llama_context: max_nodes = 2488
llama_context: pipeline parallelism enabled (n_copies=4)
llama_context: reserving full memory module
llama_context: worst-case: n_tokens = 512, n_seqs = 2, n_outputs = 2
graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  2, n_outputs =  512
graph_reserve: reserving a graph for ubatch with n_tokens =    2, n_seqs =  2, n_outputs =    2
graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  2, n_outputs =  512
llama_context:      CUDA0 compute buffer size =   100.04 MiB
llama_context:      CUDA1 compute buffer size =    96.04 MiB
llama_context:      CUDA2 compute buffer size =    96.04 MiB
llama_context:      CUDA3 compute buffer size =    96.04 MiB
llama_context:      CUDA4 compute buffer size =    96.04 MiB
llama_context:      CUDA5 compute buffer size =    96.04 MiB
llama_context:      CUDA6 compute buffer size =    96.04 MiB
llama_context:      CUDA7 compute buffer size =   348.80 MiB
llama_context:  CUDA_Host compute buffer size =    36.05 MiB
llama_context: graph nodes  = 1043
llama_context: graph splits = 9
llama_context: constructing llama_context
llama_context: n_seq_max     = 2
llama_context: n_ctx         = 8192
llama_context: n_ctx_seq     = 4096
llama_context: n_batch       = 2048
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = enabled
llama_context: kv_unified    = false
llama_context: freq_base     = 1000000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_seq (4096) < n_ctx_train (32768) -- the full capacity of the model will not be utilized
set_abort_callback: call
llama_context:  CUDA_Host  output buffer size =     1.16 MiB
llama_kv_cache: layer   0: dev = CUDA0
llama_kv_cache: layer   1: dev = CUDA0
llama_kv_cache: layer   2: dev = CUDA0
llama_kv_cache: layer   3: dev = CUDA0
llama_kv_cache: layer   4: dev = CUDA1
llama_kv_cache: layer   5: dev = CUDA1
llama_kv_cache: layer   6: dev = CUDA1
llama_kv_cache: layer   7: dev = CUDA1
llama_kv_cache: layer   8: dev = CUDA2
llama_kv_cache: layer   9: dev = CUDA2
llama_kv_cache: layer  10: dev = CUDA2
llama_kv_cache: layer  11: dev = CUDA3
llama_kv_cache: layer  12: dev = CUDA3
llama_kv_cache: layer  13: dev = CUDA3
llama_kv_cache: layer  14: dev = CUDA3
llama_kv_cache: layer  15: dev = CUDA4
llama_kv_cache: layer  16: dev = CUDA4
llama_kv_cache: layer  17: dev = CUDA4
llama_kv_cache: layer  18: dev = CUDA4
llama_kv_cache: layer  19: dev = CUDA5
llama_kv_cache: layer  20: dev = CUDA5
llama_kv_cache: layer  21: dev = CUDA5
llama_kv_cache: layer  22: dev = CUDA6
llama_kv_cache: layer  23: dev = CUDA6
llama_kv_cache: layer  24: dev = CUDA6
llama_kv_cache: layer  25: dev = CUDA6
llama_kv_cache: layer  26: dev = CUDA7
llama_kv_cache: layer  27: dev = CUDA7
llama_kv_cache:      CUDA0 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA1 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA2 KV buffer size =    96.00 MiB
llama_kv_cache:      CUDA3 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA4 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA5 KV buffer size =    96.00 MiB
llama_kv_cache:      CUDA6 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA7 KV buffer size =    64.00 MiB
llama_kv_cache: size =  896.00 MiB (  4096 cells,  28 layers,  2/2 seqs), K (f16):  448.00 MiB, V (f16):  448.00 MiB
llama_context: enumerating backends
llama_context: backend_ptrs.size() = 9
llama_context: max_nodes = 2488
llama_context: pipeline parallelism enabled (n_copies=4)
llama_context: reserving full memory module
llama_context: worst-case: n_tokens = 512, n_seqs = 2, n_outputs = 2
graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  2, n_outputs =  512
graph_reserve: reserving a graph for ubatch with n_tokens =    2, n_seqs =  2, n_outputs =    2
graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  2, n_outputs =  512
llama_context:      CUDA0 compute buffer size =   100.04 MiB
llama_context:      CUDA1 compute buffer size =    96.04 MiB
llama_context:      CUDA2 compute buffer size =    96.04 MiB
llama_context:      CUDA3 compute buffer size =    96.04 MiB
llama_context:      CUDA4 compute buffer size =    96.04 MiB
llama_context:      CUDA5 compute buffer size =    96.04 MiB
llama_context:      CUDA6 compute buffer size =    96.04 MiB
llama_context:      CUDA7 compute buffer size =   348.80 MiB
llama_context:  CUDA_Host compute buffer size =    36.05 MiB
llama_context: graph nodes  = 1043
llama_context: graph splits = 9
--------------------------------------------------------------------------------
generation_phase                         643.09       643.09       1       
llama_decode                             275.96       5.63         49      
token_sampling                           260.93       5.33         49      
denoising_step_0                         199.16       11.72        17      
denoising_step_1                         169.66       9.98         17      
denoising_step_2                         169.15       10.57        16      
telemetry_gpu_invoke                     159.44       3.25         49      
finalize_block                           104.59       6.15         17      
telemetry_gpu_stage_cpu_post             77.50        1.58         49      
telemetry_gpu_logit_pack                 70.31        1.43         49      

================================================================================
Running test 14/16: Iterative Refinement (block=8, steps=3, rounds=4) + GPU
================================================================================

Run 1/3:

Run 2/3:

Run 3/3:

Average across 3 runs:

Wall Time: 723.71 ms
Tokens Generated: 149
Throughput: 205.88 tokens/sec

Top 10 Time-Consuming Sections:
Section                                  Total (ms)   Avg (ms)     Calls   
--------------------------------------------------------------------------------
generation_phase                         642.82       642.82       1       
llama_decode                             277.07       5.65         49      
token_sampling                           259.51       5.30         49      
denoising_step_0                         198.40       11.67        17      
denoising_step_1                         170.12       10.01        17      
denoising_step_2                         169.11       10.57        16      
telemetry_gpu_invoke                     158.11       3.23         49      
finalize_block                           104.64       6.16         17      
telemetry_gpu_stage_cpu_post             76.52        1.56         49      
telemetry_gpu_logit_pack                 70.29        1.43         49      

================================================================================
Running test 15/16: Iterative Refinement (block=4, steps=3, rounds=3) + GPU
================================================================================

Run 1/3:

Run 2/3:

Run 3/3:

Average across 3 runs:

Wall Time: 1029.55 ms
Tokens Generated: 149
Throughput: 144.72 tokens/sec

Top 10 Time-Consuming Sections:
Section                                  Total (ms)   Avg (ms)     Calls   
--------------------------------------------------------------------------------
generation_phase                         952.56       952.56       1       
token_sampling                           535.91       5.47         98      
telemetry_gpu_logit_pack                 347.98       3.55         98      
denoising_step_0                         326.10       9.88         33      
llama_decode                             298.94       3.05         98      
denoising_step_1                         259.11       7.85         33      
denoising_step_2                         251.02       7.61         33      
telemetry_gpu_invoke                     166.04       1.69         98      
finalize_block                           115.85       3.51         33      
telemetry_gpu_success                    98.00        1.00         98      

================================================================================
Running test 16/16: Iterative Refinement (block=8, steps=4, rounds=3) + GPU
================================================================================

Run 1/3:

Run 2/3:

Run 3/3:

Average across 3 runs:

Wall Time: 906.30 ms
Tokens Generated: 149
Throughput: 164.41 tokens/sec

Top 10 Time-Consuming Sections:
Section                                  Total (ms)   Avg (ms)     Calls   
--------------------------------------------------------------------------------
generation_phase                         824.78       824.78       1       
llama_decode                             370.80       5.62         66      
token_sampling                           346.05       5.24         66      
telemetry_gpu_invoke                     213.02       3.23         66      
denoising_step_0                         199.53       11.74        17      
denoising_step_1                         180.33       10.61        17      
denoising_step_3                         169.58       10.60        16      
denoising_step_2                         168.91       9.94         17      
finalize_block                           105.67       6.22         17      
telemetry_gpu_stage_cpu_post             104.09       1.58         66      

================================================================================
Detailed Analysis for: Baseline (block=8, steps=8)
================================================================================

================================================================================
BOTTLENECK ANALYSIS
================================================================================

Total Time: 2718.86 ms

Critical Bottlenecks (>10% of total time or >50ms avg):
Section                                  Time (ms)    Percentage   Avg (ms)    
--------------------------------------------------------------------------------
generation_phase                         2651.82      97.5        % 2651.82     
token_sampling                           1853.60      68.2        % 14.19       
telemetry_cpu_sampling                   1853.47      68.2        % 14.18       
telemetry_cpu_loop                       1853.34      68.2        % 14.18       
llama_decode                             694.87       25.6        % 5.32        
denoising_step_0                         347.19       12.8        % 20.42       
denoising_step_1                         329.23       12.1        % 19.37       
denoising_step_2                         328.79       12.1        % 19.34       
denoising_step_4                         312.51       11.5        % 19.53       
denoising_step_5                         310.20       11.4        % 19.39       
denoising_step_6                         309.79       11.4        % 19.36       
denoising_step_3                         309.69       11.4        % 18.22       
denoising_step_7                         303.30       11.2        % 18.96       
block_16                                 162.71       6.0         % 162.71      
block_15                                 162.19       6.0         % 162.19      
block_6                                  162.17       6.0         % 162.17      
block_11                                 162.15       6.0         % 162.15      
block_9                                  162.11       6.0         % 162.11      
block_17                                 162.11       6.0         % 162.11      
block_18                                 161.91       6.0         % 161.91      
block_5                                  161.77       6.0         % 161.77      
block_14                                 161.66       5.9         % 161.66      
block_8                                  161.36       5.9         % 161.36      
block_4                                  161.35       5.9         % 161.35      
block_7                                  161.34       5.9         % 161.34      
block_10                                 161.25       5.9         % 161.25      
block_12                                 161.09       5.9         % 161.09      
block_13                                 161.05       5.9         % 161.05      
block_3                                  155.09       5.7         % 155.09      
block_2                                  70.44        2.6         % 70.44       

Visualization saved to profile_0.png

================================================================================
Detailed Analysis for: Baseline (block=4, steps=4)
================================================================================

================================================================================
BOTTLENECK ANALYSIS
================================================================================

Total Time: 1872.81 ms

Critical Bottlenecks (>10% of total time or >50ms avg):
Section                                  Time (ms)    Percentage   Avg (ms)    
--------------------------------------------------------------------------------
generation_phase                         1802.03      96.2        % 1802.03     
token_sampling                           1285.22      68.6        % 9.81        
telemetry_cpu_sampling                   1285.14      68.6        % 9.81        
telemetry_cpu_loop                       1285.06      68.6        % 9.81        
denoising_step_0                         482.29       25.8        % 14.61       
denoising_step_1                         405.88       21.7        % 12.30       
denoising_step_2                         405.42       21.6        % 12.29       
llama_decode                             399.89       21.4        % 3.05        
denoising_step_3                         393.18       21.0        % 11.91       
block_5                                  55.56        3.0         % 55.56       
block_8                                  55.44        3.0         % 55.44       
block_29                                 54.73        2.9         % 54.73       
block_28                                 54.70        2.9         % 54.70       
block_34                                 54.67        2.9         % 54.67       
block_21                                 54.66        2.9         % 54.66       
block_27                                 54.66        2.9         % 54.66       
block_13                                 54.64        2.9         % 54.64       
block_22                                 54.63        2.9         % 54.63       
block_19                                 54.63        2.9         % 54.63       
block_18                                 54.62        2.9         % 54.62       
block_32                                 54.62        2.9         % 54.62       
block_17                                 54.59        2.9         % 54.59       
block_12                                 54.59        2.9         % 54.59       
block_26                                 54.58        2.9         % 54.58       
block_33                                 54.57        2.9         % 54.57       
block_11                                 54.55        2.9         % 54.55       
block_23                                 54.54        2.9         % 54.54       
block_36                                 54.54        2.9         % 54.54       
block_15                                 54.53        2.9         % 54.53       
block_31                                 54.52        2.9         % 54.52       
block_16                                 54.52        2.9         % 54.52       
block_25                                 54.52        2.9         % 54.52       
block_20                                 54.52        2.9         % 54.52       
block_24                                 54.51        2.9         % 54.51       
block_10                                 54.51        2.9         % 54.51       
block_35                                 54.50        2.9         % 54.50       
block_14                                 54.49        2.9         % 54.49       
block_30                                 54.49        2.9         % 54.49       
block_6                                  54.39        2.9         % 54.39       
block_7                                  54.36        2.9         % 54.36       
block_9                                  54.32        2.9         % 54.32       
block_37                                 54.26        2.9         % 54.26       

Visualization saved to profile_1.png

================================================================================
Detailed Analysis for: Baseline (block=8, steps=4)
================================================================================

================================================================================
BOTTLENECK ANALYSIS
================================================================================

Total Time: 1468.01 ms

Critical Bottlenecks (>10% of total time or >50ms avg):
Section                                  Time (ms)    Percentage   Avg (ms)    
--------------------------------------------------------------------------------
generation_phase                         1402.01      95.5        % 1402.01     
token_sampling                           942.00       64.2        % 14.27       
telemetry_cpu_sampling                   941.94       64.2        % 14.27       
telemetry_cpu_loop                       941.89       64.2        % 14.27       
llama_decode                             358.56       24.4        % 5.43        
denoising_step_0                         348.58       23.7        % 20.50       
denoising_step_1                         330.82       22.5        % 19.46       
denoising_step_2                         311.43       21.2        % 18.32       
denoising_step_3                         310.80       21.2        % 19.43       
block_8                                  84.83        5.8         % 84.83       
block_7                                  84.80        5.8         % 84.80       
block_9                                  84.77        5.8         % 84.77       
block_5                                  84.76        5.8         % 84.76       
block_10                                 84.75        5.8         % 84.75       
block_6                                  84.63        5.8         % 84.63       
block_12                                 84.60        5.8         % 84.60       
block_11                                 84.60        5.8         % 84.60       
block_18                                 84.37        5.7         % 84.37       
block_14                                 84.28        5.7         % 84.28       
block_4                                  84.28        5.7         % 84.28       
block_13                                 84.21        5.7         % 84.21       
block_16                                 84.11        5.7         % 84.11       
block_17                                 84.07        5.7         % 84.07       
block_3                                  84.07        5.7         % 84.07       
block_15                                 83.89        5.7         % 83.89       
block_2                                  50.93        3.5         % 50.93       

Visualization saved to profile_2.png

================================================================================
Detailed Analysis for: Baseline (block=8, steps=8) + GPU
================================================================================

================================================================================
BOTTLENECK ANALYSIS
================================================================================

Total Time: 1323.85 ms

Critical Bottlenecks (>10% of total time or >50ms avg):
Section                                  Time (ms)    Percentage   Avg (ms)    
--------------------------------------------------------------------------------
generation_phase                         1241.72      93.8        % 1241.72     
llama_decode                             587.11       44.3        % 5.62        
token_sampling                           544.69       41.1        % 5.28        
telemetry_gpu_invoke                     341.26       25.8        % 3.31        
denoising_step_0                         204.05       15.4        % 12.00       
telemetry_gpu_stage_cpu_post             167.49       12.7        % 1.63        
denoising_step_1                         157.20       11.9        % 9.25        
telemetry_gpu_logit_pack                 139.82       10.6        % 1.35        
denoising_step_2                         137.87       10.4        % 8.84        
block_3                                  92.98        7.0         % 92.98       
block_6                                  91.70        6.9         % 91.70       
block_5                                  91.69        6.9         % 91.69       
block_4                                  91.57        6.9         % 91.57       
block_9                                  71.32        5.4         % 71.32       
block_7                                  71.27        5.4         % 71.27       
block_8                                  70.97        5.4         % 70.97       
block_17                                 70.64        5.3         % 70.64       
block_16                                 70.53        5.3         % 70.53       
block_13                                 67.74        5.1         % 67.74       
block_10                                 67.72        5.1         % 67.72       
block_12                                 67.62        5.1         % 67.62       
block_14                                 66.96        5.1         % 66.96       
block_11                                 66.91        5.1         % 66.91       
block_15                                 66.88        5.1         % 66.88       
block_18                                 66.85        5.1         % 66.85       

Visualization saved to profile_3.png

================================================================================
Detailed Analysis for: Baseline (block=4, steps=4) + GPU
================================================================================

================================================================================
BOTTLENECK ANALYSIS
================================================================================

Total Time: 1278.84 ms

Critical Bottlenecks (>10% of total time or >50ms avg):
Section                                  Time (ms)    Percentage   Avg (ms)    
--------------------------------------------------------------------------------
generation_phase                         1199.49      93.8        % 1199.49     
token_sampling                           682.46       53.4        % 5.22        
telemetry_gpu_logit_pack                 444.31       34.7        % 3.40        
llama_decode                             397.01       31.0        % 3.04        
denoising_step_0                         322.08       25.2        % 9.76        
denoising_step_1                         257.11       20.1        % 7.79        
denoising_step_2                         256.14       20.0        % 7.76        
denoising_step_3                         246.07       19.2        % 7.46        
telemetry_gpu_invoke                     209.69       16.4        % 1.60        
telemetry_gpu_success                    130.67       10.2        % 1.00        

Visualization saved to profile_4.png

================================================================================
Detailed Analysis for: Baseline (block=8, steps=4) + GPU
================================================================================

================================================================================
BOTTLENECK ANALYSIS
================================================================================

Total Time: 827.30 ms

Critical Bottlenecks (>10% of total time or >50ms avg):
Section                                  Time (ms)    Percentage   Avg (ms)    
--------------------------------------------------------------------------------
generation_phase                         745.47       90.1        % 745.47      
llama_decode                             329.52       39.8        % 5.65        
token_sampling                           308.53       37.3        % 5.30        
denoising_step_0                         200.59       24.2        % 11.80       
telemetry_gpu_invoke                     192.07       23.2        % 3.30        
denoising_step_1                         159.96       19.3        % 9.41        
denoising_step_2                         141.33       17.1        % 9.22        
denoising_step_3                         137.52       16.6        % 10.13       
finalize_block                           105.43       12.7        % 6.20        
telemetry_gpu_stage_cpu_post             95.45        11.5        % 1.64        

Visualization saved to profile_5.png

================================================================================
Detailed Analysis for: Iterative Refinement (block=4, steps=2, rounds=4)
================================================================================

================================================================================
BOTTLENECK ANALYSIS
================================================================================

Total Time: 1056.42 ms

Critical Bottlenecks (>10% of total time or >50ms avg):
Section                                  Time (ms)    Percentage   Avg (ms)    
--------------------------------------------------------------------------------
generation_phase                         990.04       93.7        % 990.04      
token_sampling                           666.51       63.1        % 10.10       
telemetry_cpu_sampling                   666.47       63.1        % 10.10       
telemetry_cpu_loop                       666.42       63.1        % 10.10       
denoising_step_0                         471.00       44.6        % 14.27       
denoising_step_1                         405.09       38.3        % 12.28       
llama_decode                             208.75       19.8        % 3.16        
finalize_block                           113.72       10.8        % 3.45        

Visualization saved to profile_6.png

================================================================================
Detailed Analysis for: Iterative Refinement (block=8, steps=3, rounds=3)
================================================================================

================================================================================
BOTTLENECK ANALYSIS
================================================================================

Total Time: 1135.77 ms

Critical Bottlenecks (>10% of total time or >50ms avg):
Section                                  Time (ms)    Percentage   Avg (ms)    
--------------------------------------------------------------------------------
generation_phase                         1069.18      94.1        % 1069.18     
token_sampling                           698.00       61.5        % 14.24       
telemetry_cpu_sampling                   697.96       61.5        % 14.24       
telemetry_cpu_loop                       697.91       61.4        % 14.24       
denoising_step_0                         347.77       30.6        % 20.46       
denoising_step_1                         310.63       27.3        % 18.27       
denoising_step_2                         310.43       27.3        % 19.40       
llama_decode                             270.04       23.8        % 5.51        
block_13                                 65.11        5.7         % 65.11       
block_9                                  65.08        5.7         % 65.08       
block_7                                  65.06        5.7         % 65.06       
block_15                                 65.05        5.7         % 65.05       
block_14                                 65.00        5.7         % 65.00       
block_8                                  64.94        5.7         % 64.94       
block_16                                 64.80        5.7         % 64.80       
block_10                                 64.79        5.7         % 64.79       
block_12                                 64.75        5.7         % 64.75       
block_11                                 64.72        5.7         % 64.72       
block_18                                 64.71        5.7         % 64.71       
block_17                                 64.71        5.7         % 64.71       
block_3                                  64.68        5.7         % 64.68       
block_4                                  64.68        5.7         % 64.68       
block_5                                  64.67        5.7         % 64.67       
block_6                                  64.63        5.7         % 64.63       

Visualization saved to profile_7.png

================================================================================
Detailed Analysis for: Iterative Refinement (block=8, steps=3, rounds=4)
================================================================================

================================================================================
BOTTLENECK ANALYSIS
================================================================================

Total Time: 1142.76 ms

Critical Bottlenecks (>10% of total time or >50ms avg):
Section                                  Time (ms)    Percentage   Avg (ms)    
--------------------------------------------------------------------------------
generation_phase                         1077.33      94.3        % 1077.33     
token_sampling                           704.14       61.6        % 14.37       
telemetry_cpu_sampling                   704.10       61.6        % 14.37       
telemetry_cpu_loop                       704.05       61.6        % 14.37       
denoising_step_0                         349.46       30.6        % 20.56       
denoising_step_1                         315.06       27.6        % 18.53       
denoising_step_2                         311.94       27.3        % 19.50       
llama_decode                             271.54       23.8        % 5.54        
block_15                                 68.23        6.0         % 68.23       
block_4                                  65.25        5.7         % 65.25       
block_18                                 65.23        5.7         % 65.23       
block_12                                 65.21        5.7         % 65.21       
block_5                                  65.20        5.7         % 65.20       
block_6                                  65.19        5.7         % 65.19       
block_8                                  65.19        5.7         % 65.19       
block_3                                  65.13        5.7         % 65.13       
block_14                                 65.13        5.7         % 65.13       
block_7                                  65.09        5.7         % 65.09       
block_10                                 65.07        5.7         % 65.07       
block_17                                 65.07        5.7         % 65.07       
block_16                                 65.05        5.7         % 65.05       
block_13                                 65.04        5.7         % 65.04       
block_11                                 65.03        5.7         % 65.03       
block_9                                  64.98        5.7         % 64.98       

Visualization saved to profile_8.png

================================================================================
Detailed Analysis for: Iterative Refinement (block=4, steps=3, rounds=3)
================================================================================

================================================================================
BOTTLENECK ANALYSIS
================================================================================

Total Time: 1448.97 ms

Critical Bottlenecks (>10% of total time or >50ms avg):
Section                                  Time (ms)    Percentage   Avg (ms)    
--------------------------------------------------------------------------------
generation_phase                         1382.80      95.4        % 1382.80     
token_sampling                           970.08       66.9        % 9.90        
telemetry_cpu_sampling                   970.02       66.9        % 9.90        
telemetry_cpu_loop                       969.95       66.9        % 9.90        
denoising_step_0                         471.11       32.5        % 14.28       
denoising_step_1                         404.78       27.9        % 12.27       
denoising_step_2                         392.63       27.1        % 11.90       
llama_decode                             297.23       20.5        % 3.03        

Visualization saved to profile_9.png

================================================================================
Detailed Analysis for: Iterative Refinement (block=8, steps=4, rounds=3)
================================================================================

================================================================================
BOTTLENECK ANALYSIS
================================================================================

Total Time: 1453.42 ms

Critical Bottlenecks (>10% of total time or >50ms avg):
Section                                  Time (ms)    Percentage   Avg (ms)    
--------------------------------------------------------------------------------
generation_phase                         1388.85      95.6        % 1388.85     
token_sampling                           934.68       64.3        % 14.16       
telemetry_cpu_sampling                   934.63       64.3        % 14.16       
telemetry_cpu_loop                       934.55       64.3        % 14.16       
llama_decode                             354.05       24.4        % 5.36        
denoising_step_0                         345.17       23.7        % 20.30       
denoising_step_1                         327.74       22.5        % 19.28       
denoising_step_2                         308.62       21.2        % 18.15       
denoising_step_3                         308.18       21.2        % 19.26       
block_17                                 84.08        5.8         % 84.08       
block_18                                 83.97        5.8         % 83.97       
block_14                                 83.78        5.8         % 83.78       
block_16                                 83.71        5.8         % 83.71       
block_15                                 83.67        5.8         % 83.67       
block_10                                 83.64        5.8         % 83.64       
block_13                                 83.56        5.7         % 83.56       
block_6                                  83.54        5.7         % 83.54       
block_3                                  83.54        5.7         % 83.54       
block_11                                 83.53        5.7         % 83.53       
block_12                                 83.51        5.7         % 83.51       
block_8                                  83.51        5.7         % 83.51       
block_9                                  83.50        5.7         % 83.50       
block_5                                  83.48        5.7         % 83.48       
block_4                                  83.48        5.7         % 83.48       
block_7                                  83.47        5.7         % 83.47       
block_2                                  50.84        3.5         % 50.84       

Visualization saved to profile_10.png

================================================================================
Detailed Analysis for: Iterative Refinement (block=4, steps=2, rounds=4) + GPU
================================================================================

================================================================================
BOTTLENECK ANALYSIS
================================================================================

Total Time: 780.40 ms

Critical Bottlenecks (>10% of total time or >50ms avg):
Section                                  Time (ms)    Percentage   Avg (ms)    
--------------------------------------------------------------------------------
generation_phase                         702.79       90.1        % 702.79      
token_sampling                           376.08       48.2        % 5.70        
denoising_step_0                         326.97       41.9        % 9.91        
denoising_step_1                         259.92       33.3        % 7.88        
telemetry_gpu_logit_pack                 246.28       31.6        % 3.73        
llama_decode                             209.93       26.9        % 3.18        
finalize_block                           115.60       14.8        % 3.50        
telemetry_gpu_invoke                     114.71       14.7        % 1.74        

Visualization saved to profile_11.png

================================================================================
Detailed Analysis for: Iterative Refinement (block=8, steps=3, rounds=3) + GPU
================================================================================

================================================================================
BOTTLENECK ANALYSIS
================================================================================

Total Time: 724.58 ms

Critical Bottlenecks (>10% of total time or >50ms avg):
Section                                  Time (ms)    Percentage   Avg (ms)    
--------------------------------------------------------------------------------
generation_phase                         643.09       88.8        % 643.09      
llama_decode                             275.96       38.1        % 5.63        
token_sampling                           260.93       36.0        % 5.33        
denoising_step_0                         199.16       27.5        % 11.72       
denoising_step_1                         169.66       23.4        % 9.98        
denoising_step_2                         169.15       23.3        % 10.57       
telemetry_gpu_invoke                     159.44       22.0        % 3.25        
finalize_block                           104.59       14.4        % 6.15        
telemetry_gpu_stage_cpu_post             77.50        10.7        % 1.58        

Visualization saved to profile_12.png

================================================================================
Detailed Analysis for: Iterative Refinement (block=8, steps=3, rounds=4) + GPU
================================================================================

================================================================================
BOTTLENECK ANALYSIS
================================================================================

Total Time: 723.71 ms

Critical Bottlenecks (>10% of total time or >50ms avg):
Section                                  Time (ms)    Percentage   Avg (ms)    
--------------------------------------------------------------------------------
generation_phase                         642.82       88.8        % 642.82      
llama_decode                             277.07       38.3        % 5.65        
token_sampling                           259.51       35.9        % 5.30        
denoising_step_0                         198.40       27.4        % 11.67       
denoising_step_1                         170.12       23.5        % 10.01       
denoising_step_2                         169.11       23.4        % 10.57       
telemetry_gpu_invoke                     158.11       21.8        % 3.23        
finalize_block                           104.64       14.5        % 6.16        
telemetry_gpu_stage_cpu_post             76.52        10.6        % 1.56        

Visualization saved to profile_13.png

================================================================================
Detailed Analysis for: Iterative Refinement (block=4, steps=3, rounds=3) + GPU
================================================================================

================================================================================
BOTTLENECK ANALYSIS
================================================================================

Total Time: 1029.55 ms

Critical Bottlenecks (>10% of total time or >50ms avg):
Section                                  Time (ms)    Percentage   Avg (ms)    
--------------------------------------------------------------------------------
generation_phase                         952.56       92.5        % 952.56      
token_sampling                           535.91       52.1        % 5.47        
telemetry_gpu_logit_pack                 347.98       33.8        % 3.55        
denoising_step_0                         326.10       31.7        % 9.88        
llama_decode                             298.94       29.0        % 3.05        
denoising_step_1                         259.11       25.2        % 7.85        
denoising_step_2                         251.02       24.4        % 7.61        
telemetry_gpu_invoke                     166.04       16.1        % 1.69        
finalize_block                           115.85       11.3        % 3.51        

Visualization saved to profile_14.png

================================================================================
Detailed Analysis for: Iterative Refinement (block=8, steps=4, rounds=3) + GPU
================================================================================

================================================================================
BOTTLENECK ANALYSIS
================================================================================

Total Time: 906.30 ms

Critical Bottlenecks (>10% of total time or >50ms avg):
Section                                  Time (ms)    Percentage   Avg (ms)    
--------------------------------------------------------------------------------
generation_phase                         824.78       91.0        % 824.78      
llama_decode                             370.80       40.9        % 5.62        
token_sampling                           346.05       38.2        % 5.24        
telemetry_gpu_invoke                     213.02       23.5        % 3.23        
denoising_step_0                         199.53       22.0        % 11.74       
denoising_step_1                         180.33       19.9        % 10.61       
denoising_step_3                         169.58       18.7        % 10.60       
denoising_step_2                         168.91       18.6        % 9.94        
finalize_block                           105.67       11.7        % 6.22        
telemetry_gpu_stage_cpu_post             104.09       11.5        % 1.58        
telemetry_gpu_logit_pack                 90.81        10.0        % 1.38        

Visualization saved to profile_15.png

Results exported to profile_results.json
Results archived to profile_runs/20251129_085908

================================================================================
COMPARATIVE SUMMARY
================================================================================

Config                         Wall Time (ms)  Tokens/sec      Speedup   
----------------------------------------------------------------------
Baseline (block=8, steps=8)    2718.86         54.80           1.00      x
Baseline (block=4, steps=4)    1872.81         79.56           1.45      x
Baseline (block=8, steps=4)    1468.01         101.50          1.85      x
Baseline (block=8, steps=8) + GPU 1323.85         112.55          2.05      x
Baseline (block=4, steps=4) + GPU 1278.84         116.51          2.13      x
Baseline (block=8, steps=4) + GPU 827.30          180.10          3.29      x
Iterative Refinement (block=4, steps=2, rounds=4) 1056.42         141.04          2.57      x
Iterative Refinement (block=8, steps=3, rounds=3) 1135.77         131.19          2.39      x
Iterative Refinement (block=8, steps=3, rounds=4) 1142.76         130.39          2.38      x
Iterative Refinement (block=4, steps=3, rounds=3) 1448.97         102.83          1.88      x
Iterative Refinement (block=8, steps=4, rounds=3) 1453.42         102.52          1.87      x
Iterative Refinement (block=4, steps=2, rounds=4) + GPU 780.40          190.93          3.48      x
Iterative Refinement (block=8, steps=3, rounds=3) + GPU 724.58          205.64          3.75      x
Iterative Refinement (block=8, steps=3, rounds=4) + GPU 723.71          205.88          3.76      x
Iterative Refinement (block=4, steps=3, rounds=3) + GPU 1029.55         144.72          2.64      x
Iterative Refinement (block=8, steps=4, rounds=3) + GPU 906.30          164.41          3.00      x
