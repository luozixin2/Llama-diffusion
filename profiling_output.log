ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    no
ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no
ggml_cuda_init: found 8 CUDA devices:
  Device 0: NVIDIA GeForce RTX 4090, compute capability 8.9, VMM: yes
  Device 1: NVIDIA GeForce RTX 4090, compute capability 8.9, VMM: yes
  Device 2: NVIDIA GeForce RTX 4090, compute capability 8.9, VMM: yes
  Device 3: NVIDIA GeForce RTX 4090, compute capability 8.9, VMM: yes
  Device 4: NVIDIA GeForce RTX 4090, compute capability 8.9, VMM: yes
  Device 5: NVIDIA GeForce RTX 4090, compute capability 8.9, VMM: yes
  Device 6: NVIDIA GeForce RTX 4090, compute capability 8.9, VMM: yes
  Device 7: NVIDIA GeForce RTX 4090, compute capability 8.9, VMM: yes
llama_model_load_from_file_impl: using device CUDA0 (NVIDIA GeForce RTX 4090) (0000:01:00.0) - 47437 MiB free
llama_model_load_from_file_impl: using device CUDA1 (NVIDIA GeForce RTX 4090) (0000:24:00.0) - 48126 MiB free
llama_model_load_from_file_impl: using device CUDA2 (NVIDIA GeForce RTX 4090) (0000:41:00.0) - 48126 MiB free
llama_model_load_from_file_impl: using device CUDA3 (NVIDIA GeForce RTX 4090) (0000:61:00.0) - 48126 MiB free
llama_model_load_from_file_impl: using device CUDA4 (NVIDIA GeForce RTX 4090) (0000:81:00.0) - 48126 MiB free
llama_model_load_from_file_impl: using device CUDA5 (NVIDIA GeForce RTX 4090) (0000:a1:00.0) - 48126 MiB free
llama_model_load_from_file_impl: using device CUDA6 (NVIDIA GeForce RTX 4090) (0000:c1:00.0) - 48126 MiB free
llama_model_load_from_file_impl: using device CUDA7 (NVIDIA GeForce RTX 4090) (0000:e1:00.0) - 48126 MiB free
llama_model_loader: loaded meta data with 29 key-value pairs and 311 tensors from /home/lzx/SDAR/training/model/SDAR-1.7B-Chat/SDAR-1.7B-Chat-F16.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = qwen3
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = SDAR 1.7B Chat
llama_model_loader: - kv   3:                           general.finetune str              = Chat
llama_model_loader: - kv   4:                           general.basename str              = SDAR
llama_model_loader: - kv   5:                         general.size_label str              = 1.7B
llama_model_loader: - kv   6:                            general.license str              = apache-2.0
llama_model_loader: - kv   7:                          qwen3.block_count u32              = 28
llama_model_loader: - kv   8:                       qwen3.context_length u32              = 32768
llama_model_loader: - kv   9:                     qwen3.embedding_length u32              = 2048
llama_model_loader: - kv  10:                  qwen3.feed_forward_length u32              = 6144
llama_model_loader: - kv  11:                 qwen3.attention.head_count u32              = 16
llama_model_loader: - kv  12:              qwen3.attention.head_count_kv u32              = 8
llama_model_loader: - kv  13:                       qwen3.rope.freq_base f32              = 1000000.000000
llama_model_loader: - kv  14:     qwen3.attention.layer_norm_rms_epsilon f32              = 0.000001
llama_model_loader: - kv  15:                 qwen3.attention.key_length u32              = 128
llama_model_loader: - kv  16:               qwen3.attention.value_length u32              = 128
llama_model_loader: - kv  17:                          general.file_type u32              = 1
llama_model_loader: - kv  18:               general.quantization_version u32              = 2
llama_model_loader: - kv  19:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  20:                         tokenizer.ggml.pre str              = qwen2
llama_model_loader: - kv  21:                      tokenizer.ggml.tokens arr[str,151936]  = ["!", "\"", "#", "$", "%", "&", "'", ...
llama_model_loader: - kv  22:                  tokenizer.ggml.token_type arr[i32,151936]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
llama_model_loader: - kv  23:                      tokenizer.ggml.merges arr[str,151387]  = ["Ġ Ġ", "ĠĠ ĠĠ", "i n", "Ġ t",...
llama_model_loader: - kv  24:                tokenizer.ggml.eos_token_id u32              = 151643
llama_model_loader: - kv  25:            tokenizer.ggml.padding_token_id u32              = 151643
llama_model_loader: - kv  26:                tokenizer.ggml.bos_token_id u32              = 151643
llama_model_loader: - kv  27:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  28:                    tokenizer.chat_template str              = {%- if tools %}\n    {{- '<|im_start|>...
llama_model_loader: - type  f32:  113 tensors
llama_model_loader: - type  f16:  198 tensors
print_info: file format = GGUF V3 (latest)
print_info: file type   = F16
print_info: file size   = 3.78 GiB (16.00 BPW) 
init_tokenizer: initializing tokenizer for type 2
load: control token: 151669 '<|MASK|>' is not marked as EOG
load: control token: 151660 '<|fim_middle|>' is not marked as EOG
load: control token: 151659 '<|fim_prefix|>' is not marked as EOG
load: control token: 151653 '<|vision_end|>' is not marked as EOG
load: control token: 151648 '<|box_start|>' is not marked as EOG
load: control token: 151646 '<|object_ref_start|>' is not marked as EOG
load: control token: 151649 '<|box_end|>' is not marked as EOG
load: control token: 151655 '<|image_pad|>' is not marked as EOG
load: control token: 151651 '<|quad_end|>' is not marked as EOG
load: control token: 151647 '<|object_ref_end|>' is not marked as EOG
load: control token: 151652 '<|vision_start|>' is not marked as EOG
load: control token: 151654 '<|vision_pad|>' is not marked as EOG
load: control token: 151656 '<|video_pad|>' is not marked as EOG
load: control token: 151644 '<|im_start|>' is not marked as EOG
load: control token: 151661 '<|fim_suffix|>' is not marked as EOG
load: control token: 151650 '<|quad_start|>' is not marked as EOG
load: printing all EOG tokens:
load:   - 151643 ('<|endoftext|>')
load:   - 151645 ('<|im_end|>')
load:   - 151662 ('<|fim_pad|>')
load:   - 151663 ('<|repo_name|>')
load:   - 151664 ('<|file_sep|>')
load: special tokens cache size = 27
load: token to piece cache size = 0.9311 MB
print_info: arch             = qwen3
print_info: vocab_only       = 0
print_info: n_ctx_train      = 32768
print_info: n_embd           = 2048
print_info: n_embd_inp       = 2048
print_info: n_layer          = 28
print_info: n_head           = 16
print_info: n_head_kv        = 8
print_info: n_rot            = 128
print_info: n_swa            = 0
print_info: is_swa_any       = 0
print_info: n_embd_head_k    = 128
print_info: n_embd_head_v    = 128
print_info: n_gqa            = 2
print_info: n_embd_k_gqa     = 1024
print_info: n_embd_v_gqa     = 1024
print_info: f_norm_eps       = 0.0e+00
print_info: f_norm_rms_eps   = 1.0e-06
print_info: f_clamp_kqv      = 0.0e+00
print_info: f_max_alibi_bias = 0.0e+00
print_info: f_logit_scale    = 0.0e+00
print_info: f_attn_scale     = 0.0e+00
print_info: n_ff             = 6144
print_info: n_expert         = 0
print_info: n_expert_used    = 0
print_info: n_expert_groups  = 0
print_info: n_group_used     = 0
print_info: causal attn      = 1
print_info: pooling type     = -1
print_info: rope type        = 2
print_info: rope scaling     = linear
print_info: freq_base_train  = 1000000.0
print_info: freq_scale_train = 1
print_info: n_ctx_orig_yarn  = 32768
print_info: rope_finetuned   = unknown
print_info: model type       = 1.7B
print_info: model params     = 2.03 B
print_info: general.name     = SDAR 1.7B Chat
print_info: vocab type       = BPE
print_info: n_vocab          = 151936
print_info: n_merges         = 151387
print_info: BOS token        = 151643 '<|endoftext|>'
print_info: EOS token        = 151643 '<|endoftext|>'
print_info: EOT token        = 151645 '<|im_end|>'
print_info: PAD token        = 151643 '<|endoftext|>'
print_info: LF token         = 198 'Ċ'
print_info: FIM PRE token    = 151659 '<|fim_prefix|>'
print_info: FIM SUF token    = 151661 '<|fim_suffix|>'
print_info: FIM MID token    = 151660 '<|fim_middle|>'
print_info: FIM PAD token    = 151662 '<|fim_pad|>'
print_info: FIM REP token    = 151663 '<|repo_name|>'
print_info: FIM SEP token    = 151664 '<|file_sep|>'
print_info: EOG token        = 151643 '<|endoftext|>'
print_info: EOG token        = 151645 '<|im_end|>'
print_info: EOG token        = 151662 '<|fim_pad|>'
print_info: EOG token        = 151663 '<|repo_name|>'
print_info: EOG token        = 151664 '<|file_sep|>'
print_info: max token length = 256
load_tensors: loading model tensors, this can take a while... (mmap = true)
load_tensors: layer   0 assigned to device CUDA0, is_swa = 0
load_tensors: layer   1 assigned to device CUDA0, is_swa = 0
load_tensors: layer   2 assigned to device CUDA0, is_swa = 0
load_tensors: layer   3 assigned to device CUDA0, is_swa = 0
load_tensors: layer   4 assigned to device CUDA1, is_swa = 0
load_tensors: layer   5 assigned to device CUDA1, is_swa = 0
load_tensors: layer   6 assigned to device CUDA1, is_swa = 0
load_tensors: layer   7 assigned to device CUDA1, is_swa = 0
load_tensors: layer   8 assigned to device CUDA2, is_swa = 0
load_tensors: layer   9 assigned to device CUDA2, is_swa = 0
load_tensors: layer  10 assigned to device CUDA2, is_swa = 0
load_tensors: layer  11 assigned to device CUDA3, is_swa = 0
load_tensors: layer  12 assigned to device CUDA3, is_swa = 0
load_tensors: layer  13 assigned to device CUDA3, is_swa = 0
load_tensors: layer  14 assigned to device CUDA3, is_swa = 0
load_tensors: layer  15 assigned to device CUDA4, is_swa = 0
load_tensors: layer  16 assigned to device CUDA4, is_swa = 0
load_tensors: layer  17 assigned to device CUDA4, is_swa = 0
load_tensors: layer  18 assigned to device CUDA4, is_swa = 0
load_tensors: layer  19 assigned to device CUDA5, is_swa = 0
load_tensors: layer  20 assigned to device CUDA5, is_swa = 0
load_tensors: layer  21 assigned to device CUDA5, is_swa = 0
load_tensors: layer  22 assigned to device CUDA6, is_swa = 0
load_tensors: layer  23 assigned to device CUDA6, is_swa = 0
load_tensors: layer  24 assigned to device CUDA6, is_swa = 0
load_tensors: layer  25 assigned to device CUDA6, is_swa = 0
load_tensors: layer  26 assigned to device CUDA7, is_swa = 0
load_tensors: layer  27 assigned to device CUDA7, is_swa = 0
load_tensors: layer  28 assigned to device CUDA7, is_swa = 0
create_tensor: loading tensor token_embd.weight
create_tensor: loading tensor output_norm.weight
create_tensor: loading tensor output.weight
create_tensor: loading tensor blk.0.attn_norm.weight
create_tensor: loading tensor blk.0.attn_q.weight
create_tensor: loading tensor blk.0.attn_k.weight
create_tensor: loading tensor blk.0.attn_v.weight
create_tensor: loading tensor blk.0.attn_output.weight
create_tensor: loading tensor blk.0.attn_k_norm.weight
create_tensor: loading tensor blk.0.attn_q_norm.weight
create_tensor: loading tensor blk.0.ffn_norm.weight
create_tensor: loading tensor blk.0.ffn_gate.weight
create_tensor: loading tensor blk.0.ffn_down.weight
create_tensor: loading tensor blk.0.ffn_up.weight
create_tensor: loading tensor blk.1.attn_norm.weight
create_tensor: loading tensor blk.1.attn_q.weight
create_tensor: loading tensor blk.1.attn_k.weight
create_tensor: loading tensor blk.1.attn_v.weight
create_tensor: loading tensor blk.1.attn_output.weight
create_tensor: loading tensor blk.1.attn_k_norm.weight
create_tensor: loading tensor blk.1.attn_q_norm.weight
create_tensor: loading tensor blk.1.ffn_norm.weight
create_tensor: loading tensor blk.1.ffn_gate.weight
create_tensor: loading tensor blk.1.ffn_down.weight
create_tensor: loading tensor blk.1.ffn_up.weight
create_tensor: loading tensor blk.2.attn_norm.weight
create_tensor: loading tensor blk.2.attn_q.weight
create_tensor: loading tensor blk.2.attn_k.weight
create_tensor: loading tensor blk.2.attn_v.weight
create_tensor: loading tensor blk.2.attn_output.weight
create_tensor: loading tensor blk.2.attn_k_norm.weight
create_tensor: loading tensor blk.2.attn_q_norm.weight
create_tensor: loading tensor blk.2.ffn_norm.weight
create_tensor: loading tensor blk.2.ffn_gate.weight
create_tensor: loading tensor blk.2.ffn_down.weight
create_tensor: loading tensor blk.2.ffn_up.weight
create_tensor: loading tensor blk.3.attn_norm.weight
create_tensor: loading tensor blk.3.attn_q.weight
create_tensor: loading tensor blk.3.attn_k.weight
create_tensor: loading tensor blk.3.attn_v.weight
create_tensor: loading tensor blk.3.attn_output.weight
create_tensor: loading tensor blk.3.attn_k_norm.weight
create_tensor: loading tensor blk.3.attn_q_norm.weight
create_tensor: loading tensor blk.3.ffn_norm.weight
create_tensor: loading tensor blk.3.ffn_gate.weight
create_tensor: loading tensor blk.3.ffn_down.weight
create_tensor: loading tensor blk.3.ffn_up.weight
create_tensor: loading tensor blk.4.attn_norm.weight
create_tensor: loading tensor blk.4.attn_q.weight
create_tensor: loading tensor blk.4.attn_k.weight
create_tensor: loading tensor blk.4.attn_v.weight
create_tensor: loading tensor blk.4.attn_output.weight
create_tensor: loading tensor blk.4.attn_k_norm.weight
create_tensor: loading tensor blk.4.attn_q_norm.weight
create_tensor: loading tensor blk.4.ffn_norm.weight
create_tensor: loading tensor blk.4.ffn_gate.weight
create_tensor: loading tensor blk.4.ffn_down.weight
create_tensor: loading tensor blk.4.ffn_up.weight
create_tensor: loading tensor blk.5.attn_norm.weight
create_tensor: loading tensor blk.5.attn_q.weight
create_tensor: loading tensor blk.5.attn_k.weight
create_tensor: loading tensor blk.5.attn_v.weight
create_tensor: loading tensor blk.5.attn_output.weight
create_tensor: loading tensor blk.5.attn_k_norm.weight
create_tensor: loading tensor blk.5.attn_q_norm.weight
create_tensor: loading tensor blk.5.ffn_norm.weight
create_tensor: loading tensor blk.5.ffn_gate.weight
create_tensor: loading tensor blk.5.ffn_down.weight
create_tensor: loading tensor blk.5.ffn_up.weight
create_tensor: loading tensor blk.6.attn_norm.weight
create_tensor: loading tensor blk.6.attn_q.weight
create_tensor: loading tensor blk.6.attn_k.weight
create_tensor: loading tensor blk.6.attn_v.weight
create_tensor: loading tensor blk.6.attn_output.weight
create_tensor: loading tensor blk.6.attn_k_norm.weight
create_tensor: loading tensor blk.6.attn_q_norm.weight
create_tensor: loading tensor blk.6.ffn_norm.weight
create_tensor: loading tensor blk.6.ffn_gate.weight
create_tensor: loading tensor blk.6.ffn_down.weight
create_tensor: loading tensor blk.6.ffn_up.weight
create_tensor: loading tensor blk.7.attn_norm.weight
create_tensor: loading tensor blk.7.attn_q.weight
create_tensor: loading tensor blk.7.attn_k.weight
create_tensor: loading tensor blk.7.attn_v.weight
create_tensor: loading tensor blk.7.attn_output.weight
create_tensor: loading tensor blk.7.attn_k_norm.weight
create_tensor: loading tensor blk.7.attn_q_norm.weight
create_tensor: loading tensor blk.7.ffn_norm.weight
create_tensor: loading tensor blk.7.ffn_gate.weight
create_tensor: loading tensor blk.7.ffn_down.weight
create_tensor: loading tensor blk.7.ffn_up.weight
create_tensor: loading tensor blk.8.attn_norm.weight
create_tensor: loading tensor blk.8.attn_q.weight
create_tensor: loading tensor blk.8.attn_k.weight
create_tensor: loading tensor blk.8.attn_v.weight
create_tensor: loading tensor blk.8.attn_output.weight
create_tensor: loading tensor blk.8.attn_k_norm.weight
create_tensor: loading tensor blk.8.attn_q_norm.weight
create_tensor: loading tensor blk.8.ffn_norm.weight
create_tensor: loading tensor blk.8.ffn_gate.weight
create_tensor: loading tensor blk.8.ffn_down.weight
create_tensor: loading tensor blk.8.ffn_up.weight
create_tensor: loading tensor blk.9.attn_norm.weight
create_tensor: loading tensor blk.9.attn_q.weight
create_tensor: loading tensor blk.9.attn_k.weight
create_tensor: loading tensor blk.9.attn_v.weight
create_tensor: loading tensor blk.9.attn_output.weight
create_tensor: loading tensor blk.9.attn_k_norm.weight
create_tensor: loading tensor blk.9.attn_q_norm.weight
create_tensor: loading tensor blk.9.ffn_norm.weight
create_tensor: loading tensor blk.9.ffn_gate.weight
create_tensor: loading tensor blk.9.ffn_down.weight
create_tensor: loading tensor blk.9.ffn_up.weight
create_tensor: loading tensor blk.10.attn_norm.weight
create_tensor: loading tensor blk.10.attn_q.weight
create_tensor: loading tensor blk.10.attn_k.weight
create_tensor: loading tensor blk.10.attn_v.weight
create_tensor: loading tensor blk.10.attn_output.weight
create_tensor: loading tensor blk.10.attn_k_norm.weight
create_tensor: loading tensor blk.10.attn_q_norm.weight
create_tensor: loading tensor blk.10.ffn_norm.weight
create_tensor: loading tensor blk.10.ffn_gate.weight
create_tensor: loading tensor blk.10.ffn_down.weight
create_tensor: loading tensor blk.10.ffn_up.weight
create_tensor: loading tensor blk.11.attn_norm.weight
create_tensor: loading tensor blk.11.attn_q.weight
create_tensor: loading tensor blk.11.attn_k.weight
create_tensor: loading tensor blk.11.attn_v.weight
create_tensor: loading tensor blk.11.attn_output.weight
create_tensor: loading tensor blk.11.attn_k_norm.weight
create_tensor: loading tensor blk.11.attn_q_norm.weight
create_tensor: loading tensor blk.11.ffn_norm.weight
create_tensor: loading tensor blk.11.ffn_gate.weight
create_tensor: loading tensor blk.11.ffn_down.weight
create_tensor: loading tensor blk.11.ffn_up.weight
create_tensor: loading tensor blk.12.attn_norm.weight
create_tensor: loading tensor blk.12.attn_q.weight
create_tensor: loading tensor blk.12.attn_k.weight
create_tensor: loading tensor blk.12.attn_v.weight
create_tensor: loading tensor blk.12.attn_output.weight
create_tensor: loading tensor blk.12.attn_k_norm.weight
create_tensor: loading tensor blk.12.attn_q_norm.weight
create_tensor: loading tensor blk.12.ffn_norm.weight
create_tensor: loading tensor blk.12.ffn_gate.weight
create_tensor: loading tensor blk.12.ffn_down.weight
create_tensor: loading tensor blk.12.ffn_up.weight
create_tensor: loading tensor blk.13.attn_norm.weight
create_tensor: loading tensor blk.13.attn_q.weight
create_tensor: loading tensor blk.13.attn_k.weight
create_tensor: loading tensor blk.13.attn_v.weight
create_tensor: loading tensor blk.13.attn_output.weight
create_tensor: loading tensor blk.13.attn_k_norm.weight
create_tensor: loading tensor blk.13.attn_q_norm.weight
create_tensor: loading tensor blk.13.ffn_norm.weight
create_tensor: loading tensor blk.13.ffn_gate.weight
create_tensor: loading tensor blk.13.ffn_down.weight
create_tensor: loading tensor blk.13.ffn_up.weight
create_tensor: loading tensor blk.14.attn_norm.weight
create_tensor: loading tensor blk.14.attn_q.weight
create_tensor: loading tensor blk.14.attn_k.weight
create_tensor: loading tensor blk.14.attn_v.weight
create_tensor: loading tensor blk.14.attn_output.weight
create_tensor: loading tensor blk.14.attn_k_norm.weight
create_tensor: loading tensor blk.14.attn_q_norm.weight
create_tensor: loading tensor blk.14.ffn_norm.weight
create_tensor: loading tensor blk.14.ffn_gate.weight
create_tensor: loading tensor blk.14.ffn_down.weight
create_tensor: loading tensor blk.14.ffn_up.weight
create_tensor: loading tensor blk.15.attn_norm.weight
create_tensor: loading tensor blk.15.attn_q.weight
create_tensor: loading tensor blk.15.attn_k.weight
create_tensor: loading tensor blk.15.attn_v.weight
create_tensor: loading tensor blk.15.attn_output.weight
create_tensor: loading tensor blk.15.attn_k_norm.weight
create_tensor: loading tensor blk.15.attn_q_norm.weight
create_tensor: loading tensor blk.15.ffn_norm.weight
create_tensor: loading tensor blk.15.ffn_gate.weight
create_tensor: loading tensor blk.15.ffn_down.weight
create_tensor: loading tensor blk.15.ffn_up.weight
create_tensor: loading tensor blk.16.attn_norm.weight
create_tensor: loading tensor blk.16.attn_q.weight
create_tensor: loading tensor blk.16.attn_k.weight
create_tensor: loading tensor blk.16.attn_v.weight
create_tensor: loading tensor blk.16.attn_output.weight
create_tensor: loading tensor blk.16.attn_k_norm.weight
create_tensor: loading tensor blk.16.attn_q_norm.weight
create_tensor: loading tensor blk.16.ffn_norm.weight
create_tensor: loading tensor blk.16.ffn_gate.weight
create_tensor: loading tensor blk.16.ffn_down.weight
create_tensor: loading tensor blk.16.ffn_up.weight
create_tensor: loading tensor blk.17.attn_norm.weight
create_tensor: loading tensor blk.17.attn_q.weight
create_tensor: loading tensor blk.17.attn_k.weight
create_tensor: loading tensor blk.17.attn_v.weight
create_tensor: loading tensor blk.17.attn_output.weight
create_tensor: loading tensor blk.17.attn_k_norm.weight
create_tensor: loading tensor blk.17.attn_q_norm.weight
create_tensor: loading tensor blk.17.ffn_norm.weight
create_tensor: loading tensor blk.17.ffn_gate.weight
create_tensor: loading tensor blk.17.ffn_down.weight
create_tensor: loading tensor blk.17.ffn_up.weight
create_tensor: loading tensor blk.18.attn_norm.weight
create_tensor: loading tensor blk.18.attn_q.weight
create_tensor: loading tensor blk.18.attn_k.weight
create_tensor: loading tensor blk.18.attn_v.weight
create_tensor: loading tensor blk.18.attn_output.weight
create_tensor: loading tensor blk.18.attn_k_norm.weight
create_tensor: loading tensor blk.18.attn_q_norm.weight
create_tensor: loading tensor blk.18.ffn_norm.weight
create_tensor: loading tensor blk.18.ffn_gate.weight
create_tensor: loading tensor blk.18.ffn_down.weight
create_tensor: loading tensor blk.18.ffn_up.weight
create_tensor: loading tensor blk.19.attn_norm.weight
create_tensor: loading tensor blk.19.attn_q.weight
create_tensor: loading tensor blk.19.attn_k.weight
create_tensor: loading tensor blk.19.attn_v.weight
create_tensor: loading tensor blk.19.attn_output.weight
create_tensor: loading tensor blk.19.attn_k_norm.weight
create_tensor: loading tensor blk.19.attn_q_norm.weight
create_tensor: loading tensor blk.19.ffn_norm.weight
create_tensor: loading tensor blk.19.ffn_gate.weight
create_tensor: loading tensor blk.19.ffn_down.weight
create_tensor: loading tensor blk.19.ffn_up.weight
create_tensor: loading tensor blk.20.attn_norm.weight
create_tensor: loading tensor blk.20.attn_q.weight
create_tensor: loading tensor blk.20.attn_k.weight
create_tensor: loading tensor blk.20.attn_v.weight
create_tensor: loading tensor blk.20.attn_output.weight
create_tensor: loading tensor blk.20.attn_k_norm.weight
create_tensor: loading tensor blk.20.attn_q_norm.weight
create_tensor: loading tensor blk.20.ffn_norm.weight
create_tensor: loading tensor blk.20.ffn_gate.weight
create_tensor: loading tensor blk.20.ffn_down.weight
create_tensor: loading tensor blk.20.ffn_up.weight
create_tensor: loading tensor blk.21.attn_norm.weight
create_tensor: loading tensor blk.21.attn_q.weight
create_tensor: loading tensor blk.21.attn_k.weight
create_tensor: loading tensor blk.21.attn_v.weight
create_tensor: loading tensor blk.21.attn_output.weight
create_tensor: loading tensor blk.21.attn_k_norm.weight
create_tensor: loading tensor blk.21.attn_q_norm.weight
create_tensor: loading tensor blk.21.ffn_norm.weight
create_tensor: loading tensor blk.21.ffn_gate.weight
create_tensor: loading tensor blk.21.ffn_down.weight
create_tensor: loading tensor blk.21.ffn_up.weight
create_tensor: loading tensor blk.22.attn_norm.weight
create_tensor: loading tensor blk.22.attn_q.weight
create_tensor: loading tensor blk.22.attn_k.weight
create_tensor: loading tensor blk.22.attn_v.weight
create_tensor: loading tensor blk.22.attn_output.weight
create_tensor: loading tensor blk.22.attn_k_norm.weight
create_tensor: loading tensor blk.22.attn_q_norm.weight
create_tensor: loading tensor blk.22.ffn_norm.weight
create_tensor: loading tensor blk.22.ffn_gate.weight
create_tensor: loading tensor blk.22.ffn_down.weight
create_tensor: loading tensor blk.22.ffn_up.weight
create_tensor: loading tensor blk.23.attn_norm.weight
create_tensor: loading tensor blk.23.attn_q.weight
create_tensor: loading tensor blk.23.attn_k.weight
create_tensor: loading tensor blk.23.attn_v.weight
create_tensor: loading tensor blk.23.attn_output.weight
create_tensor: loading tensor blk.23.attn_k_norm.weight
create_tensor: loading tensor blk.23.attn_q_norm.weight
create_tensor: loading tensor blk.23.ffn_norm.weight
create_tensor: loading tensor blk.23.ffn_gate.weight
create_tensor: loading tensor blk.23.ffn_down.weight
create_tensor: loading tensor blk.23.ffn_up.weight
create_tensor: loading tensor blk.24.attn_norm.weight
create_tensor: loading tensor blk.24.attn_q.weight
create_tensor: loading tensor blk.24.attn_k.weight
create_tensor: loading tensor blk.24.attn_v.weight
create_tensor: loading tensor blk.24.attn_output.weight
create_tensor: loading tensor blk.24.attn_k_norm.weight
create_tensor: loading tensor blk.24.attn_q_norm.weight
create_tensor: loading tensor blk.24.ffn_norm.weight
create_tensor: loading tensor blk.24.ffn_gate.weight
create_tensor: loading tensor blk.24.ffn_down.weight
create_tensor: loading tensor blk.24.ffn_up.weight
create_tensor: loading tensor blk.25.attn_norm.weight
create_tensor: loading tensor blk.25.attn_q.weight
create_tensor: loading tensor blk.25.attn_k.weight
create_tensor: loading tensor blk.25.attn_v.weight
create_tensor: loading tensor blk.25.attn_output.weight
create_tensor: loading tensor blk.25.attn_k_norm.weight
create_tensor: loading tensor blk.25.attn_q_norm.weight
create_tensor: loading tensor blk.25.ffn_norm.weight
create_tensor: loading tensor blk.25.ffn_gate.weight
create_tensor: loading tensor blk.25.ffn_down.weight
create_tensor: loading tensor blk.25.ffn_up.weight
create_tensor: loading tensor blk.26.attn_norm.weight
create_tensor: loading tensor blk.26.attn_q.weight
create_tensor: loading tensor blk.26.attn_k.weight
create_tensor: loading tensor blk.26.attn_v.weight
create_tensor: loading tensor blk.26.attn_output.weight
create_tensor: loading tensor blk.26.attn_k_norm.weight
create_tensor: loading tensor blk.26.attn_q_norm.weight
create_tensor: loading tensor blk.26.ffn_norm.weight
create_tensor: loading tensor blk.26.ffn_gate.weight
create_tensor: loading tensor blk.26.ffn_down.weight
create_tensor: loading tensor blk.26.ffn_up.weight
create_tensor: loading tensor blk.27.attn_norm.weight
create_tensor: loading tensor blk.27.attn_q.weight
create_tensor: loading tensor blk.27.attn_k.weight
create_tensor: loading tensor blk.27.attn_v.weight
create_tensor: loading tensor blk.27.attn_output.weight
create_tensor: loading tensor blk.27.attn_k_norm.weight
create_tensor: loading tensor blk.27.attn_q_norm.weight
create_tensor: loading tensor blk.27.ffn_norm.weight
create_tensor: loading tensor blk.27.ffn_gate.weight
create_tensor: loading tensor blk.27.ffn_down.weight
create_tensor: loading tensor blk.27.ffn_up.weight
load_tensors: tensor 'token_embd.weight' (f16) (and 0 others) cannot be used with preferred buffer type CUDA_Host, using CPU instead
load_tensors: offloading 28 repeating layers to GPU
load_tensors: offloading output layer to GPU
load_tensors: offloaded 29/29 layers to GPU
load_tensors:   CPU_Mapped model buffer size =   593.50 MiB
load_tensors:        CUDA0 model buffer size =   384.07 MiB
load_tensors:        CUDA1 model buffer size =   384.07 MiB
load_tensors:        CUDA2 model buffer size =   288.05 MiB
load_tensors:        CUDA3 model buffer size =   384.07 MiB
load_tensors:        CUDA4 model buffer size =   384.07 MiB
load_tensors:        CUDA5 model buffer size =   288.05 MiB
load_tensors:        CUDA6 model buffer size =   384.07 MiB
load_tensors:        CUDA7 model buffer size =   785.54 MiB
.......................................................................

================================================================================
GPU WARMUP - Running 3 iterations
================================================================================
Warmup iteration 1/3... llama_context: constructing llama_context
llama_context: n_seq_max     = 2
llama_context: n_ctx         = 8192
llama_context: n_ctx_seq     = 4096
llama_context: n_batch       = 2048
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = enabled
llama_context: kv_unified    = false
llama_context: freq_base     = 1000000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_seq (4096) < n_ctx_train (32768) -- the full capacity of the model will not be utilized
set_abort_callback: call
llama_context:  CUDA_Host  output buffer size =     1.16 MiB
llama_kv_cache: layer   0: dev = CUDA0
llama_kv_cache: layer   1: dev = CUDA0
llama_kv_cache: layer   2: dev = CUDA0
llama_kv_cache: layer   3: dev = CUDA0
llama_kv_cache: layer   4: dev = CUDA1
llama_kv_cache: layer   5: dev = CUDA1
llama_kv_cache: layer   6: dev = CUDA1
llama_kv_cache: layer   7: dev = CUDA1
llama_kv_cache: layer   8: dev = CUDA2
llama_kv_cache: layer   9: dev = CUDA2
llama_kv_cache: layer  10: dev = CUDA2
llama_kv_cache: layer  11: dev = CUDA3
llama_kv_cache: layer  12: dev = CUDA3
llama_kv_cache: layer  13: dev = CUDA3
llama_kv_cache: layer  14: dev = CUDA3
llama_kv_cache: layer  15: dev = CUDA4
llama_kv_cache: layer  16: dev = CUDA4
llama_kv_cache: layer  17: dev = CUDA4
llama_kv_cache: layer  18: dev = CUDA4
llama_kv_cache: layer  19: dev = CUDA5
llama_kv_cache: layer  20: dev = CUDA5
llama_kv_cache: layer  21: dev = CUDA5
llama_kv_cache: layer  22: dev = CUDA6
llama_kv_cache: layer  23: dev = CUDA6
llama_kv_cache: layer  24: dev = CUDA6
llama_kv_cache: layer  25: dev = CUDA6
llama_kv_cache: layer  26: dev = CUDA7
llama_kv_cache: layer  27: dev = CUDA7
llama_kv_cache:      CUDA0 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA1 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA2 KV buffer size =    96.00 MiB
llama_kv_cache:      CUDA3 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA4 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA5 KV buffer size =    96.00 MiB
llama_kv_cache:      CUDA6 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA7 KV buffer size =    64.00 MiB
llama_kv_cache: size =  896.00 MiB (  4096 cells,  28 layers,  2/2 seqs), K (f16):  448.00 MiB, V (f16):  448.00 MiB
llama_context: enumerating backends
llama_context: backend_ptrs.size() = 9
llama_context: max_nodes = 2488
llama_context: pipeline parallelism enabled (n_copies=4)
llama_context: reserving full memory module
llama_context: worst-case: n_tokens = 512, n_seqs = 2, n_outputs = 2
graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  2, n_outputs =  512
graph_reserve: reserving a graph for ubatch with n_tokens =    2, n_seqs =  2, n_outputs =    2
graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  2, n_outputs =  512
llama_context:      CUDA0 compute buffer size =   100.04 MiB
llama_context:      CUDA1 compute buffer size =    96.04 MiB
llama_context:      CUDA2 compute buffer size =    96.04 MiB
llama_context:      CUDA3 compute buffer size =    96.04 MiB
llama_context:      CUDA4 compute buffer size =    96.04 MiB
llama_context:      CUDA5 compute buffer size =    96.04 MiB
llama_context:      CUDA6 compute buffer size =    96.04 MiB
llama_context:      CUDA7 compute buffer size =   348.80 MiB
llama_context:  CUDA_Host compute buffer size =    36.05 MiB
llama_context: graph nodes  = 1043
llama_context: graph splits = 9
completed in 1066.92 ms
Warmup iteration 2/3... llama_context: constructing llama_context
llama_context: n_seq_max     = 2
llama_context: n_ctx         = 8192
llama_context: n_ctx_seq     = 4096
llama_context: n_batch       = 2048
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = enabled
llama_context: kv_unified    = false
llama_context: freq_base     = 1000000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_seq (4096) < n_ctx_train (32768) -- the full capacity of the model will not be utilized
set_abort_callback: call
llama_context:  CUDA_Host  output buffer size =     1.16 MiB
llama_kv_cache: layer   0: dev = CUDA0
llama_kv_cache: layer   1: dev = CUDA0
llama_kv_cache: layer   2: dev = CUDA0
llama_kv_cache: layer   3: dev = CUDA0
llama_kv_cache: layer   4: dev = CUDA1
llama_kv_cache: layer   5: dev = CUDA1
llama_kv_cache: layer   6: dev = CUDA1
llama_kv_cache: layer   7: dev = CUDA1
llama_kv_cache: layer   8: dev = CUDA2
llama_kv_cache: layer   9: dev = CUDA2
llama_kv_cache: layer  10: dev = CUDA2
llama_kv_cache: layer  11: dev = CUDA3
llama_kv_cache: layer  12: dev = CUDA3
llama_kv_cache: layer  13: dev = CUDA3
llama_kv_cache: layer  14: dev = CUDA3
llama_kv_cache: layer  15: dev = CUDA4
llama_kv_cache: layer  16: dev = CUDA4
llama_kv_cache: layer  17: dev = CUDA4
llama_kv_cache: layer  18: dev = CUDA4
llama_kv_cache: layer  19: dev = CUDA5
llama_kv_cache: layer  20: dev = CUDA5
llama_kv_cache: layer  21: dev = CUDA5
llama_kv_cache: layer  22: dev = CUDA6
llama_kv_cache: layer  23: dev = CUDA6
llama_kv_cache: layer  24: dev = CUDA6
llama_kv_cache: layer  25: dev = CUDA6
llama_kv_cache: layer  26: dev = CUDA7
llama_kv_cache: layer  27: dev = CUDA7
llama_kv_cache:      CUDA0 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA1 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA2 KV buffer size =    96.00 MiB
llama_kv_cache:      CUDA3 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA4 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA5 KV buffer size =    96.00 MiB
llama_kv_cache:      CUDA6 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA7 KV buffer size =    64.00 MiB
llama_kv_cache: size =  896.00 MiB (  4096 cells,  28 layers,  2/2 seqs), K (f16):  448.00 MiB, V (f16):  448.00 MiB
llama_context: enumerating backends
llama_context: backend_ptrs.size() = 9
llama_context: max_nodes = 2488
llama_context: pipeline parallelism enabled (n_copies=4)
llama_context: reserving full memory module
llama_context: worst-case: n_tokens = 512, n_seqs = 2, n_outputs = 2
graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  2, n_outputs =  512
graph_reserve: reserving a graph for ubatch with n_tokens =    2, n_seqs =  2, n_outputs =    2
graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  2, n_outputs =  512
llama_context:      CUDA0 compute buffer size =   100.04 MiB
llama_context:      CUDA1 compute buffer size =    96.04 MiB
llama_context:      CUDA2 compute buffer size =    96.04 MiB
llama_context:      CUDA3 compute buffer size =    96.04 MiB
llama_context:      CUDA4 compute buffer size =    96.04 MiB
llama_context:      CUDA5 compute buffer size =    96.04 MiB
llama_context:      CUDA6 compute buffer size =    96.04 MiB
llama_context:      CUDA7 compute buffer size =   348.80 MiB
llama_context:  CUDA_Host compute buffer size =    36.05 MiB
llama_context: graph nodes  = 1043
llama_context: graph splits = 9
completed in 778.47 ms
Warmup iteration 3/3... llama_context: constructing llama_context
llama_context: n_seq_max     = 2
llama_context: n_ctx         = 8192
llama_context: n_ctx_seq     = 4096
llama_context: n_batch       = 2048
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = enabled
llama_context: kv_unified    = false
llama_context: freq_base     = 1000000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_seq (4096) < n_ctx_train (32768) -- the full capacity of the model will not be utilized
set_abort_callback: call
llama_context:  CUDA_Host  output buffer size =     1.16 MiB
llama_kv_cache: layer   0: dev = CUDA0
llama_kv_cache: layer   1: dev = CUDA0
llama_kv_cache: layer   2: dev = CUDA0
llama_kv_cache: layer   3: dev = CUDA0
llama_kv_cache: layer   4: dev = CUDA1
llama_kv_cache: layer   5: dev = CUDA1
llama_kv_cache: layer   6: dev = CUDA1
llama_kv_cache: layer   7: dev = CUDA1
llama_kv_cache: layer   8: dev = CUDA2
llama_kv_cache: layer   9: dev = CUDA2
llama_kv_cache: layer  10: dev = CUDA2
llama_kv_cache: layer  11: dev = CUDA3
llama_kv_cache: layer  12: dev = CUDA3
llama_kv_cache: layer  13: dev = CUDA3
llama_kv_cache: layer  14: dev = CUDA3
llama_kv_cache: layer  15: dev = CUDA4
llama_kv_cache: layer  16: dev = CUDA4
llama_kv_cache: layer  17: dev = CUDA4
llama_kv_cache: layer  18: dev = CUDA4
llama_kv_cache: layer  19: dev = CUDA5
llama_kv_cache: layer  20: dev = CUDA5
llama_kv_cache: layer  21: dev = CUDA5
llama_kv_cache: layer  22: dev = CUDA6
llama_kv_cache: layer  23: dev = CUDA6
llama_kv_cache: layer  24: dev = CUDA6
llama_kv_cache: layer  25: dev = CUDA6
llama_kv_cache: layer  26: dev = CUDA7
llama_kv_cache: layer  27: dev = CUDA7
llama_kv_cache:      CUDA0 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA1 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA2 KV buffer size =    96.00 MiB
llama_kv_cache:      CUDA3 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA4 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA5 KV buffer size =    96.00 MiB
llama_kv_cache:      CUDA6 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA7 KV buffer size =    64.00 MiB
llama_kv_cache: size =  896.00 MiB (  4096 cells,  28 layers,  2/2 seqs), K (f16):  448.00 MiB, V (f16):  448.00 MiB
llama_context: enumerating backends
llama_context: backend_ptrs.size() = 9
llama_context: max_nodes = 2488
llama_context: pipeline parallelism enabled (n_copies=4)
llama_context: reserving full memory module
llama_context: worst-case: n_tokens = 512, n_seqs = 2, n_outputs = 2
graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  2, n_outputs =  512
graph_reserve: reserving a graph for ubatch with n_tokens =    2, n_seqs =  2, n_outputs =    2
graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  2, n_outputs =  512
llama_context:      CUDA0 compute buffer size =   100.04 MiB
llama_context:      CUDA1 compute buffer size =    96.04 MiB
llama_context:      CUDA2 compute buffer size =    96.04 MiB
llama_context:      CUDA3 compute buffer size =    96.04 MiB
llama_context:      CUDA4 compute buffer size =    96.04 MiB
llama_context:      CUDA5 compute buffer size =    96.04 MiB
llama_context:      CUDA6 compute buffer size =    96.04 MiB
llama_context:      CUDA7 compute buffer size =   348.80 MiB
llama_context:  CUDA_Host compute buffer size =    36.05 MiB
llama_context: graph nodes  = 1043
llama_context: graph splits = 9
llama_context: constructing llama_context
llama_context: n_seq_max     = 2
llama_context: n_ctx         = 8192
llama_context: n_ctx_seq     = 4096
llama_context: n_batch       = 2048
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = enabled
llama_context: kv_unified    = false
llama_context: freq_base     = 1000000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_seq (4096) < n_ctx_train (32768) -- the full capacity of the model will not be utilized
set_abort_callback: call
llama_context:  CUDA_Host  output buffer size =     1.16 MiB
llama_kv_cache: layer   0: dev = CUDA0
llama_kv_cache: layer   1: dev = CUDA0
llama_kv_cache: layer   2: dev = CUDA0
llama_kv_cache: layer   3: dev = CUDA0
llama_kv_cache: layer   4: dev = CUDA1
llama_kv_cache: layer   5: dev = CUDA1
llama_kv_cache: layer   6: dev = CUDA1
llama_kv_cache: layer   7: dev = CUDA1
llama_kv_cache: layer   8: dev = CUDA2
llama_kv_cache: layer   9: dev = CUDA2
llama_kv_cache: layer  10: dev = CUDA2
llama_kv_cache: layer  11: dev = CUDA3
llama_kv_cache: layer  12: dev = CUDA3
llama_kv_cache: layer  13: dev = CUDA3
llama_kv_cache: layer  14: dev = CUDA3
llama_kv_cache: layer  15: dev = CUDA4
llama_kv_cache: layer  16: dev = CUDA4
llama_kv_cache: layer  17: dev = CUDA4
llama_kv_cache: layer  18: dev = CUDA4
llama_kv_cache: layer  19: dev = CUDA5
llama_kv_cache: layer  20: dev = CUDA5
llama_kv_cache: layer  21: dev = CUDA5
llama_kv_cache: layer  22: dev = CUDA6
llama_kv_cache: layer  23: dev = CUDA6
llama_kv_cache: layer  24: dev = CUDA6
llama_kv_cache: layer  25: dev = CUDA6
llama_kv_cache: layer  26: dev = CUDA7
llama_kv_cache: layer  27: dev = CUDA7
llama_kv_cache:      CUDA0 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA1 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA2 KV buffer size =    96.00 MiB
llama_kv_cache:      CUDA3 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA4 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA5 KV buffer size =    96.00 MiB
llama_kv_cache:      CUDA6 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA7 KV buffer size =    64.00 MiB
llama_kv_cache: size =  896.00 MiB (  4096 cells,  28 layers,  2/2 seqs), K (f16):  448.00 MiB, V (f16):  448.00 MiB
llama_context: enumerating backends
llama_context: backend_ptrs.size() = 9
llama_context: max_nodes = 2488
llama_context: pipeline parallelism enabled (n_copies=4)
llama_context: reserving full memory module
llama_context: worst-case: n_tokens = 512, n_seqs = 2, n_outputs = 2
graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  2, n_outputs =  512
graph_reserve: reserving a graph for ubatch with n_tokens =    2, n_seqs =  2, n_outputs =    2
graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  2, n_outputs =  512
llama_context:      CUDA0 compute buffer size =   100.04 MiB
llama_context:      CUDA1 compute buffer size =    96.04 MiB
llama_context:      CUDA2 compute buffer size =    96.04 MiB
llama_context:      CUDA3 compute buffer size =    96.04 MiB
llama_context:      CUDA4 compute buffer size =    96.04 MiB
llama_context:      CUDA5 compute buffer size =    96.04 MiB
llama_context:      CUDA6 compute buffer size =    96.04 MiB
llama_context:      CUDA7 compute buffer size =   348.80 MiB
llama_context:  CUDA_Host compute buffer size =    36.05 MiB
llama_context: graph nodes  = 1043
llama_context: graph splits = 9
llama_context: constructing llama_context
llama_context: n_seq_max     = 2
llama_context: n_ctx         = 8192
llama_context: n_ctx_seq     = 4096
llama_context: n_batch       = 2048
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = enabled
llama_context: kv_unified    = false
llama_context: freq_base     = 1000000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_seq (4096) < n_ctx_train (32768) -- the full capacity of the model will not be utilized
set_abort_callback: call
llama_context:  CUDA_Host  output buffer size =     1.16 MiB
llama_kv_cache: layer   0: dev = CUDA0
llama_kv_cache: layer   1: dev = CUDA0
llama_kv_cache: layer   2: dev = CUDA0
llama_kv_cache: layer   3: dev = CUDA0
llama_kv_cache: layer   4: dev = CUDA1
llama_kv_cache: layer   5: dev = CUDA1
llama_kv_cache: layer   6: dev = CUDA1
llama_kv_cache: layer   7: dev = CUDA1
llama_kv_cache: layer   8: dev = CUDA2
llama_kv_cache: layer   9: dev = CUDA2
llama_kv_cache: layer  10: dev = CUDA2
llama_kv_cache: layer  11: dev = CUDA3
llama_kv_cache: layer  12: dev = CUDA3
llama_kv_cache: layer  13: dev = CUDA3
llama_kv_cache: layer  14: dev = CUDA3
llama_kv_cache: layer  15: dev = CUDA4
llama_kv_cache: layer  16: dev = CUDA4
llama_kv_cache: layer  17: dev = CUDA4
llama_kv_cache: layer  18: dev = CUDA4
llama_kv_cache: layer  19: dev = CUDA5
llama_kv_cache: layer  20: dev = CUDA5
llama_kv_cache: layer  21: dev = CUDA5
llama_kv_cache: layer  22: dev = CUDA6
llama_kv_cache: layer  23: dev = CUDA6
llama_kv_cache: layer  24: dev = CUDA6
llama_kv_cache: layer  25: dev = CUDA6
llama_kv_cache: layer  26: dev = CUDA7
llama_kv_cache: layer  27: dev = CUDA7
llama_kv_cache:      CUDA0 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA1 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA2 KV buffer size =    96.00 MiB
llama_kv_cache:      CUDA3 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA4 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA5 KV buffer size =    96.00 MiB
llama_kv_cache:      CUDA6 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA7 KV buffer size =    64.00 MiB
llama_kv_cache: size =  896.00 MiB (  4096 cells,  28 layers,  2/2 seqs), K (f16):  448.00 MiB, V (f16):  448.00 MiB
llama_context: enumerating backends
llama_context: backend_ptrs.size() = 9
llama_context: max_nodes = 2488
llama_context: pipeline parallelism enabled (n_copies=4)
llama_context: reserving full memory module
llama_context: worst-case: n_tokens = 512, n_seqs = 2, n_outputs = 2
graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  2, n_outputs =  512
graph_reserve: reserving a graph for ubatch with n_tokens =    2, n_seqs =  2, n_outputs =    2
graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  2, n_outputs =  512
llama_context:      CUDA0 compute buffer size =   100.04 MiB
llama_context:      CUDA1 compute buffer size =    96.04 MiB
llama_context:      CUDA2 compute buffer size =    96.04 MiB
llama_context:      CUDA3 compute buffer size =    96.04 MiB
llama_context:      CUDA4 compute buffer size =    96.04 MiB
llama_context:      CUDA5 compute buffer size =    96.04 MiB
llama_context:      CUDA6 compute buffer size =    96.04 MiB
llama_context:      CUDA7 compute buffer size =   348.80 MiB
llama_context:  CUDA_Host compute buffer size =    36.05 MiB
llama_context: graph nodes  = 1043
llama_context: graph splits = 9
llama_context: constructing llama_context
llama_context: n_seq_max     = 2
llama_context: n_ctx         = 8192
llama_context: n_ctx_seq     = 4096
llama_context: n_batch       = 2048
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = enabled
llama_context: kv_unified    = false
llama_context: freq_base     = 1000000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_seq (4096) < n_ctx_train (32768) -- the full capacity of the model will not be utilized
set_abort_callback: call
llama_context:  CUDA_Host  output buffer size =     1.16 MiB
llama_kv_cache: layer   0: dev = CUDA0
llama_kv_cache: layer   1: dev = CUDA0
llama_kv_cache: layer   2: dev = CUDA0
llama_kv_cache: layer   3: dev = CUDA0
llama_kv_cache: layer   4: dev = CUDA1
llama_kv_cache: layer   5: dev = CUDA1
llama_kv_cache: layer   6: dev = CUDA1
llama_kv_cache: layer   7: dev = CUDA1
llama_kv_cache: layer   8: dev = CUDA2
llama_kv_cache: layer   9: dev = CUDA2
llama_kv_cache: layer  10: dev = CUDA2
llama_kv_cache: layer  11: dev = CUDA3
llama_kv_cache: layer  12: dev = CUDA3
llama_kv_cache: layer  13: dev = CUDA3
llama_kv_cache: layer  14: dev = CUDA3
llama_kv_cache: layer  15: dev = CUDA4
llama_kv_cache: layer  16: dev = CUDA4
llama_kv_cache: layer  17: dev = CUDA4
llama_kv_cache: layer  18: dev = CUDA4
llama_kv_cache: layer  19: dev = CUDA5
llama_kv_cache: layer  20: dev = CUDA5
llama_kv_cache: layer  21: dev = CUDA5
llama_kv_cache: layer  22: dev = CUDA6
llama_kv_cache: layer  23: dev = CUDA6
llama_kv_cache: layer  24: dev = CUDA6
llama_kv_cache: layer  25: dev = CUDA6
llama_kv_cache: layer  26: dev = CUDA7
llama_kv_cache: layer  27: dev = CUDA7
llama_kv_cache:      CUDA0 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA1 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA2 KV buffer size =    96.00 MiB
llama_kv_cache:      CUDA3 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA4 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA5 KV buffer size =    96.00 MiB
llama_kv_cache:      CUDA6 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA7 KV buffer size =    64.00 MiB
llama_kv_cache: size =  896.00 MiB (  4096 cells,  28 layers,  2/2 seqs), K (f16):  448.00 MiB, V (f16):  448.00 MiB
llama_context: enumerating backends
llama_context: backend_ptrs.size() = 9
llama_context: max_nodes = 2488
llama_context: pipeline parallelism enabled (n_copies=4)
llama_context: reserving full memory module
llama_context: worst-case: n_tokens = 512, n_seqs = 2, n_outputs = 2
graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  2, n_outputs =  512
graph_reserve: reserving a graph for ubatch with n_tokens =    2, n_seqs =  2, n_outputs =    2
graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  2, n_outputs =  512
llama_context:      CUDA0 compute buffer size =   100.04 MiB
llama_context:      CUDA1 compute buffer size =    96.04 MiB
llama_context:      CUDA2 compute buffer size =    96.04 MiB
llama_context:      CUDA3 compute buffer size =    96.04 MiB
llama_context:      CUDA4 compute buffer size =    96.04 MiB
llama_context:      CUDA5 compute buffer size =    96.04 MiB
llama_context:      CUDA6 compute buffer size =    96.04 MiB
llama_context:      CUDA7 compute buffer size =   348.80 MiB
llama_context:  CUDA_Host compute buffer size =    36.05 MiB
llama_context: graph nodes  = 1043
llama_context: graph splits = 9
llama_context: constructing llama_context
llama_context: n_seq_max     = 2
llama_context: n_ctx         = 8192
llama_context: n_ctx_seq     = 4096
llama_context: n_batch       = 2048
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = enabled
llama_context: kv_unified    = false
llama_context: freq_base     = 1000000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_seq (4096) < n_ctx_train (32768) -- the full capacity of the model will not be utilized
set_abort_callback: call
llama_context:  CUDA_Host  output buffer size =     1.16 MiB
llama_kv_cache: layer   0: dev = CUDA0
llama_kv_cache: layer   1: dev = CUDA0
llama_kv_cache: layer   2: dev = CUDA0
llama_kv_cache: layer   3: dev = CUDA0
llama_kv_cache: layer   4: dev = CUDA1
llama_kv_cache: layer   5: dev = CUDA1
llama_kv_cache: layer   6: dev = CUDA1
llama_kv_cache: layer   7: dev = CUDA1
llama_kv_cache: layer   8: dev = CUDA2
llama_kv_cache: layer   9: dev = CUDA2
llama_kv_cache: layer  10: dev = CUDA2
llama_kv_cache: layer  11: dev = CUDA3
llama_kv_cache: layer  12: dev = CUDA3
llama_kv_cache: layer  13: dev = CUDA3
llama_kv_cache: layer  14: dev = CUDA3
llama_kv_cache: layer  15: dev = CUDA4
llama_kv_cache: layer  16: dev = CUDA4
llama_kv_cache: layer  17: dev = CUDA4
llama_kv_cache: layer  18: dev = CUDA4
llama_kv_cache: layer  19: dev = CUDA5
llama_kv_cache: layer  20: dev = CUDA5
llama_kv_cache: layer  21: dev = CUDA5
llama_kv_cache: layer  22: dev = CUDA6
llama_kv_cache: layer  23: dev = CUDA6
llama_kv_cache: layer  24: dev = CUDA6
llama_kv_cache: layer  25: dev = CUDA6
llama_kv_cache: layer  26: dev = CUDA7
llama_kv_cache: layer  27: dev = CUDA7
llama_kv_cache:      CUDA0 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA1 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA2 KV buffer size =    96.00 MiB
llama_kv_cache:      CUDA3 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA4 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA5 KV buffer size =    96.00 MiB
llama_kv_cache:      CUDA6 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA7 KV buffer size =    64.00 MiB
llama_kv_cache: size =  896.00 MiB (  4096 cells,  28 layers,  2/2 seqs), K (f16):  448.00 MiB, V (f16):  448.00 MiB
llama_context: enumerating backends
llama_context: backend_ptrs.size() = 9
llama_context: max_nodes = 2488
llama_context: pipeline parallelism enabled (n_copies=4)
llama_context: reserving full memory module
llama_context: worst-case: n_tokens = 512, n_seqs = 2, n_outputs = 2
graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  2, n_outputs =  512
graph_reserve: reserving a graph for ubatch with n_tokens =    2, n_seqs =  2, n_outputs =    2
graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  2, n_outputs =  512
llama_context:      CUDA0 compute buffer size =   100.04 MiB
llama_context:      CUDA1 compute buffer size =    96.04 MiB
llama_context:      CUDA2 compute buffer size =    96.04 MiB
llama_context:      CUDA3 compute buffer size =    96.04 MiB
llama_context:      CUDA4 compute buffer size =    96.04 MiB
llama_context:      CUDA5 compute buffer size =    96.04 MiB
llama_context:      CUDA6 compute buffer size =    96.04 MiB
llama_context:      CUDA7 compute buffer size =   348.80 MiB
llama_context:  CUDA_Host compute buffer size =    36.05 MiB
llama_context: graph nodes  = 1043
llama_context: graph splits = 9
llama_context: constructing llama_context
llama_context: n_seq_max     = 2
llama_context: n_ctx         = 8192
llama_context: n_ctx_seq     = 4096
llama_context: n_batch       = 2048
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = enabled
llama_context: kv_unified    = false
llama_context: freq_base     = 1000000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_seq (4096) < n_ctx_train (32768) -- the full capacity of the model will not be utilized
set_abort_callback: call
llama_context:  CUDA_Host  output buffer size =     1.16 MiB
llama_kv_cache: layer   0: dev = CUDA0
llama_kv_cache: layer   1: dev = CUDA0
llama_kv_cache: layer   2: dev = CUDA0
llama_kv_cache: layer   3: dev = CUDA0
llama_kv_cache: layer   4: dev = CUDA1
llama_kv_cache: layer   5: dev = CUDA1
llama_kv_cache: layer   6: dev = CUDA1
llama_kv_cache: layer   7: dev = CUDA1
llama_kv_cache: layer   8: dev = CUDA2
llama_kv_cache: layer   9: dev = CUDA2
llama_kv_cache: layer  10: dev = CUDA2
llama_kv_cache: layer  11: dev = CUDA3
llama_kv_cache: layer  12: dev = CUDA3
llama_kv_cache: layer  13: dev = CUDA3
llama_kv_cache: layer  14: dev = CUDA3
llama_kv_cache: layer  15: dev = CUDA4
llama_kv_cache: layer  16: dev = CUDA4
llama_kv_cache: layer  17: dev = CUDA4
llama_kv_cache: layer  18: dev = CUDA4
llama_kv_cache: layer  19: dev = CUDA5
llama_kv_cache: layer  20: dev = CUDA5
llama_kv_cache: layer  21: dev = CUDA5
llama_kv_cache: layer  22: dev = CUDA6
llama_kv_cache: layer  23: dev = CUDA6
llama_kv_cache: layer  24: dev = CUDA6
llama_kv_cache: layer  25: dev = CUDA6
llama_kv_cache: layer  26: dev = CUDA7
llama_kv_cache: layer  27: dev = CUDA7
llama_kv_cache:      CUDA0 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA1 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA2 KV buffer size =    96.00 MiB
llama_kv_cache:      CUDA3 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA4 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA5 KV buffer size =    96.00 MiB
llama_kv_cache:      CUDA6 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA7 KV buffer size =    64.00 MiB
llama_kv_cache: size =  896.00 MiB (  4096 cells,  28 layers,  2/2 seqs), K (f16):  448.00 MiB, V (f16):  448.00 MiB
llama_context: enumerating backends
llama_context: backend_ptrs.size() = 9
llama_context: max_nodes = 2488
llama_context: pipeline parallelism enabled (n_copies=4)
llama_context: reserving full memory module
llama_context: worst-case: n_tokens = 512, n_seqs = 2, n_outputs = 2
graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  2, n_outputs =  512
graph_reserve: reserving a graph for ubatch with n_tokens =    2, n_seqs =  2, n_outputs =    2
graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  2, n_outputs =  512
llama_context:      CUDA0 compute buffer size =   100.04 MiB
llama_context:      CUDA1 compute buffer size =    96.04 MiB
llama_context:      CUDA2 compute buffer size =    96.04 MiB
llama_context:      CUDA3 compute buffer size =    96.04 MiB
llama_context:      CUDA4 compute buffer size =    96.04 MiB
llama_context:      CUDA5 compute buffer size =    96.04 MiB
llama_context:      CUDA6 compute buffer size =    96.04 MiB
llama_context:      CUDA7 compute buffer size =   348.80 MiB
llama_context:  CUDA_Host compute buffer size =    36.05 MiB
llama_context: graph nodes  = 1043
llama_context: graph splits = 9
llama_context: constructing llama_context
llama_context: n_seq_max     = 2
llama_context: n_ctx         = 8192
llama_context: n_ctx_seq     = 4096
llama_context: n_batch       = 2048
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = enabled
llama_context: kv_unified    = false
llama_context: freq_base     = 1000000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_seq (4096) < n_ctx_train (32768) -- the full capacity of the model will not be utilized
set_abort_callback: call
llama_context:  CUDA_Host  output buffer size =     1.16 MiB
llama_kv_cache: layer   0: dev = CUDA0
llama_kv_cache: layer   1: dev = CUDA0
llama_kv_cache: layer   2: dev = CUDA0
llama_kv_cache: layer   3: dev = CUDA0
llama_kv_cache: layer   4: dev = CUDA1
llama_kv_cache: layer   5: dev = CUDA1
llama_kv_cache: layer   6: dev = CUDA1
llama_kv_cache: layer   7: dev = CUDA1
llama_kv_cache: layer   8: dev = CUDA2
llama_kv_cache: layer   9: dev = CUDA2
llama_kv_cache: layer  10: dev = CUDA2
llama_kv_cache: layer  11: dev = CUDA3
llama_kv_cache: layer  12: dev = CUDA3
llama_kv_cache: layer  13: dev = CUDA3
llama_kv_cache: layer  14: dev = CUDA3
llama_kv_cache: layer  15: dev = CUDA4
llama_kv_cache: layer  16: dev = CUDA4
llama_kv_cache: layer  17: dev = CUDA4
llama_kv_cache: layer  18: dev = CUDA4
llama_kv_cache: layer  19: dev = CUDA5
llama_kv_cache: layer  20: dev = CUDA5
llama_kv_cache: layer  21: dev = CUDA5
llama_kv_cache: layer  22: dev = CUDA6
llama_kv_cache: layer  23: dev = CUDA6
llama_kv_cache: layer  24: dev = CUDA6
llama_kv_cache: layer  25: dev = CUDA6
llama_kv_cache: layer  26: dev = CUDA7
llama_kv_cache: layer  27: dev = CUDA7
llama_kv_cache:      CUDA0 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA1 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA2 KV buffer size =    96.00 MiB
llama_kv_cache:      CUDA3 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA4 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA5 KV buffer size =    96.00 MiB
llama_kv_cache:      CUDA6 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA7 KV buffer size =    64.00 MiB
llama_kv_cache: size =  896.00 MiB (  4096 cells,  28 layers,  2/2 seqs), K (f16):  448.00 MiB, V (f16):  448.00 MiB
llama_context: enumerating backends
llama_context: backend_ptrs.size() = 9
llama_context: max_nodes = 2488
llama_context: pipeline parallelism enabled (n_copies=4)
llama_context: reserving full memory module
llama_context: worst-case: n_tokens = 512, n_seqs = 2, n_outputs = 2
graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  2, n_outputs =  512
graph_reserve: reserving a graph for ubatch with n_tokens =    2, n_seqs =  2, n_outputs =    2
graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  2, n_outputs =  512
llama_context:      CUDA0 compute buffer size =   100.04 MiB
llama_context:      CUDA1 compute buffer size =    96.04 MiB
llama_context:      CUDA2 compute buffer size =    96.04 MiB
llama_context:      CUDA3 compute buffer size =    96.04 MiB
llama_context:      CUDA4 compute buffer size =    96.04 MiB
llama_context:      CUDA5 compute buffer size =    96.04 MiB
llama_context:      CUDA6 compute buffer size =    96.04 MiB
llama_context:      CUDA7 compute buffer size =   348.80 MiB
llama_context:  CUDA_Host compute buffer size =    36.05 MiB
llama_context: graph nodes  = 1043
llama_context: graph splits = 9
llama_context: constructing llama_context
llama_context: n_seq_max     = 2
llama_context: n_ctx         = 8192
llama_context: n_ctx_seq     = 4096
llama_context: n_batch       = 2048
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = enabled
llama_context: kv_unified    = false
llama_context: freq_base     = 1000000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_seq (4096) < n_ctx_train (32768) -- the full capacity of the model will not be utilized
set_abort_callback: call
llama_context:  CUDA_Host  output buffer size =     1.16 MiB
llama_kv_cache: layer   0: dev = CUDA0
llama_kv_cache: layer   1: dev = CUDA0
llama_kv_cache: layer   2: dev = CUDA0
llama_kv_cache: layer   3: dev = CUDA0
llama_kv_cache: layer   4: dev = CUDA1
llama_kv_cache: layer   5: dev = CUDA1
llama_kv_cache: layer   6: dev = CUDA1
llama_kv_cache: layer   7: dev = CUDA1
llama_kv_cache: layer   8: dev = CUDA2
llama_kv_cache: layer   9: dev = CUDA2
llama_kv_cache: layer  10: dev = CUDA2
llama_kv_cache: layer  11: dev = CUDA3
llama_kv_cache: layer  12: dev = CUDA3
llama_kv_cache: layer  13: dev = CUDA3
llama_kv_cache: layer  14: dev = CUDA3
llama_kv_cache: layer  15: dev = CUDA4
llama_kv_cache: layer  16: dev = CUDA4
llama_kv_cache: layer  17: dev = CUDA4
llama_kv_cache: layer  18: dev = CUDA4
llama_kv_cache: layer  19: dev = CUDA5
llama_kv_cache: layer  20: dev = CUDA5
llama_kv_cache: layer  21: dev = CUDA5
llama_kv_cache: layer  22: dev = CUDA6
llama_kv_cache: layer  23: dev = CUDA6
llama_kv_cache: layer  24: dev = CUDA6
llama_kv_cache: layer  25: dev = CUDA6
llama_kv_cache: layer  26: dev = CUDA7
llama_kv_cache: layer  27: dev = CUDA7
llama_kv_cache:      CUDA0 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA1 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA2 KV buffer size =    96.00 MiB
llama_kv_cache:      CUDA3 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA4 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA5 KV buffer size =    96.00 MiB
llama_kv_cache:      CUDA6 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA7 KV buffer size =    64.00 MiB
llama_kv_cache: size =  896.00 MiB (  4096 cells,  28 layers,  2/2 seqs), K (f16):  448.00 MiB, V (f16):  448.00 MiB
llama_context: enumerating backends
llama_context: backend_ptrs.size() = 9
llama_context: max_nodes = 2488
llama_context: pipeline parallelism enabled (n_copies=4)
llama_context: reserving full memory module
llama_context: worst-case: n_tokens = 512, n_seqs = 2, n_outputs = 2
graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  2, n_outputs =  512
graph_reserve: reserving a graph for ubatch with n_tokens =    2, n_seqs =  2, n_outputs =    2
graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  2, n_outputs =  512
llama_context:      CUDA0 compute buffer size =   100.04 MiB
llama_context:      CUDA1 compute buffer size =    96.04 MiB
llama_context:      CUDA2 compute buffer size =    96.04 MiB
llama_context:      CUDA3 compute buffer size =    96.04 MiB
llama_context:      CUDA4 compute buffer size =    96.04 MiB
llama_context:      CUDA5 compute buffer size =    96.04 MiB
llama_context:      CUDA6 compute buffer size =    96.04 MiB
llama_context:      CUDA7 compute buffer size =   348.80 MiB
llama_context:  CUDA_Host compute buffer size =    36.05 MiB
llama_context: graph nodes  = 1043
llama_context: graph splits = 9
llama_context: constructing llama_context
llama_context: n_seq_max     = 2
llama_context: n_ctx         = 8192
llama_context: n_ctx_seq     = 4096
llama_context: n_batch       = 2048
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = enabled
llama_context: kv_unified    = false
llama_context: freq_base     = 1000000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_seq (4096) < n_ctx_train (32768) -- the full capacity of the model will not be utilized
set_abort_callback: call
llama_context:  CUDA_Host  output buffer size =     1.16 MiB
llama_kv_cache: layer   0: dev = CUDA0
llama_kv_cache: layer   1: dev = CUDA0
llama_kv_cache: layer   2: dev = CUDA0
llama_kv_cache: layer   3: dev = CUDA0
llama_kv_cache: layer   4: dev = CUDA1
llama_kv_cache: layer   5: dev = CUDA1
llama_kv_cache: layer   6: dev = CUDA1
llama_kv_cache: layer   7: dev = CUDA1
llama_kv_cache: layer   8: dev = CUDA2
llama_kv_cache: layer   9: dev = CUDA2
llama_kv_cache: layer  10: dev = CUDA2
llama_kv_cache: layer  11: dev = CUDA3
llama_kv_cache: layer  12: dev = CUDA3
llama_kv_cache: layer  13: dev = CUDA3
llama_kv_cache: layer  14: dev = CUDA3
llama_kv_cache: layer  15: dev = CUDA4
llama_kv_cache: layer  16: dev = CUDA4
llama_kv_cache: layer  17: dev = CUDA4
llama_kv_cache: layer  18: dev = CUDA4
llama_kv_cache: layer  19: dev = CUDA5
llama_kv_cache: layer  20: dev = CUDA5
llama_kv_cache: layer  21: dev = CUDA5
llama_kv_cache: layer  22: dev = CUDA6
llama_kv_cache: layer  23: dev = CUDA6
llama_kv_cache: layer  24: dev = CUDA6
llama_kv_cache: layer  25: dev = CUDA6
llama_kv_cache: layer  26: dev = CUDA7
llama_kv_cache: layer  27: dev = CUDA7
llama_kv_cache:      CUDA0 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA1 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA2 KV buffer size =    96.00 MiB
llama_kv_cache:      CUDA3 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA4 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA5 KV buffer size =    96.00 MiB
llama_kv_cache:      CUDA6 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA7 KV buffer size =    64.00 MiB
llama_kv_cache: size =  896.00 MiB (  4096 cells,  28 layers,  2/2 seqs), K (f16):  448.00 MiB, V (f16):  448.00 MiB
llama_context: enumerating backends
llama_context: backend_ptrs.size() = 9
llama_context: max_nodes = 2488
llama_context: pipeline parallelism enabled (n_copies=4)
llama_context: reserving full memory module
llama_context: worst-case: n_tokens = 512, n_seqs = 2, n_outputs = 2
graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  2, n_outputs =  512
graph_reserve: reserving a graph for ubatch with n_tokens =    2, n_seqs =  2, n_outputs =    2
graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  2, n_outputs =  512
llama_context:      CUDA0 compute buffer size =   100.04 MiB
llama_context:      CUDA1 compute buffer size =    96.04 MiB
llama_context:      CUDA2 compute buffer size =    96.04 MiB
llama_context:      CUDA3 compute buffer size =    96.04 MiB
llama_context:      CUDA4 compute buffer size =    96.04 MiB
llama_context:      CUDA5 compute buffer size =    96.04 MiB
llama_context:      CUDA6 compute buffer size =    96.04 MiB
llama_context:      CUDA7 compute buffer size =   348.80 MiB
llama_context:  CUDA_Host compute buffer size =    36.05 MiB
llama_context: graph nodes  = 1043
llama_context: graph splits = 9
llama_context: constructing llama_context
llama_context: n_seq_max     = 2
llama_context: n_ctx         = 8192
llama_context: n_ctx_seq     = 4096
llama_context: n_batch       = 2048
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = enabled
llama_context: kv_unified    = false
llama_context: freq_base     = 1000000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_seq (4096) < n_ctx_train (32768) -- the full capacity of the model will not be utilized
set_abort_callback: call
llama_context:  CUDA_Host  output buffer size =     1.16 MiB
llama_kv_cache: layer   0: dev = CUDA0
llama_kv_cache: layer   1: dev = CUDA0
llama_kv_cache: layer   2: dev = CUDA0
llama_kv_cache: layer   3: dev = CUDA0
llama_kv_cache: layer   4: dev = CUDA1
llama_kv_cache: layer   5: dev = CUDA1
llama_kv_cache: layer   6: dev = CUDA1
llama_kv_cache: layer   7: dev = CUDA1
llama_kv_cache: layer   8: dev = CUDA2
llama_kv_cache: layer   9: dev = CUDA2
llama_kv_cache: layer  10: dev = CUDA2
llama_kv_cache: layer  11: dev = CUDA3
llama_kv_cache: layer  12: dev = CUDA3
llama_kv_cache: layer  13: dev = CUDA3
llama_kv_cache: layer  14: dev = CUDA3
llama_kv_cache: layer  15: dev = CUDA4
llama_kv_cache: layer  16: dev = CUDA4
llama_kv_cache: layer  17: dev = CUDA4
llama_kv_cache: layer  18: dev = CUDA4
llama_kv_cache: layer  19: dev = CUDA5
llama_kv_cache: layer  20: dev = CUDA5
llama_kv_cache: layer  21: dev = CUDA5
llama_kv_cache: layer  22: dev = CUDA6
llama_kv_cache: layer  23: dev = CUDA6
llama_kv_cache: layer  24: dev = CUDA6
llama_kv_cache: layer  25: dev = CUDA6
llama_kv_cache: layer  26: dev = CUDA7
llama_kv_cache: layer  27: dev = CUDA7
llama_kv_cache:      CUDA0 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA1 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA2 KV buffer size =    96.00 MiB
llama_kv_cache:      CUDA3 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA4 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA5 KV buffer size =    96.00 MiB
llama_kv_cache:      CUDA6 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA7 KV buffer size =    64.00 MiB
llama_kv_cache: size =  896.00 MiB (  4096 cells,  28 layers,  2/2 seqs), K (f16):  448.00 MiB, V (f16):  448.00 MiB
llama_context: enumerating backends
llama_context: backend_ptrs.size() = 9
llama_context: max_nodes = 2488
llama_context: pipeline parallelism enabled (n_copies=4)
llama_context: reserving full memory module
llama_context: worst-case: n_tokens = 512, n_seqs = 2, n_outputs = 2
graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  2, n_outputs =  512
graph_reserve: reserving a graph for ubatch with n_tokens =    2, n_seqs =  2, n_outputs =    2
graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  2, n_outputs =  512
llama_context:      CUDA0 compute buffer size =   100.04 MiB
llama_context:      CUDA1 compute buffer size =    96.04 MiB
llama_context:      CUDA2 compute buffer size =    96.04 MiB
llama_context:      CUDA3 compute buffer size =    96.04 MiB
llama_context:      CUDA4 compute buffer size =    96.04 MiB
llama_context:      CUDA5 compute buffer size =    96.04 MiB
llama_context:      CUDA6 compute buffer size =    96.04 MiB
llama_context:      CUDA7 compute buffer size =   348.80 MiB
llama_context:  CUDA_Host compute buffer size =    36.05 MiB
llama_context: graph nodes  = 1043
llama_context: graph splits = 9
llama_context: constructing llama_context
llama_context: n_seq_max     = 2
llama_context: n_ctx         = 8192
llama_context: n_ctx_seq     = 4096
llama_context: n_batch       = 2048
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = enabled
llama_context: kv_unified    = false
llama_context: freq_base     = 1000000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_seq (4096) < n_ctx_train (32768) -- the full capacity of the model will not be utilized
set_abort_callback: call
llama_context:  CUDA_Host  output buffer size =     1.16 MiB
llama_kv_cache: layer   0: dev = CUDA0
llama_kv_cache: layer   1: dev = CUDA0
llama_kv_cache: layer   2: dev = CUDA0
llama_kv_cache: layer   3: dev = CUDA0
llama_kv_cache: layer   4: dev = CUDA1
llama_kv_cache: layer   5: dev = CUDA1
llama_kv_cache: layer   6: dev = CUDA1
llama_kv_cache: layer   7: dev = CUDA1
llama_kv_cache: layer   8: dev = CUDA2
llama_kv_cache: layer   9: dev = CUDA2
llama_kv_cache: layer  10: dev = CUDA2
llama_kv_cache: layer  11: dev = CUDA3
llama_kv_cache: layer  12: dev = CUDA3
llama_kv_cache: layer  13: dev = CUDA3
llama_kv_cache: layer  14: dev = CUDA3
llama_kv_cache: layer  15: dev = CUDA4
llama_kv_cache: layer  16: dev = CUDA4
llama_kv_cache: layer  17: dev = CUDA4
llama_kv_cache: layer  18: dev = CUDA4
llama_kv_cache: layer  19: dev = CUDA5
llama_kv_cache: layer  20: dev = CUDA5
llama_kv_cache: layer  21: dev = CUDA5
llama_kv_cache: layer  22: dev = CUDA6
llama_kv_cache: layer  23: dev = CUDA6
llama_kv_cache: layer  24: dev = CUDA6
llama_kv_cache: layer  25: dev = CUDA6
llama_kv_cache: layer  26: dev = CUDA7
llama_kv_cache: layer  27: dev = CUDA7
llama_kv_cache:      CUDA0 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA1 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA2 KV buffer size =    96.00 MiB
llama_kv_cache:      CUDA3 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA4 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA5 KV buffer size =    96.00 MiB
llama_kv_cache:      CUDA6 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA7 KV buffer size =    64.00 MiB
llama_kv_cache: size =  896.00 MiB (  4096 cells,  28 layers,  2/2 seqs), K (f16):  448.00 MiB, V (f16):  448.00 MiB
llama_context: enumerating backends
llama_context: backend_ptrs.size() = 9
llama_context: max_nodes = 2488
llama_context: pipeline parallelism enabled (n_copies=4)
llama_context: reserving full memory module
llama_context: worst-case: n_tokens = 512, n_seqs = 2, n_outputs = 2
graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  2, n_outputs =  512
graph_reserve: reserving a graph for ubatch with n_tokens =    2, n_seqs =  2, n_outputs =    2
graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  2, n_outputs =  512
llama_context:      CUDA0 compute buffer size =   100.04 MiB
llama_context:      CUDA1 compute buffer size =    96.04 MiB
llama_context:      CUDA2 compute buffer size =    96.04 MiB
llama_context:      CUDA3 compute buffer size =    96.04 MiB
llama_context:      CUDA4 compute buffer size =    96.04 MiB
llama_context:      CUDA5 compute buffer size =    96.04 MiB
llama_context:      CUDA6 compute buffer size =    96.04 MiB
llama_context:      CUDA7 compute buffer size =   348.80 MiB
llama_context:  CUDA_Host compute buffer size =    36.05 MiB
llama_context: graph nodes  = 1043
llama_context: graph splits = 9
llama_context: constructing llama_context
llama_context: n_seq_max     = 2
llama_context: n_ctx         = 8192
llama_context: n_ctx_seq     = 4096
llama_context: n_batch       = 2048
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = enabled
llama_context: kv_unified    = false
llama_context: freq_base     = 1000000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_seq (4096) < n_ctx_train (32768) -- the full capacity of the model will not be utilized
set_abort_callback: call
llama_context:  CUDA_Host  output buffer size =     1.16 MiB
llama_kv_cache: layer   0: dev = CUDA0
llama_kv_cache: layer   1: dev = CUDA0
llama_kv_cache: layer   2: dev = CUDA0
llama_kv_cache: layer   3: dev = CUDA0
llama_kv_cache: layer   4: dev = CUDA1
llama_kv_cache: layer   5: dev = CUDA1
llama_kv_cache: layer   6: dev = CUDA1
llama_kv_cache: layer   7: dev = CUDA1
llama_kv_cache: layer   8: dev = CUDA2
llama_kv_cache: layer   9: dev = CUDA2
llama_kv_cache: layer  10: dev = CUDA2
llama_kv_cache: layer  11: dev = CUDA3
llama_kv_cache: layer  12: dev = CUDA3
llama_kv_cache: layer  13: dev = CUDA3
llama_kv_cache: layer  14: dev = CUDA3
llama_kv_cache: layer  15: dev = CUDA4
llama_kv_cache: layer  16: dev = CUDA4
llama_kv_cache: layer  17: dev = CUDA4
llama_kv_cache: layer  18: dev = CUDA4
llama_kv_cache: layer  19: dev = CUDA5
llama_kv_cache: layer  20: dev = CUDA5
llama_kv_cache: layer  21: dev = CUDA5
llama_kv_cache: layer  22: dev = CUDA6
llama_kv_cache: layer  23: dev = CUDA6
llama_kv_cache: layer  24: dev = CUDA6
llama_kv_cache: layer  25: dev = CUDA6
llama_kv_cache: layer  26: dev = CUDA7
llama_kv_cache: layer  27: dev = CUDA7
llama_kv_cache:      CUDA0 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA1 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA2 KV buffer size =    96.00 MiB
llama_kv_cache:      CUDA3 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA4 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA5 KV buffer size =    96.00 MiB
llama_kv_cache:      CUDA6 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA7 KV buffer size =    64.00 MiB
llama_kv_cache: size =  896.00 MiB (  4096 cells,  28 layers,  2/2 seqs), K (f16):  448.00 MiB, V (f16):  448.00 MiB
llama_context: enumerating backends
llama_context: backend_ptrs.size() = 9
llama_context: max_nodes = 2488
llama_context: pipeline parallelism enabled (n_copies=4)
llama_context: reserving full memory module
llama_context: worst-case: n_tokens = 512, n_seqs = 2, n_outputs = 2
graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  2, n_outputs =  512
graph_reserve: reserving a graph for ubatch with n_tokens =    2, n_seqs =  2, n_outputs =    2
graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  2, n_outputs =  512
llama_context:      CUDA0 compute buffer size =   100.04 MiB
llama_context:      CUDA1 compute buffer size =    96.04 MiB
llama_context:      CUDA2 compute buffer size =    96.04 MiB
llama_context:      CUDA3 compute buffer size =    96.04 MiB
llama_context:      CUDA4 compute buffer size =    96.04 MiB
llama_context:      CUDA5 compute buffer size =    96.04 MiB
llama_context:      CUDA6 compute buffer size =    96.04 MiB
llama_context:      CUDA7 compute buffer size =   348.80 MiB
llama_context:  CUDA_Host compute buffer size =    36.05 MiB
llama_context: graph nodes  = 1043
llama_context: graph splits = 9
llama_context: constructing llama_context
llama_context: n_seq_max     = 2
llama_context: n_ctx         = 8192
llama_context: n_ctx_seq     = 4096
llama_context: n_batch       = 2048
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = enabled
llama_context: kv_unified    = false
llama_context: freq_base     = 1000000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_seq (4096) < n_ctx_train (32768) -- the full capacity of the model will not be utilized
set_abort_callback: call
llama_context:  CUDA_Host  output buffer size =     1.16 MiB
llama_kv_cache: layer   0: dev = CUDA0
llama_kv_cache: layer   1: dev = CUDA0
llama_kv_cache: layer   2: dev = CUDA0
llama_kv_cache: layer   3: dev = CUDA0
llama_kv_cache: layer   4: dev = CUDA1
llama_kv_cache: layer   5: dev = CUDA1
llama_kv_cache: layer   6: dev = CUDA1
llama_kv_cache: layer   7: dev = CUDA1
llama_kv_cache: layer   8: dev = CUDA2
llama_kv_cache: layer   9: dev = CUDA2
llama_kv_cache: layer  10: dev = CUDA2
llama_kv_cache: layer  11: dev = CUDA3
llama_kv_cache: layer  12: dev = CUDA3
llama_kv_cache: layer  13: dev = CUDA3
llama_kv_cache: layer  14: dev = CUDA3
llama_kv_cache: layer  15: dev = CUDA4
llama_kv_cache: layer  16: dev = CUDA4
llama_kv_cache: layer  17: dev = CUDA4
llama_kv_cache: layer  18: dev = CUDA4
llama_kv_cache: layer  19: dev = CUDA5
llama_kv_cache: layer  20: dev = CUDA5
llama_kv_cache: layer  21: dev = CUDA5
llama_kv_cache: layer  22: dev = CUDA6
llama_kv_cache: layer  23: dev = CUDA6
llama_kv_cache: layer  24: dev = CUDA6
llama_kv_cache: layer  25: dev = CUDA6
llama_kv_cache: layer  26: dev = CUDA7
llama_kv_cache: layer  27: dev = CUDA7
llama_kv_cache:      CUDA0 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA1 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA2 KV buffer size =    96.00 MiB
llama_kv_cache:      CUDA3 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA4 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA5 KV buffer size =    96.00 MiB
llama_kv_cache:      CUDA6 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA7 KV buffer size =    64.00 MiB
llama_kv_cache: size =  896.00 MiB (  4096 cells,  28 layers,  2/2 seqs), K (f16):  448.00 MiB, V (f16):  448.00 MiB
llama_context: enumerating backends
llama_context: backend_ptrs.size() = 9
llama_context: max_nodes = 2488
llama_context: pipeline parallelism enabled (n_copies=4)
llama_context: reserving full memory module
llama_context: worst-case: n_tokens = 512, n_seqs = 2, n_outputs = 2
graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  2, n_outputs =  512
graph_reserve: reserving a graph for ubatch with n_tokens =    2, n_seqs =  2, n_outputs =    2
graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  2, n_outputs =  512
llama_context:      CUDA0 compute buffer size =   100.04 MiB
llama_context:      CUDA1 compute buffer size =    96.04 MiB
llama_context:      CUDA2 compute buffer size =    96.04 MiB
llama_context:      CUDA3 compute buffer size =    96.04 MiB
llama_context:      CUDA4 compute buffer size =    96.04 MiB
llama_context:      CUDA5 compute buffer size =    96.04 MiB
llama_context:      CUDA6 compute buffer size =    96.04 MiB
llama_context:      CUDA7 compute buffer size =   348.80 MiB
llama_context:  CUDA_Host compute buffer size =    36.05 MiB
llama_context: graph nodes  = 1043
llama_context: graph splits = 9
llama_context: constructing llama_context
llama_context: n_seq_max     = 2
llama_context: n_ctx         = 8192
llama_context: n_ctx_seq     = 4096
llama_context: n_batch       = 2048
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = enabled
llama_context: kv_unified    = false
llama_context: freq_base     = 1000000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_seq (4096) < n_ctx_train (32768) -- the full capacity of the model will not be utilized
set_abort_callback: call
llama_context:  CUDA_Host  output buffer size =     1.16 MiB
llama_kv_cache: layer   0: dev = CUDA0
llama_kv_cache: layer   1: dev = CUDA0
llama_kv_cache: layer   2: dev = CUDA0
llama_kv_cache: layer   3: dev = CUDA0
llama_kv_cache: layer   4: dev = CUDA1
llama_kv_cache: layer   5: dev = CUDA1
llama_kv_cache: layer   6: dev = CUDA1
llama_kv_cache: layer   7: dev = CUDA1
llama_kv_cache: layer   8: dev = CUDA2
llama_kv_cache: layer   9: dev = CUDA2
llama_kv_cache: layer  10: dev = CUDA2
llama_kv_cache: layer  11: dev = CUDA3
llama_kv_cache: layer  12: dev = CUDA3
llama_kv_cache: layer  13: dev = CUDA3
llama_kv_cache: layer  14: dev = CUDA3
llama_kv_cache: layer  15: dev = CUDA4
llama_kv_cache: layer  16: dev = CUDA4
llama_kv_cache: layer  17: dev = CUDA4
llama_kv_cache: layer  18: dev = CUDA4
llama_kv_cache: layer  19: dev = CUDA5
llama_kv_cache: layer  20: dev = CUDA5
llama_kv_cache: layer  21: dev = CUDA5
llama_kv_cache: layer  22: dev = CUDA6
llama_kv_cache: layer  23: dev = CUDA6
llama_kv_cache: layer  24: dev = CUDA6
llama_kv_cache: layer  25: dev = CUDA6
llama_kv_cache: layer  26: dev = CUDA7
llama_kv_cache: layer  27: dev = CUDA7
llama_kv_cache:      CUDA0 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA1 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA2 KV buffer size =    96.00 MiB
llama_kv_cache:      CUDA3 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA4 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA5 KV buffer size =    96.00 MiB
llama_kv_cache:      CUDA6 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA7 KV buffer size =    64.00 MiB
llama_kv_cache: size =  896.00 MiB (  4096 cells,  28 layers,  2/2 seqs), K (f16):  448.00 MiB, V (f16):  448.00 MiB
llama_context: enumerating backends
llama_context: backend_ptrs.size() = 9
llama_context: max_nodes = 2488
llama_context: pipeline parallelism enabled (n_copies=4)
llama_context: reserving full memory module
llama_context: worst-case: n_tokens = 512, n_seqs = 2, n_outputs = 2
graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  2, n_outputs =  512
graph_reserve: reserving a graph for ubatch with n_tokens =    2, n_seqs =  2, n_outputs =    2
graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  2, n_outputs =  512
llama_context:      CUDA0 compute buffer size =   100.04 MiB
llama_context:      CUDA1 compute buffer size =    96.04 MiB
llama_context:      CUDA2 compute buffer size =    96.04 MiB
llama_context:      CUDA3 compute buffer size =    96.04 MiB
llama_context:      CUDA4 compute buffer size =    96.04 MiB
llama_context:      CUDA5 compute buffer size =    96.04 MiB
llama_context:      CUDA6 compute buffer size =    96.04 MiB
llama_context:      CUDA7 compute buffer size =   348.80 MiB
llama_context:  CUDA_Host compute buffer size =    36.05 MiB
llama_context: graph nodes  = 1043
llama_context: graph splits = 9
llama_context: constructing llama_context
llama_context: n_seq_max     = 2
llama_context: n_ctx         = 8192
llama_context: n_ctx_seq     = 4096
llama_context: n_batch       = 2048
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = enabled
llama_context: kv_unified    = false
llama_context: freq_base     = 1000000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_seq (4096) < n_ctx_train (32768) -- the full capacity of the model will not be utilized
set_abort_callback: call
llama_context:  CUDA_Host  output buffer size =     1.16 MiB
llama_kv_cache: layer   0: dev = CUDA0
llama_kv_cache: layer   1: dev = CUDA0
llama_kv_cache: layer   2: dev = CUDA0
llama_kv_cache: layer   3: dev = CUDA0
llama_kv_cache: layer   4: dev = CUDA1
llama_kv_cache: layer   5: dev = CUDA1
llama_kv_cache: layer   6: dev = CUDA1
llama_kv_cache: layer   7: dev = CUDA1
llama_kv_cache: layer   8: dev = CUDA2
llama_kv_cache: layer   9: dev = CUDA2
llama_kv_cache: layer  10: dev = CUDA2
llama_kv_cache: layer  11: dev = CUDA3
llama_kv_cache: layer  12: dev = CUDA3
llama_kv_cache: layer  13: dev = CUDA3
llama_kv_cache: layer  14: dev = CUDA3
llama_kv_cache: layer  15: dev = CUDA4
llama_kv_cache: layer  16: dev = CUDA4
llama_kv_cache: layer  17: dev = CUDA4
llama_kv_cache: layer  18: dev = CUDA4
llama_kv_cache: layer  19: dev = CUDA5
llama_kv_cache: layer  20: dev = CUDA5
llama_kv_cache: layer  21: dev = CUDA5
llama_kv_cache: layer  22: dev = CUDA6
llama_kv_cache: layer  23: dev = CUDA6
llama_kv_cache: layer  24: dev = CUDA6
llama_kv_cache: layer  25: dev = CUDA6
llama_kv_cache: layer  26: dev = CUDA7
llama_kv_cache: layer  27: dev = CUDA7
llama_kv_cache:      CUDA0 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA1 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA2 KV buffer size =    96.00 MiB
llama_kv_cache:      CUDA3 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA4 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA5 KV buffer size =    96.00 MiB
llama_kv_cache:      CUDA6 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA7 KV buffer size =    64.00 MiB
llama_kv_cache: size =  896.00 MiB (  4096 cells,  28 layers,  2/2 seqs), K (f16):  448.00 MiB, V (f16):  448.00 MiB
llama_context: enumerating backends
llama_context: backend_ptrs.size() = 9
llama_context: max_nodes = 2488
llama_context: pipeline parallelism enabled (n_copies=4)
llama_context: reserving full memory module
llama_context: worst-case: n_tokens = 512, n_seqs = 2, n_outputs = 2
graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  2, n_outputs =  512
graph_reserve: reserving a graph for ubatch with n_tokens =    2, n_seqs =  2, n_outputs =    2
graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  2, n_outputs =  512
llama_context:      CUDA0 compute buffer size =   100.04 MiB
llama_context:      CUDA1 compute buffer size =    96.04 MiB
llama_context:      CUDA2 compute buffer size =    96.04 MiB
llama_context:      CUDA3 compute buffer size =    96.04 MiB
llama_context:      CUDA4 compute buffer size =    96.04 MiB
llama_context:      CUDA5 compute buffer size =    96.04 MiB
llama_context:      CUDA6 compute buffer size =    96.04 MiB
llama_context:      CUDA7 compute buffer size =   348.80 MiB
llama_context:  CUDA_Host compute buffer size =    36.05 MiB
llama_context: graph nodes  = 1043
llama_context: graph splits = 9
llama_context: constructing llama_context
llama_context: n_seq_max     = 2
llama_context: n_ctx         = 8192
llama_context: n_ctx_seq     = 4096
llama_context: n_batch       = 2048
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = enabled
llama_context: kv_unified    = false
llama_context: freq_base     = 1000000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_seq (4096) < n_ctx_train (32768) -- the full capacity of the model will not be utilized
set_abort_callback: call
llama_context:  CUDA_Host  output buffer size =     1.16 MiB
llama_kv_cache: layer   0: dev = CUDA0
llama_kv_cache: layer   1: dev = CUDA0
llama_kv_cache: layer   2: dev = CUDA0
llama_kv_cache: layer   3: dev = CUDA0
llama_kv_cache: layer   4: dev = CUDA1
llama_kv_cache: layer   5: dev = CUDA1
llama_kv_cache: layer   6: dev = CUDA1
llama_kv_cache: layer   7: dev = CUDA1
llama_kv_cache: layer   8: dev = CUDA2
llama_kv_cache: layer   9: dev = CUDA2
llama_kv_cache: layer  10: dev = CUDA2
llama_kv_cache: layer  11: dev = CUDA3
llama_kv_cache: layer  12: dev = CUDA3
llama_kv_cache: layer  13: dev = CUDA3
llama_kv_cache: layer  14: dev = CUDA3
llama_kv_cache: layer  15: dev = CUDA4
llama_kv_cache: layer  16: dev = CUDA4
llama_kv_cache: layer  17: dev = CUDA4
llama_kv_cache: layer  18: dev = CUDA4
llama_kv_cache: layer  19: dev = CUDA5
llama_kv_cache: layer  20: dev = CUDA5
llama_kv_cache: layer  21: dev = CUDA5
llama_kv_cache: layer  22: dev = CUDA6
llama_kv_cache: layer  23: dev = CUDA6
llama_kv_cache: layer  24: dev = CUDA6
llama_kv_cache: layer  25: dev = CUDA6
llama_kv_cache: layer  26: dev = CUDA7
llama_kv_cache: layer  27: dev = CUDA7
llama_kv_cache:      CUDA0 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA1 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA2 KV buffer size =    96.00 MiB
llama_kv_cache:      CUDA3 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA4 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA5 KV buffer size =    96.00 MiB
llama_kv_cache:      CUDA6 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA7 KV buffer size =    64.00 MiB
llama_kv_cache: size =  896.00 MiB (  4096 cells,  28 layers,  2/2 seqs), K (f16):  448.00 MiB, V (f16):  448.00 MiB
llama_context: enumerating backends
llama_context: backend_ptrs.size() = 9
llama_context: max_nodes = 2488
llama_context: pipeline parallelism enabled (n_copies=4)
llama_context: reserving full memory module
llama_context: worst-case: n_tokens = 512, n_seqs = 2, n_outputs = 2
graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  2, n_outputs =  512
graph_reserve: reserving a graph for ubatch with n_tokens =    2, n_seqs =  2, n_outputs =    2
graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  2, n_outputs =  512
llama_context:      CUDA0 compute buffer size =   100.04 MiB
llama_context:      CUDA1 compute buffer size =    96.04 MiB
llama_context:      CUDA2 compute buffer size =    96.04 MiB
llama_context:      CUDA3 compute buffer size =    96.04 MiB
llama_context:      CUDA4 compute buffer size =    96.04 MiB
llama_context:      CUDA5 compute buffer size =    96.04 MiB
llama_context:      CUDA6 compute buffer size =    96.04 MiB
llama_context:      CUDA7 compute buffer size =   348.80 MiB
llama_context:  CUDA_Host compute buffer size =    36.05 MiB
llama_context: graph nodes  = 1043
llama_context: graph splits = 9
llama_context: constructing llama_context
llama_context: n_seq_max     = 2
llama_context: n_ctx         = 8192
llama_context: n_ctx_seq     = 4096
llama_context: n_batch       = 2048
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = enabled
llama_context: kv_unified    = false
llama_context: freq_base     = 1000000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_seq (4096) < n_ctx_train (32768) -- the full capacity of the model will not be utilized
set_abort_callback: call
llama_context:  CUDA_Host  output buffer size =     1.16 MiB
llama_kv_cache: layer   0: dev = CUDA0
llama_kv_cache: layer   1: dev = CUDA0
llama_kv_cache: layer   2: dev = CUDA0
llama_kv_cache: layer   3: dev = CUDA0
llama_kv_cache: layer   4: dev = CUDA1
llama_kv_cache: layer   5: dev = CUDA1
llama_kv_cache: layer   6: dev = CUDA1
llama_kv_cache: layer   7: dev = CUDA1
llama_kv_cache: layer   8: dev = CUDA2
llama_kv_cache: layer   9: dev = CUDA2
llama_kv_cache: layer  10: dev = CUDA2
llama_kv_cache: layer  11: dev = CUDA3
llama_kv_cache: layer  12: dev = CUDA3
llama_kv_cache: layer  13: dev = CUDA3
llama_kv_cache: layer  14: dev = CUDA3
llama_kv_cache: layer  15: dev = CUDA4
llama_kv_cache: layer  16: dev = CUDA4
llama_kv_cache: layer  17: dev = CUDA4
llama_kv_cache: layer  18: dev = CUDA4
llama_kv_cache: layer  19: dev = CUDA5
llama_kv_cache: layer  20: dev = CUDA5
llama_kv_cache: layer  21: dev = CUDA5
llama_kv_cache: layer  22: dev = CUDA6
llama_kv_cache: layer  23: dev = CUDA6
llama_kv_cache: layer  24: dev = CUDA6
llama_kv_cache: layer  25: dev = CUDA6
llama_kv_cache: layer  26: dev = CUDA7
llama_kv_cache: layer  27: dev = CUDA7
llama_kv_cache:      CUDA0 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA1 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA2 KV buffer size =    96.00 MiB
llama_kv_cache:      CUDA3 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA4 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA5 KV buffer size =    96.00 MiB
llama_kv_cache:      CUDA6 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA7 KV buffer size =    64.00 MiB
llama_kv_cache: size =  896.00 MiB (  4096 cells,  28 layers,  2/2 seqs), K (f16):  448.00 MiB, V (f16):  448.00 MiB
llama_context: enumerating backends
llama_context: backend_ptrs.size() = 9
llama_context: max_nodes = 2488
llama_context: pipeline parallelism enabled (n_copies=4)
llama_context: reserving full memory module
llama_context: worst-case: n_tokens = 512, n_seqs = 2, n_outputs = 2
graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  2, n_outputs =  512
graph_reserve: reserving a graph for ubatch with n_tokens =    2, n_seqs =  2, n_outputs =    2
graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  2, n_outputs =  512
llama_context:      CUDA0 compute buffer size =   100.04 MiB
llama_context:      CUDA1 compute buffer size =    96.04 MiB
llama_context:      CUDA2 compute buffer size =    96.04 MiB
llama_context:      CUDA3 compute buffer size =    96.04 MiB
llama_context:      CUDA4 compute buffer size =    96.04 MiB
llama_context:      CUDA5 compute buffer size =    96.04 MiB
llama_context:      CUDA6 compute buffer size =    96.04 MiB
llama_context:      CUDA7 compute buffer size =   348.80 MiB
llama_context:  CUDA_Host compute buffer size =    36.05 MiB
llama_context: graph nodes  = 1043
llama_context: graph splits = 9
llama_context: constructing llama_context
llama_context: n_seq_max     = 2
llama_context: n_ctx         = 8192
llama_context: n_ctx_seq     = 4096
llama_context: n_batch       = 2048
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = enabled
llama_context: kv_unified    = false
llama_context: freq_base     = 1000000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_seq (4096) < n_ctx_train (32768) -- the full capacity of the model will not be utilized
set_abort_callback: call
llama_context:  CUDA_Host  output buffer size =     1.16 MiB
llama_kv_cache: layer   0: dev = CUDA0
llama_kv_cache: layer   1: dev = CUDA0
llama_kv_cache: layer   2: dev = CUDA0
llama_kv_cache: layer   3: dev = CUDA0
llama_kv_cache: layer   4: dev = CUDA1
llama_kv_cache: layer   5: dev = CUDA1
llama_kv_cache: layer   6: dev = CUDA1
llama_kv_cache: layer   7: dev = CUDA1
llama_kv_cache: layer   8: dev = CUDA2
llama_kv_cache: layer   9: dev = CUDA2
llama_kv_cache: layer  10: dev = CUDA2
llama_kv_cache: layer  11: dev = CUDA3
llama_kv_cache: layer  12: dev = CUDA3
llama_kv_cache: layer  13: dev = CUDA3
llama_kv_cache: layer  14: dev = CUDA3
llama_kv_cache: layer  15: dev = CUDA4
llama_kv_cache: layer  16: dev = CUDA4
llama_kv_cache: layer  17: dev = CUDA4
llama_kv_cache: layer  18: dev = CUDA4
llama_kv_cache: layer  19: dev = CUDA5
llama_kv_cache: layer  20: dev = CUDA5
llama_kv_cache: layer  21: dev = CUDA5
llama_kv_cache: layer  22: dev = CUDA6
llama_kv_cache: layer  23: dev = CUDA6
llama_kv_cache: layer  24: dev = CUDA6
llama_kv_cache: layer  25: dev = CUDA6
llama_kv_cache: layer  26: dev = CUDA7
llama_kv_cache: layer  27: dev = CUDA7
llama_kv_cache:      CUDA0 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA1 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA2 KV buffer size =    96.00 MiB
llama_kv_cache:      CUDA3 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA4 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA5 KV buffer size =    96.00 MiB
llama_kv_cache:      CUDA6 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA7 KV buffer size =    64.00 MiB
llama_kv_cache: size =  896.00 MiB (  4096 cells,  28 layers,  2/2 seqs), K (f16):  448.00 MiB, V (f16):  448.00 MiB
llama_context: enumerating backends
llama_context: backend_ptrs.size() = 9
llama_context: max_nodes = 2488
llama_context: pipeline parallelism enabled (n_copies=4)
llama_context: reserving full memory module
llama_context: worst-case: n_tokens = 512, n_seqs = 2, n_outputs = 2
graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  2, n_outputs =  512
graph_reserve: reserving a graph for ubatch with n_tokens =    2, n_seqs =  2, n_outputs =    2
graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  2, n_outputs =  512
llama_context:      CUDA0 compute buffer size =   100.04 MiB
llama_context:      CUDA1 compute buffer size =    96.04 MiB
llama_context:      CUDA2 compute buffer size =    96.04 MiB
llama_context:      CUDA3 compute buffer size =    96.04 MiB
llama_context:      CUDA4 compute buffer size =    96.04 MiB
llama_context:      CUDA5 compute buffer size =    96.04 MiB
llama_context:      CUDA6 compute buffer size =    96.04 MiB
llama_context:      CUDA7 compute buffer size =   348.80 MiB
llama_context:  CUDA_Host compute buffer size =    36.05 MiB
llama_context: graph nodes  = 1043
llama_context: graph splits = 9
llama_context: constructing llama_context
llama_context: n_seq_max     = 2
llama_context: n_ctx         = 8192
llama_context: n_ctx_seq     = 4096
llama_context: n_batch       = 2048
llama_context: n_ubatch      = 512
llama_context: causal_attn   = 1
llama_context: flash_attn    = enabled
llama_context: kv_unified    = false
llama_context: freq_base     = 1000000.0
llama_context: freq_scale    = 1
llama_context: n_ctx_seq (4096) < n_ctx_train (32768) -- the full capacity of the model will not be utilized
set_abort_callback: call
llama_context:  CUDA_Host  output buffer size =     1.16 MiB
llama_kv_cache: layer   0: dev = CUDA0
llama_kv_cache: layer   1: dev = CUDA0
llama_kv_cache: layer   2: dev = CUDA0
llama_kv_cache: layer   3: dev = CUDA0
llama_kv_cache: layer   4: dev = CUDA1
llama_kv_cache: layer   5: dev = CUDA1
llama_kv_cache: layer   6: dev = CUDA1
llama_kv_cache: layer   7: dev = CUDA1
llama_kv_cache: layer   8: dev = CUDA2
llama_kv_cache: layer   9: dev = CUDA2
llama_kv_cache: layer  10: dev = CUDA2
llama_kv_cache: layer  11: dev = CUDA3
llama_kv_cache: layer  12: dev = CUDA3
llama_kv_cache: layer  13: dev = CUDA3
llama_kv_cache: layer  14: dev = CUDA3
llama_kv_cache: layer  15: dev = CUDA4
llama_kv_cache: layer  16: dev = CUDA4
llama_kv_cache: layer  17: dev = CUDA4
llama_kv_cache: layer  18: dev = CUDA4
llama_kv_cache: layer  19: dev = CUDA5
llama_kv_cache: layer  20: dev = CUDA5
llama_kv_cache: layer  21: dev = CUDA5
llama_kv_cache: layer  22: dev = CUDA6
llama_kv_cache: layer  23: dev = CUDA6
llama_kv_cache: layer  24: dev = CUDA6
llama_kv_cache: layer  25: dev = CUDA6
llama_kv_cache: layer  26: dev = CUDA7
llama_kv_cache: layer  27: dev = CUDA7
llama_kv_cache:      CUDA0 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA1 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA2 KV buffer size =    96.00 MiB
llama_kv_cache:      CUDA3 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA4 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA5 KV buffer size =    96.00 MiB
llama_kv_cache:      CUDA6 KV buffer size =   128.00 MiB
llama_kv_cache:      CUDA7 KV buffer size =    64.00 MiB
llama_kv_cache: size =  896.00 MiB (  4096 cells,  28 layers,  2/2 seqs), K (f16):  448.00 MiB, V (f16):  448.00 MiB
llama_context: enumerating backends
llama_context: backend_ptrs.size() = 9
llama_context: max_nodes = 2488
llama_context: pipeline parallelism enabled (n_copies=4)
llama_context: reserving full memory module
llama_context: worst-case: n_tokens = 512, n_seqs = 2, n_outputs = 2
graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  2, n_outputs =  512
graph_reserve: reserving a graph for ubatch with n_tokens =    2, n_seqs =  2, n_outputs =    2
graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  2, n_outputs =  512
llama_context:      CUDA0 compute buffer size =   100.04 MiB
llama_context:      CUDA1 compute buffer size =    96.04 MiB
llama_context:      CUDA2 compute buffer size =    96.04 MiB
llama_context:      CUDA3 compute buffer size =    96.04 MiB
llama_context:      CUDA4 compute buffer size =    96.04 MiB
llama_context:      CUDA5 compute buffer size =    96.04 MiB
llama_context:      CUDA6 compute buffer size =    96.04 MiB
llama_context:      CUDA7 compute buffer size =   348.80 MiB
llama_context:  CUDA_Host compute buffer size =    36.05 MiB
llama_context: graph nodes  = 1043
llama_context: graph splits = 9
completed in 783.11 ms
================================================================================
Warmup completed!
================================================================================


================================================================================
Running test 1/6: Baseline (block=8, steps=8)
================================================================================

Run 1/3:

Run 2/3:

Run 3/3:

Average across 3 runs:

Wall Time: 2705.10 ms
Tokens Generated: 149
Throughput: 55.08 tokens/sec

Top 10 Time-Consuming Sections:
Section                                  Total (ms)   Avg (ms)     Calls   
--------------------------------------------------------------------------------
generation_phase                         2640.21      2640.21      1       
token_sampling                           1854.85      14.16        131     
telemetry_cpu_sampling                   1854.74      14.16        131     
telemetry_cpu_loop                       1854.68      14.16        131     
llama_decode                             684.14       5.22         131     
denoising_step_0                         344.47       20.26        17      
denoising_step_1                         327.17       19.25        17      
denoising_step_2                         326.95       19.23        17      
denoising_step_5                         309.89       19.37        16      
denoising_step_3                         308.62       18.15        17      

================================================================================
Running test 2/6: Baseline (block=4, steps=4)
================================================================================

Run 1/3:

Run 2/3:

Run 3/3:

Average across 3 runs:

Wall Time: 1864.56 ms
Tokens Generated: 149
Throughput: 79.91 tokens/sec

Top 10 Time-Consuming Sections:
Section                                  Total (ms)   Avg (ms)     Calls   
--------------------------------------------------------------------------------
generation_phase                         1795.43      1795.43      1       
token_sampling                           1284.59      9.81         131     
telemetry_cpu_sampling                   1284.49      9.81         131     
telemetry_cpu_loop                       1284.44      9.80         131     
denoising_step_0                         482.58       14.62        33      
denoising_step_1                         403.66       12.23        33      
denoising_step_2                         403.55       12.23        33      
llama_decode                             395.99       3.02         131     
denoising_step_3                         392.41       11.89        33      
finalize_block                           112.90       3.42         33      

================================================================================
Running test 3/6: Baseline (block=8, steps=4)
================================================================================

Run 1/3:

Run 2/3:

Run 3/3:

Average across 3 runs:

Wall Time: 1452.33 ms
Tokens Generated: 149
Throughput: 102.59 tokens/sec

Top 10 Time-Consuming Sections:
Section                                  Total (ms)   Avg (ms)     Calls   
--------------------------------------------------------------------------------
generation_phase                         1388.83      1388.83      1       
token_sampling                           935.39       14.17        66      
telemetry_cpu_sampling                   935.33       14.17        66      
telemetry_cpu_loop                       935.28       14.17        66      
llama_decode                             353.69       5.36         66      
denoising_step_0                         345.47       20.32        17      
denoising_step_1                         327.65       19.27        17      
denoising_step_3                         308.60       19.29        16      
denoising_step_2                         308.40       18.14        17      
finalize_block                           98.46        5.79         17      

================================================================================
Running test 4/6: Baseline (block=8, steps=8) + GPU
================================================================================

Run 1/3:

Run 2/3:

Run 3/3:

Average across 3 runs:

Wall Time: 1599.51 ms
Tokens Generated: 149
Throughput: 93.15 tokens/sec

Top 10 Time-Consuming Sections:
Section                                  Total (ms)   Avg (ms)     Calls   
--------------------------------------------------------------------------------
generation_phase                         1520.05      1520.05      1       
llama_decode                             715.63       5.46         131     
token_sampling                           696.17       5.31         131     
telemetry_gpu_invoke                     417.07       3.18         131     
denoising_step_0                         203.21       11.95        17      
telemetry_gpu_stage_cpu_post             195.92       1.50         131     
telemetry_gpu_logit_pack                 189.78       1.45         131     
denoising_step_1                         181.65       10.69        17      
denoising_step_2                         180.90       10.64        17      
denoising_step_3                         169.94       10.00        17      

================================================================================
Running test 5/6: Baseline (block=4, steps=4) + GPU
================================================================================

Run 1/3:

Run 2/3:

Run 3/3:

Average across 3 runs:

Wall Time: 1293.60 ms
Tokens Generated: 149
Throughput: 115.18 tokens/sec

Top 10 Time-Consuming Sections:
Section                                  Total (ms)   Avg (ms)     Calls   
--------------------------------------------------------------------------------
generation_phase                         1217.68      1217.68      1       
token_sampling                           711.40       5.43         131     
telemetry_gpu_logit_pack                 456.52       3.48         131     
llama_decode                             388.96       2.97         131     
denoising_step_0                         327.76       9.93         33      
denoising_step_1                         261.44       7.92         33      
denoising_step_2                         260.60       7.90         33      
denoising_step_3                         252.49       7.65         33      
telemetry_gpu_invoke                     224.40       1.71         131     
telemetry_gpu_success                    131.00       1.00         131     

================================================================================
Running test 6/6: Baseline (block=8, steps=4) + GPU
================================================================================

Run 1/3:

Run 2/3:

Run 3/3:

Average across 3 runs:

Wall Time: 907.78 ms
Tokens Generated: 149
Throughput: 164.14 tokens/sec

Top 10 Time-Consuming Sections:
Section                                  Total (ms)   Avg (ms)     Calls   
--------------------------------------------------------------------------------
generation_phase                         828.13       828.13       1       
llama_decode                             365.20       5.53         66      
token_sampling                           357.20       5.41         66      
telemetry_gpu_invoke                     214.68       3.25         66      
denoising_step_0                         199.52       11.74        17      
denoising_step_1                         182.33       10.73        17      
denoising_step_2                         171.26       10.07        17      
denoising_step_3                         170.79       10.67        16      
finalize_block                           103.60       6.09         17      
telemetry_gpu_stage_cpu_post             101.48       1.54         66      

================================================================================
Detailed Analysis for: Baseline (block=8, steps=8)
================================================================================

================================================================================
BOTTLENECK ANALYSIS
================================================================================

Total Time: 2705.10 ms

Critical Bottlenecks (>10% of total time or >50ms avg):
Section                                  Time (ms)    Percentage   Avg (ms)    
--------------------------------------------------------------------------------
generation_phase                         2640.21      97.6        % 2640.21     
token_sampling                           1854.85      68.6        % 14.16       
telemetry_cpu_sampling                   1854.74      68.6        % 14.16       
telemetry_cpu_loop                       1854.68      68.6        % 14.16       
llama_decode                             684.14       25.3        % 5.22        
denoising_step_0                         344.47       12.7        % 20.26       
denoising_step_1                         327.17       12.1        % 19.25       
denoising_step_2                         326.95       12.1        % 19.23       
denoising_step_5                         309.89       11.5        % 19.37       
denoising_step_3                         308.62       11.4        % 18.15       
denoising_step_7                         308.17       11.4        % 19.26       
denoising_step_6                         308.00       11.4        % 19.25       
denoising_step_4                         307.70       11.4        % 19.23       
block_9                                  161.40       6.0         % 161.40      
block_6                                  161.35       6.0         % 161.35      
block_4                                  161.15       6.0         % 161.15      
block_14                                 160.99       6.0         % 160.99      
block_3                                  160.89       5.9         % 160.89      
block_5                                  160.82       5.9         % 160.82      
block_18                                 160.49       5.9         % 160.49      
block_12                                 160.42       5.9         % 160.42      
block_10                                 160.40       5.9         % 160.40      
block_8                                  160.37       5.9         % 160.37      
block_16                                 160.34       5.9         % 160.34      
block_15                                 160.33       5.9         % 160.33      
block_17                                 160.31       5.9         % 160.31      
block_13                                 160.31       5.9         % 160.31      
block_7                                  160.30       5.9         % 160.30      
block_11                                 160.28       5.9         % 160.28      
block_2                                  70.03        2.6         % 70.03       

Visualization saved to profile_0.png

================================================================================
Detailed Analysis for: Baseline (block=4, steps=4)
================================================================================

================================================================================
BOTTLENECK ANALYSIS
================================================================================

Total Time: 1864.56 ms

Critical Bottlenecks (>10% of total time or >50ms avg):
Section                                  Time (ms)    Percentage   Avg (ms)    
--------------------------------------------------------------------------------
generation_phase                         1795.43      96.3        % 1795.43     
token_sampling                           1284.59      68.9        % 9.81        
telemetry_cpu_sampling                   1284.49      68.9        % 9.81        
telemetry_cpu_loop                       1284.44      68.9        % 9.80        
denoising_step_0                         482.58       25.9        % 14.62       
denoising_step_1                         403.66       21.6        % 12.23       
denoising_step_2                         403.55       21.6        % 12.23       
llama_decode                             395.99       21.2        % 3.02        
denoising_step_3                         392.41       21.0        % 11.89       
block_5                                  55.78        3.0         % 55.78       
block_13                                 55.42        3.0         % 55.42       
block_20                                 54.71        2.9         % 54.71       
block_35                                 54.67        2.9         % 54.67       
block_21                                 54.39        2.9         % 54.39       
block_8                                  54.35        2.9         % 54.35       
block_33                                 54.34        2.9         % 54.34       
block_37                                 54.34        2.9         % 54.34       
block_30                                 54.34        2.9         % 54.34       
block_16                                 54.33        2.9         % 54.33       
block_31                                 54.32        2.9         % 54.32       
block_6                                  54.32        2.9         % 54.32       
block_26                                 54.32        2.9         % 54.32       
block_25                                 54.32        2.9         % 54.32       
block_23                                 54.31        2.9         % 54.31       
block_11                                 54.31        2.9         % 54.31       
block_22                                 54.30        2.9         % 54.30       
block_12                                 54.30        2.9         % 54.30       
block_34                                 54.30        2.9         % 54.30       
block_32                                 54.30        2.9         % 54.30       
block_29                                 54.30        2.9         % 54.30       
block_18                                 54.30        2.9         % 54.30       
block_10                                 54.29        2.9         % 54.29       
block_28                                 54.29        2.9         % 54.29       
block_24                                 54.29        2.9         % 54.29       
block_17                                 54.29        2.9         % 54.29       
block_9                                  54.29        2.9         % 54.29       
block_7                                  54.28        2.9         % 54.28       
block_36                                 54.28        2.9         % 54.28       
block_19                                 54.26        2.9         % 54.26       
block_15                                 54.24        2.9         % 54.24       
block_27                                 54.23        2.9         % 54.23       
block_14                                 54.23        2.9         % 54.23       

Visualization saved to profile_1.png

================================================================================
Detailed Analysis for: Baseline (block=8, steps=4)
================================================================================

================================================================================
BOTTLENECK ANALYSIS
================================================================================

Total Time: 1452.33 ms

Critical Bottlenecks (>10% of total time or >50ms avg):
Section                                  Time (ms)    Percentage   Avg (ms)    
--------------------------------------------------------------------------------
generation_phase                         1388.83      95.6        % 1388.83     
token_sampling                           935.39       64.4        % 14.17       
telemetry_cpu_sampling                   935.33       64.4        % 14.17       
telemetry_cpu_loop                       935.28       64.4        % 14.17       
llama_decode                             353.69       24.4        % 5.36        
denoising_step_0                         345.47       23.8        % 20.32       
denoising_step_1                         327.65       22.6        % 19.27       
denoising_step_3                         308.60       21.2        % 19.29       
denoising_step_2                         308.40       21.2        % 18.14       
block_5                                  84.20        5.8         % 84.20       
block_8                                  83.94        5.8         % 83.94       
block_7                                  83.82        5.8         % 83.82       
block_6                                  83.60        5.8         % 83.60       
block_14                                 83.58        5.8         % 83.58       
block_3                                  83.56        5.8         % 83.56       
block_10                                 83.53        5.8         % 83.53       
block_18                                 83.53        5.8         % 83.53       
block_17                                 83.53        5.8         % 83.53       
block_12                                 83.52        5.8         % 83.52       
block_4                                  83.51        5.8         % 83.51       
block_11                                 83.51        5.8         % 83.51       
block_9                                  83.51        5.8         % 83.51       
block_13                                 83.51        5.8         % 83.51       
block_15                                 83.50        5.7         % 83.50       
block_16                                 83.49        5.7         % 83.49       
block_2                                  50.94        3.5         % 50.94       

Visualization saved to profile_2.png

================================================================================
Detailed Analysis for: Baseline (block=8, steps=8) + GPU
================================================================================

================================================================================
BOTTLENECK ANALYSIS
================================================================================

Total Time: 1599.51 ms

Critical Bottlenecks (>10% of total time or >50ms avg):
Section                                  Time (ms)    Percentage   Avg (ms)    
--------------------------------------------------------------------------------
generation_phase                         1520.05      95.0        % 1520.05     
llama_decode                             715.63       44.7        % 5.46        
token_sampling                           696.17       43.5        % 5.31        
telemetry_gpu_invoke                     417.07       26.1        % 3.18        
denoising_step_0                         203.21       12.7        % 11.95       
telemetry_gpu_stage_cpu_post             195.92       12.2        % 1.50        
telemetry_gpu_logit_pack                 189.78       11.9        % 1.45        
denoising_step_1                         181.65       11.4        % 10.69       
denoising_step_2                         180.90       11.3        % 10.64       
denoising_step_3                         169.94       10.6        % 10.00       
denoising_step_7                         169.90       10.6        % 10.62       
denoising_step_4                         169.89       10.6        % 10.62       
denoising_step_5                         169.84       10.6        % 10.61       
denoising_step_6                         169.54       10.6        % 10.60       
block_4                                  93.00        5.8         % 93.00       
block_10                                 92.82        5.8         % 92.82       
block_7                                  92.72        5.8         % 92.72       
block_6                                  92.64        5.8         % 92.64       
block_5                                  92.59        5.8         % 92.59       
block_8                                  92.56        5.8         % 92.56       
block_9                                  92.54        5.8         % 92.54       
block_3                                  92.43        5.8         % 92.43       
block_18                                 91.84        5.7         % 91.84       
block_17                                 91.71        5.7         % 91.71       
block_16                                 91.54        5.7         % 91.54       
block_14                                 91.32        5.7         % 91.32       
block_15                                 91.22        5.7         % 91.22       
block_13                                 91.18        5.7         % 91.18       
block_11                                 91.09        5.7         % 91.09       
block_12                                 90.76        5.7         % 90.76       

Visualization saved to profile_3.png

================================================================================
Detailed Analysis for: Baseline (block=4, steps=4) + GPU
================================================================================

================================================================================
BOTTLENECK ANALYSIS
================================================================================

Total Time: 1293.60 ms

Critical Bottlenecks (>10% of total time or >50ms avg):
Section                                  Time (ms)    Percentage   Avg (ms)    
--------------------------------------------------------------------------------
generation_phase                         1217.68      94.1        % 1217.68     
token_sampling                           711.40       55.0        % 5.43        
telemetry_gpu_logit_pack                 456.52       35.3        % 3.48        
llama_decode                             388.96       30.1        % 2.97        
denoising_step_0                         327.76       25.3        % 9.93        
denoising_step_1                         261.44       20.2        % 7.92        
denoising_step_2                         260.60       20.1        % 7.90        
denoising_step_3                         252.49       19.5        % 7.65        
telemetry_gpu_invoke                     224.40       17.3        % 1.71        
telemetry_gpu_success                    131.00       10.1        % 1.00        

Visualization saved to profile_4.png

================================================================================
Detailed Analysis for: Baseline (block=8, steps=4) + GPU
================================================================================

================================================================================
BOTTLENECK ANALYSIS
================================================================================

Total Time: 907.78 ms

Critical Bottlenecks (>10% of total time or >50ms avg):
Section                                  Time (ms)    Percentage   Avg (ms)    
--------------------------------------------------------------------------------
generation_phase                         828.13       91.2        % 828.13      
llama_decode                             365.20       40.2        % 5.53        
token_sampling                           357.20       39.3        % 5.41        
telemetry_gpu_invoke                     214.68       23.6        % 3.25        
denoising_step_0                         199.52       22.0        % 11.74       
denoising_step_1                         182.33       20.1        % 10.73       
denoising_step_2                         171.26       18.9        % 10.07       
denoising_step_3                         170.79       18.8        % 10.67       
finalize_block                           103.60       11.4        % 6.09        
telemetry_gpu_stage_cpu_post             101.48       11.2        % 1.54        
telemetry_gpu_logit_pack                 95.96        10.6        % 1.45        
block_5                                  50.84        5.6         % 50.84       

Visualization saved to profile_5.png

Results exported to profile_results.json
Results archived to profile_runs/20251129_103237

================================================================================
COMPARATIVE SUMMARY
================================================================================

Config                         Wall Time (ms)  Tokens/sec      Speedup   
----------------------------------------------------------------------
Baseline (block=8, steps=8)    2705.10         55.08           1.00      x
Baseline (block=4, steps=4)    1864.56         79.91           1.45      x
Baseline (block=8, steps=4)    1452.33         102.59          1.86      x
Baseline (block=8, steps=8) + GPU 1599.51         93.15           1.69      x
Baseline (block=4, steps=4) + GPU 1293.60         115.18          2.09      x
Baseline (block=8, steps=4) + GPU 907.78          164.14          2.98      x
